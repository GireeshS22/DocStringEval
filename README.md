# DocStringEval: Evaluating the Effectiveness of Language Models for Code Explanation Through DocString Generation

A comprehensive framework for evaluating Large Language Models (LLMs) in automated docstring generation tasks for Python classes.

**Authors:** Gireesh Sundaram, Balaji Venktesh V, Sundharakumar K B

## Overview

This project evaluates the performance of various LLMs in generating docstrings for Python classes. The framework includes:

- **Data Extraction**: Automated extraction of Python classes from repositories
- **Code Preprocessing**: Cleaning and standardizing code for consistent evaluation
- **Docstring Generation**: Using multiple LLMs to generate docstrings
- **Evaluation Metrics**: Comprehensive evaluation using ROUGE, BLEU, and custom metrics
- **Analysis Tools**: Detailed analysis and comparison of model performance

## Project Structure

```
DocStringEval/
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ LICENSE                     # MIT License
â”œâ”€â”€ classes/                    # Python class files for evaluation
â”‚   â”œâ”€â”€ _BaseEncoder.py
â”‚   â”œâ”€â”€ Adamax.py
â”‚   â”œâ”€â”€ AgglomerationTransform.py
â”‚   â”œâ”€â”€ AveragePooling1D.py
â”‚   â”œâ”€â”€ AveragePooling2D.py
â”‚   â”œâ”€â”€ AveragePooling3D.py
â”‚   â”œâ”€â”€ BayesianGaussianMixture.py
â”‚   â”œâ”€â”€ Conv.py
â”‚   â”œâ”€â”€ Conv1D.py
â”‚   â”œâ”€â”€ Conv1DTranspose.py
â”‚   â”œâ”€â”€ Conv2D.py
â”‚   â”œâ”€â”€ Conv2DTranspose.py
â”‚   â”œâ”€â”€ Conv3D.py
â”‚   â”œâ”€â”€ Conv3DTranspose.py
â”‚   â”œâ”€â”€ Cropping1D.py
â”‚   â”œâ”€â”€ Cropping2D.py
â”‚   â”œâ”€â”€ Cropping3D.py
â”‚   â”œâ”€â”€ DBSCAN.py
â”‚   â”œâ”€â”€ DepthwiseConv2D.py
â”‚   â”œâ”€â”€ Embedding.py
â”‚   â”œâ”€â”€ Flask.py
â”‚   â”œâ”€â”€ FunctionTransformer.py
â”‚   â”œâ”€â”€ GaussianMixture.py
â”‚   â”œâ”€â”€ GlobalAveragePooling1D.py
â”‚   â”œâ”€â”€ GlobalAveragePooling2D.py
â”‚   â”œâ”€â”€ GlobalAveragePooling3D.py
â”‚   â”œâ”€â”€ GlobalMaxPooling1D.py
â”‚   â”œâ”€â”€ GlobalMaxPooling2D.py
â”‚   â”œâ”€â”€ GlobalMaxPooling3D.py
â”‚   â”œâ”€â”€ GlobalPooling1D.py
â”‚   â”œâ”€â”€ GlobalPooling2D.py
â”‚   â”œâ”€â”€ GlobalPooling3D.py
â”‚   â”œâ”€â”€ GroupTimeSeriesSplit.py
â”‚   â”œâ”€â”€ Kmeans.py
â”‚   â”œâ”€â”€ LabelBinarizer.py
â”‚   â”œâ”€â”€ LabelEncoder.py
â”‚   â”œâ”€â”€ LinearRegression.py
â”‚   â”œâ”€â”€ LogisticRegression.py
â”‚   â”œâ”€â”€ Loss.py
â”‚   â”œâ”€â”€ MaxPooling1D.py
â”‚   â”œâ”€â”€ MaxPooling2D.py
â”‚   â”œâ”€â”€ MaxPooling3D.py
â”‚   â”œâ”€â”€ Metric.py
â”‚   â”œâ”€â”€ MultiLabelBinarizer.py
â”‚   â”œâ”€â”€ OneHotEncoder.py
â”‚   â”œâ”€â”€ OPTICS.py
â”‚   â”œâ”€â”€ OrdinalEncoder.py
â”‚   â”œâ”€â”€ Pooling1D.py
â”‚   â”œâ”€â”€ Pooling2D.py
â”‚   â”œâ”€â”€ Pooling3D.py
â”‚   â”œâ”€â”€ PrincipalComponentAnalysis.py
â”‚   â”œâ”€â”€ RMSprop.py
â”‚   â”œâ”€â”€ SelfTrainingClassifier.py
â”‚   â”œâ”€â”€ SeparableConv.py
â”‚   â”œâ”€â”€ SeparableConv1D.py
â”‚   â”œâ”€â”€ SeparableConv2D.py
â”‚   â”œâ”€â”€ SequentialFeatureSelector.py
â”‚   â”œâ”€â”€ SGD.py
â”‚   â”œâ”€â”€ SoftmaxRegression.py
â”‚   â”œâ”€â”€ TargetEncoder.py
â”‚   â”œâ”€â”€ TransactionEncoder.py
â”‚   â”œâ”€â”€ UpSampling1D.py
â”‚   â”œâ”€â”€ UpSampling2D.py
â”‚   â”œâ”€â”€ UpSampling3D.py
â”‚   â”œâ”€â”€ ZeroPadding1D.py
â”‚   â”œâ”€â”€ ZeroPadding2D.py
â”‚   â””â”€â”€ ZeroPadding3D.py
â”œâ”€â”€ Output/                     # Generated docstring outputs
â”‚   â”œâ”€â”€ code-to-docstring-clean_codegemma-7b-it-dfe.json
â”‚   â”œâ”€â”€ code-to-docstring-clean_codellama-7b-instruct-hf-yig.json
â”‚   â”œâ”€â”€ code-to-docstring-clean_deepseek-coder-7b-instruct-v-riq.json
â”‚   â”œâ”€â”€ code-to-docstring-clean_qwen2-5-coder-7b-instruct-ljc.json
â”‚   â”œâ”€â”€ code-to-docstring-codegemma-7b-it-dfe.json
â”‚   â”œâ”€â”€ code-to-docstring-codellama-34b-instruct-hf-kzi_NEW.json
â”‚   â”œâ”€â”€ code-to-docstring-codellama-7b-instruct-hf-yig.json
â”‚   â”œâ”€â”€ code-to-docstring-COT_codegemma-7b-it-dfe.json
â”‚   â”œâ”€â”€ code-to-docstring-COT_codellama-7b-instruct-hf-yig.json
â”‚   â”œâ”€â”€ code-to-docstring-COT_deepseek-coder-7b-instruct-v-fkg.json
â”‚   â”œâ”€â”€ code-to-docstring-COT_qwen2-5-coder-7b-instruct-ljc.json
â”‚   â”œâ”€â”€ code-to-docstring-deepseek-coder-7b-instruct-v-riq.json
â”‚   â”œâ”€â”€ code-to-docstring-meta-llama-3-8b-instruct-gtq_NEW.json
â”‚   â”œâ”€â”€ code-to-docstring-qwen2-5-7b-instruct-qne_NEW.json
â”‚   â”œâ”€â”€ code-to-docstring-qwen2-5-coder-32b-instruct-qyb_NEW.json
â”‚   â””â”€â”€ code-to-docstring-qwen2-5-coder-7b-instruct-ljc.json
â”œâ”€â”€ backup/                     # Backup files and previous outputs
â”‚   â”œâ”€â”€ clean_code.json
â”‚   â”œâ”€â”€ code-to-comments-clean-code-codellama-7b-instruct-hf-xfe.json
â”‚   â”œâ”€â”€ code-to-comments-clean-code-qwen2-5-coder-7b-instruct-iul.json
â”‚   â”œâ”€â”€ code-to-comments-codegemma-1-1-7b-it-uof.json
â”‚   â”œâ”€â”€ code-to-comments-codellama-7b-instruct-hf-xfe.json
â”‚   â”œâ”€â”€ code-to-comments-deepseek-coder-v2-lite-instr-zxn.json
â”‚   â”œâ”€â”€ code-to-comments-llama-3-2-3b-instruct-msv.json
â”‚   â”œâ”€â”€ code-to-comments-mistral-7b-instruct-v0-3-uly.json
â”‚   â”œâ”€â”€ code-to-comments-qwen2-5-coder-7b-instruct-iul.json
â”‚   â”œâ”€â”€ code-to-docstring-clean-code-codellama-7b-instruct-hf-xfe.json
â”‚   â”œâ”€â”€ code-to-docstring-clean-codellama-13b-instruct-hf-abo.json
â”‚   â”œâ”€â”€ code-to-docstring-clean-deepseek-coder-v2-lite-instr-atp.json
â”‚   â”œâ”€â”€ code-to-docstring-clean-mistral-7b-instruct-v0-3-uly.json
â”‚   â””â”€â”€ code-to-docstring-clean-qwen2-5-coder-7b-instruct-iul-xfe.json
â”œâ”€â”€ Core Scripts/               # Main evaluation and processing scripts
â”‚   â”œâ”€â”€ analysis.py            # Analysis of evaluation results
â”‚   â”œâ”€â”€ cleanup.py             # Code cleaning and preprocessing
â”‚   â”œâ”€â”€ code to comments evaluation.py
â”‚   â”œâ”€â”€ code to comments.py
â”‚   â”œâ”€â”€ code to docstring evaluation.py
â”‚   â”œâ”€â”€ code to docstring.py
â”‚   â”œâ”€â”€ docstring generation.py
â”‚   â”œâ”€â”€ extract_classes.py
â”‚   â”œâ”€â”€ scoring.py
â”‚   â””â”€â”€ sample_class.py
â”œâ”€â”€ Data Files/                 # Input data and processed files
â”‚   â”œâ”€â”€ class collection.csv
â”‚   â”œâ”€â”€ class collection.xlsx
â”‚   â”œâ”€â”€ class_files_df.pkl
â”‚   â”œâ”€â”€ clean_code
â”‚   â”œâ”€â”€ LLM Hard questions.csv
â”‚   â”œâ”€â”€ new scoring.xlsx
â”‚   â””â”€â”€ sdfsdf.csv
â””â”€â”€ Results/                    # Evaluation results and reports
    â”œâ”€â”€ all llm scoring.xlsx
    â””â”€â”€ all_scoiring.pkl
```

## Quick Start

### Installation

```bash
git clone https://github.com/GireeshS22/DocStringEval.git
cd DocStringEval
pip install -r requirements.txt
```

### Basic Usage

1. **Extract Classes**:
```bash
python extract_classes.py
```

2. **Generate Docstrings**:
```bash
python "code to docstring.py"
```

3. **Evaluate Results**:
```bash
python "code to docstring evaluation.py"
```

4. **Analyze Results**:
```bash
python analysis.py
```

## Supported Models

- **CodeLlama** (7B, 13B, 34B)
- **Qwen2.5 Coder** (7B, 32B)
- **DeepSeek Coder** (7B)
- **CodeGemma** (7B)
- **Mistral** (7B)
- **Meta Llama 3** (8B)

## Evaluation Metrics

- **ROUGE-1**: Measures word overlap between generated and reference docstrings
- **BLEU**: Evaluates n-gram precision and brevity penalty
- **Conciseness**: Measures the length efficiency of generated docstrings
- **Custom Metrics**: Domain-specific evaluation criteria

## Project Components

### Data Processing
- `extract_classes.py`: Extracts Python classes from repositories
- `cleanup.py`: Cleans and standardizes code for evaluation
- `class_files_df.pkl`: Processed dataset of Python classes

### Docstring Generation
- `code to docstring.py`: Main script for generating docstrings using LLMs
- `docstring generation.py`: Alternative generation approach
- `Output/`: Directory containing generated docstrings for each model

### Evaluation
- `code to docstring evaluation.py`: Evaluates generated docstrings using ROUGE and BLEU
- `scoring.py`: Custom scoring mechanisms
- `analysis.py`: Comprehensive analysis of evaluation results

### Results
- `all llm scoring.xlsx`: Comprehensive scoring results
- `all_scoiring.pkl`: Pickled scoring data for further analysis

## Results

Detailed evaluation results and model comparisons are available in the `Results/` directory and `Output/` directory.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Citation

If you use this framework in your research, please cite:

```bibtex
@INPROCEEDINGS{11108633,
  author={Sundaram, Gireesh and Venktesh V, Balaji and K B, Sundharakumar},
  booktitle={2025 International Conference on Emerging Technologies in Computing and Communication (ETCC)}, 
  title={DocStringEval: Evaluating the Effectiveness of Language Models for Code Explanation Through DocString Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  keywords={Measurement;Codes;Accuracy;Large language models;Computational modeling;Computer architecture;Benchmark testing;Python;Large Language Models (LLMs);Code Explanation;Docstring Generation;Chain of Thought (CoT) Prompting;Code Summarization},
  doi={10.1109/ETCC65847.2025.11108633}
}
```

## Latest Updates

ðŸš€ **Paper Published!** Our work "DocStringEval: Evaluating the Effectiveness of Language Models for Code Explanation through DocString Generation" has been published at IEEE ETCC 2025! 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">ðŸ§µ ðŸš€ NEW PAPER: &quot;DocStringEval: Evaluating the Effectiveness of Language Models for Code Explanation through DocString Generation&quot;<br>Published at IEEE ETCC 2025! ðŸ“„ <br>With <a href="https://twitter.com/Balajivenky4288?ref_src=twsrc%5Etfw">@Balajivenky4288</a> <br>We benchmarked how well LLMs can explain Python code by generating DocStrings.</p>&mdash; Gireesh (@GireeshS22) <a href="https://twitter.com/GireeshS22/status/1956054735523303689?ref_src=twsrc%5Etfw">August 14, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

## Contact

For questions and support, please:
- Open an issue on [GitHub](https://github.com/GireeshS22/DocStringEval/issues)
- Follow us on [X (Twitter)](https://x.com/GireeshS22) for updates and discussions
