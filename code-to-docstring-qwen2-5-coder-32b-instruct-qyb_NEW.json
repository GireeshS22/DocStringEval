["```python\n\"\"\"\nAdamax optimizer.\n\nThis class implements the Adamax optimizer, which is a variant of the Adam optimizer\nbased on the infinity norm. Adamax is particularly useful for sparse tensors and\ncan be more stable in some cases compared to the standard Adam optimizer.\n\nArguments:\n    learning_rate (float): The learning rate. Defaults to 0.001.\n    beta_1 (float): The exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n    beta_2 (float): The exponential decay rate for the exponentially weighted infinity norm. Defaults to 0.999.\n    epsilon (float): A small constant for numerical stability. Defaults to 1e-07.\n    name (str): Optional name for the operations created when applying gradients. Defaults to 'Adamax'.\n    **kwargs: Keyword arguments. Allowed to be one of `clipnorm` or `clipvalue`.\n        `clipnorm` (float): Clips gradients by global norm. If set, specifies the maximum global norm value.\n        `clipvalue` (float): Clips gradients by value. If set, specifies the maximum absolute value for each gradient.\n\nMethods:\n    _create_slots: Creates slots for the optimizer.\n    _prepare_local: Prepares local variables for the optimizer.\n    _resource_apply_dense: Applies gradients to variables.\n    _resource_apply_sparse: Applies sparse gradients to variables.\n    get_config: Returns the configuration of the optimizer.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer has aggregate gradients.\n\"\"\"\n```", "```python\n\"\"\"\nA transformer that aggregates data based on cluster labels using a specified pooling function.\n\nParameters\n----------\npooling_func : callable, default=np.mean\n    The function to use for pooling data within each cluster. It should take a 2D array and an axis\n    argument and return a 1D array.\n\nlabels_ : array-like of shape (n_samples,)\n    The cluster labels for each sample.\n\nAttributes\n----------\n__metadata_request__inverse_transform : dict\n    Metadata request for the inverse_transform method, indicating that the 'Xt' parameter is unused.\n\nMethods\n-------\ntransform(X)\n    Aggregates the input data X based on the cluster labels using the specified pooling function.\n\ninverse_transform(X=None, *, Xt=None)\n    Reverses the transformation by expanding the aggregated data back to the original shape using the\n    cluster labels.\n\"\"\"\n```", "```python\n\"\"\"\n1D average pooling layer.\n\nThis class implements the 1D average pooling operation, which downsamples the input representation by taking the average\nof the input values over a specified window size (pool_size) and stride. The pooling operation can be configured with\ndifferent padding modes and data formats.\n\nParameters:\n    pool_size (int): Size of the pooling window. Defaults to 2.\n    strides (int or tuple of int): Strides of the pooling operation. If None, it is set to pool_size. Defaults to None.\n    padding (str): One of 'valid' or 'same'. 'valid' means no padding is added, and 'same' results in padding evenly to the\n                   left/right or up/down of the input such that output has the same height/width dimension as the input.\n                   Defaults to 'valid'.\n    data_format (str): One of 'channels_last' (default) or 'channels_first'. Specifies the ordering of the dimensions in the\n                       inputs. 'channels_last' corresponds to inputs with shape (batch, steps, features), while\n                       'channels_first' corresponds to inputs with shape (batch, features, steps).\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"\n```", "```python\n\"\"\"\nApplies average pooling operation for 2D spatial data.\n\nThis class inherits from the Pooling2D class and uses the average pooling function\nto down-sample the input representation by taking the average of the input values\nover a specified window size (pool_size) and stride.\n\nParameters:\n    pool_size (tuple of int): Factors by which to downscale in each dimension (vertical, horizontal).\n                              Default is (2, 2).\n    strides (tuple of int or None): Strides values for the pooling operation.\n                                    If None, it will default to pool_size.\n    padding (str): One of 'valid' or 'same' (case-insensitive).\n                   'valid' means no padding.\n                   'same' results in padding the input such that the output has the same length as the original input.\n                   Default is 'valid'.\n    data_format (str or None): A string, one of 'channels_last' (default) or 'channels_first'.\n                               The ordering of the dimensions in the inputs.\n                               'channels_last' corresponds to inputs with shape (batch_size, height, width, channels)\n                               while 'channels_first' corresponds to inputs with shape (batch_size, channels, height, width).\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"\n```", "```python\n\"\"\"\n3D Average Pooling layer.\n\nThis class implements the 3D average pooling operation, which downsamples the input\nvolume by taking the average of values in the pooling window defined by `pool_size`.\nThe pooling window slides over the input volume with a stride defined by `strides`.\nPadding can be applied to the input volume to control the output size.\n\nParameters:\n    pool_size (tuple of 3 ints): Size of the pooling window in each dimension (depth, height, width).\n    strides (tuple of 3 ints, optional): Stride of the pooling window in each dimension.\n        If not specified, it defaults to `pool_size`.\n    padding (str, optional): Type of padding to apply. Can be 'valid' or 'same'.\n        'valid' means no padding, and 'same' means padding the input such that the output\n        has the same dimensions as the input.\n    data_format (str, optional): The data format of the input. Can be 'channels_first' or 'channels_last'.\n        'channels_first' means the input shape is (batch_size, channels, depth, height, width),\n        and 'channels_last' means the input shape is (batch_size, depth, height, width, channels).\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"\n```", "```python\n\"\"\"\nBayesian Gaussian Mixture Model\n\nA Gaussian Mixture Model (GMM) with Bayesian priors on the parameters. This model\nextends the traditional GMM by incorporating prior distributions on the weights,\nmeans, and covariances, allowing for a more flexible and robust estimation of the\nmodel parameters.\n\nParameters\n----------\nn_components : int, default=1\n    The number of mixture components.\n\ncovariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n    The type of covariance parameters to use:\n    - 'full': each component has its own general covariance matrix.\n    - 'tied': all components share the same general covariance matrix.\n    - 'diag': each component has its own diagonal covariance matrix.\n    - 'spherical': each component has its own single variance.\n\ntol : float, default=0.001\n    The convergence threshold. EM iterations will stop when the lower bound\n    average gain is below this threshold.\n\nreg_covar : float, default=1e-06\n    Non-negative regularization added to the diagonal of covariance.\n    Allows to assure that the covariance matrices are all positive.\n\nmax_iter : int, default=100\n    The number of EM iterations to perform.\n\nn_init : int, default=1\n    The number of initializations to perform. The best results are kept.\n\ninit_params : {'kmeans', 'random'}, default='kmeans'\n    The method used to initialize the weights, the means and the precisions.\n    Must be one of:\n    - 'kmeans' : responsibilities are initialized using kmeans.\n    - 'random' : responsibilities are initialized randomly.\n\nweight_concentration_prior_type : {'dirichlet_process', 'dirichlet_distribution'}, default='dirichlet_process'\n    The type of the weight concentration prior.\n\nweight_concentration_prior : float or None, default=None\n    The parameter of the dirichlet distribution or the dirichlet process that\n    is set as the prior on the weights.\n\nmean_precision_prior : float or None, default=None\n    The precision prior on the means, representing the inverse variance.\n\nmean_prior : array-like, shape (n_features,), default=None\n    The prior on the means distributed according to a Gaussian distribution with\n    this mean and the mean_precision_prior as the precision.\n\ndegrees_of_freedom_prior : float or None, default=None\n    The prior of the number of degrees of freedom on the covariance, distributed\n    as an inverse Wishart distribution.\n\ncovariance_prior : array-like or None, default=None\n    The prior on the covariance, distributed according to an inverse Wishart\n    distribution.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to the method chosen to initialize the\n    parameters (see `init_params`).\n\nwarm_start : bool, default=False\n    If 'warm_start' is True, the solution of the last fitting is used as\n    initialization for the next call of fit(). This can speed up convergence\n    when fit is called several times on similar problems.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints the current initialization and each\n    iteration step. If greater than 1 then it prints also the log probability and\n    the time needed for each step.\n\nverbose_interval : int, default=10\n    Number of iteration done before the next print.\n\"\"\"\n```", "```python\n\"\"\"\nA convolutional layer for processing multi-dimensional inputs.\n\nThis class extends the `Layer` class and provides a flexible convolutional operation\nthat can handle various configurations such as different ranks, filter sizes, strides,\npadding types, and more. It supports both causal and non-causal padding and can be\nconfigured to use different initializers, regularizers, and constraints for the kernel\nand bias.\n\nParameters:\n    rank (int): The rank of the convolution, e.g., 1 for Conv1D, 2 for Conv2D.\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple of ints): The size of the convolution window.\n    strides (int or tuple of ints, optional): The stride of the convolution. Defaults to 1.\n    padding (str, optional): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding.\n        `\"same\"` results in padding the input such that the output has the same length as the input.\n        `\"causal\"` results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1:]. Defaults to `\"valid\"`.\n    data_format (str, optional): A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape\n        `(batch, ..., channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, ...)`.\n    dilation_rate (int or tuple of ints, optional): An integer or tuple/list of n integers, specifying the dilation rate\n        to use for dilated convolution. Defaults to 1.\n    groups (int, optional): A positive integer specifying the number of groups in which the input is split along the channel axis.\n        Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups.\n        Defaults to 1.\n    activation (str or callable, optional): Activation function to use. If you don't specify anything, no activation is applied.\n    use_bias (bool, optional): Whether the layer uses a bias vector. Defaults to `True`.\n    kernel_initializer (str or callable, optional): Initializer for the kernel weights matrix. Defaults to `\"glorot_uniform\"`.\n    bias_initializer (str or callable, optional): Initializer for the bias vector. Defaults to `\"zeros\"`.\n    kernel_regularizer (str or callable, optional): Regularizer function applied to the kernel weights matrix.\n    bias_regularizer (str or callable, optional): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable, optional): Regularizer function applied to the output of the layer (its \"activation\")..\n    kernel_constraint (str or callable, optional): Constraint function applied to the kernel weights matrix.\n    bias_constraint (str or callable, optional): Constraint function applied to the bias vector.\n    trainable (bool, optional): If `True`, the layer's weights will be updated during training. Defaults to `True`.\n    name (str, optional): A string, the name of the layer.\n    conv_op (callable, optional): A custom convolution operation to use. Defaults to `None`.\n    **kwargs: Additional keyword arguments to be passed to the base class.\n\nAttributes:\n    rank (int): The rank of the convolution.\n    filters (int): The number of filters in the convolution.\n    kernel_size (tuple of ints): The size of the convolution window.\n    strides (tuple of ints): The stride of the convolution.\n    padding (str): The type of padding used.\n    data_format (str): The data format used.\n    dilation_rate (tuple of ints): The dilation rate used.\n    groups (int): The number of groups used.\n    activation (callable): The activation function used.\n    use_bias (bool): Whether a bias vector is used.\n    kernel_initializer (callable): The initializer for the kernel weights.\n    bias_initializer (callable): The initializer for the bias vector.\n    kernel_regularizer (callable): The regularizer for the kernel weights.\n    bias_regularizer (callable): The regularizer for the bias vector.\n    activity_regularizer (callable): The regularizer for the output of the layer.\n    kernel_constraint (callable): The constraint for the kernel weights.\n    bias_constraint (callable): The constraint for the bias vector.\n    input_spec (InputSpec): The specification of the input shape.\n    _is_causal (bool): Whether the padding is causal.\n    _channels_first (bool): Whether the data format is channels first.\n    _tf_data_format (str): The TensorFlow data format used.\n    kernel (tf.Variable): The kernel weights of the convolution.\n    bias (tf.Variable): The bias vector", "\"\"\"\nA 1D convolution layer.\n\nThis layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If `use_bias` is True, a bias vector is created and added to the outputs. Finally, if `activation` is not `None`, it is applied to the outputs as well.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple/list of a single int): Specifies the length of the 1D convolution window.\n    strides (int or tuple/list of a single int): Specifies the stride of the convolution. Specifying any stride value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding the input such that the output has the same length as the original input.\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, steps, features)` while `channels_first` corresponds to inputs with shape `(batch, features, steps)`.\n    dilation_rate (int or tuple/list of a single int): Specifies the dilation rate to use for dilated convolution. Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any `strides` value != 1.\n    groups (int): A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with `filters // groups` filters. The output is the concatenation of all the groups.\n    activation (str or callable): Activation function to use. If you don't specify anything, no activation is applied (i.e., \"linear\" activation: `a(x) = x`).\n    use_bias (bool): Whether the layer uses a bias vector.\n    kernel_initializer (str or callable): Initializer for the `kernel` weights matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    kernel_regularizer (str or callable): Regularizer function applied to the `kernel` weights matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\").\n    kernel_constraint (str or callable): Constraint function applied to the `kernel` weights matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"", "```python\n\"\"\"\nA 1D transposed convolution layer (sometimes called Deconvolution).\n\nThis layer performs the operation of transposed convolution, also known as deconvolution, on a 1D input tensor.\nIt is commonly used in the upsampling phase of a convolutional neural network.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple/list of a single int): Size of the convolution kernel.\n    strides (int or tuple/list of a single int): Stride of the convolution.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding (int or tuple/list of a single int): Used to disambiguate output shape (when using `\"same\"` padding).\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n    dilation_rate (int or tuple/list of a single int): Dilation rate for an atrous convolution.\n    activation (str or callable): Activation function to use. If you don't specify anything, no activation is applied.\n    use_bias (bool): Whether the layer uses a bias vector.\n    kernel_initializer (str or callable): Initializer for the `kernel` weights matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    kernel_regularizer (str or callable): Regularizer function applied to the `kernel` weights matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\")..\n    kernel_constraint (str or callable): Constraint function applied to the `kernel` weights matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n\nMethods:\n    build(input_shape): Builds the layer.\n    call(inputs): The logic of the layer, returns the output tensors for given input tensors.\n    compute_output_shape(input_shape): Computes the output shape of the layer.\n    get_config(): Returns the config of the layer.\n\nRaises:\n    ValueError: If the input shape is not 3D, if the channel dimension of the inputs is not defined,\n                or if the stride is not greater than the output padding.\n\"\"\"\n```", "\"\"\"\nA 2D convolution layer.\n\nThis layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\nIt is a subclass of the `Conv` layer and is specifically designed for 2D inputs such as images.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple of 2 ints): Specifies the height and width of the 2D convolution window.\n    strides (tuple of 2 ints, optional): Specifies the strides of the convolution along the height and width.\n        Default is (1, 1).\n    padding (str, optional): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding.\n        `\"same\"` results in padding the input such that the output has the same length as the original input.\n        Default is `\"valid\"`.\n    data_format (str, optional): A string, one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape `(batch, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n        If you never set it, it will be `\"channels_last\"`.\n    dilation_rate (tuple of 2 ints, optional): Specifies the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any `strides` value != 1.\n        Default is (1, 1).\n    groups (int, optional): A positive integer specifying the number of groups in which the input is split along the channel axis.\n        Each group is convolved separately with `filters // groups` filters.\n        The output is the concatenation of all the `groups` results along the channel axis.\n        Input channels and `filters` must both be divisible by `groups`.\n        Default is 1.\n    activation (str or callable, optional): Activation function to use. If you don't specify anything, no activation is applied.\n    use_bias (bool, optional): Whether the layer uses a bias vector.\n    kernel_initializer (str or callable, optional): Initializer for the `kernel` weights matrix.\n    bias_initializer (str or callable, optional): Initializer for the bias vector.\n    kernel_regularizer (str or callable, optional): Regularizer function applied to the `kernel` weights matrix.\n    bias_regularizer (str or callable, optional): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable, optional): Regularizer function applied to the output of the layer (its \"activation\").\n    kernel_constraint (str or callable, optional): Constraint function applied to the `kernel` weights matrix.\n    bias_constraint (str or callable, optional): Constraint function applied to the bias vector.\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"", "```python\n\"\"\"\nA 2D transposed convolution (also known as deconvolution) layer.\n\nThis layer performs the operation of transposed convolution, which is the reverse of a convolution operation.\nIt is often used for upsampling in tasks such as image segmentation.\n\nParameters:\n    filters (int): The number of output filters in the convolution.\n    kernel_size (int or tuple of 2 ints): The height and width of the 2D convolution window.\n    strides (tuple of 2 ints): The stride of the sliding window for each dimension of input.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding.\n                   `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input.\n                   When `padding=\"same\"` and `strides=1`, the output has the same size as the input.\n    output_padding (tuple of 2 ints): Used to disambiguate output shape (when using `\"same\"` padding).\n    data_format (str): A string, one of `\"channels_last\"` (default) or `\"channels_first\"`.\n                       The ordering of the dimensions in the inputs.\n                       `\"channels_last\"` corresponds to inputs with shape `(batch, height, width, channels)`\n                       while `\"channels_first\"` corresponds to inputs with shape `(batch, channels, height, width)`.\n    dilation_rate (tuple of 2 ints): An integer or tuple/list of 2 integers, specifying the dilation rate\n                                   to use for dilated convolution.\n    activation (str or callable): Activation function to use. If you don't specify anything, no activation is applied.\n    use_bias (bool): Whether the layer uses a bias vector.\n    kernel_initializer (str or callable): Initializer for the `kernel` weights matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    kernel_regularizer (str or callable): Regularizer function applied to the `kernel` weights matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\")..\n    kernel_constraint (str or callable): Constraint function applied to the kernel matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n\nMethods:\n    build(input_shape): Builds the layer according to the specified input shape.\n    call(inputs): The logic of the layer, returns the output tensors for given input tensors.\n    compute_output_shape(input_shape): Computes the output shape of the layer for a given input shape.\n    get_config(): Returns the config of the layer.\n\nRaises:\n    ValueError: If the stride is less than or equal to the output padding.\n    ValueError: If the input shape does not have rank 4.\n    ValueError: If the channel dimension of the inputs is not defined.\n\"\"\"\n```", "\"\"\"\n3D convolution layer (e.g. spatial convolution over volumes).\n\nThis layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\nIt extends the base `Conv` class to handle 3D inputs, such as volumetric images.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple of 3 ints): Specifies the depth, height, and width of the 3D convolution window.\n    strides (tuple of 3 ints, optional): Strides of the convolution along the depth, height, and width.\n        Default is (1, 1, 1).\n    padding (str, optional): One of `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input.\n        When `padding=\"same\"` and `strides=1`, the output has the same size as the input.\n        Default is `\"valid\"`.\n    data_format (str, optional): A string, one of `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape `(batch, depth, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape `(batch, channels, depth, height, width)`.\n        It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n        If you never set it, it will be `\"channels_last\"`.\n    dilation_rate (tuple of 3 ints, optional): Dilation rate for each spatial dimension of the input.\n        Can be a single integer to specify the same value for all spatial dimensions.\n        Default is (1, 1, 1).\n    groups (int, optional): A positive integer specifying the number of groups in which the input is split along the channel axis.\n        Each group is convolved separately with `filters // groups` filters.\n        The output is the concatenation of all the groups.\n        Note that this requires a static number of filters per group, which means `filters` must be divisible by `groups`.\n        Default is 1.\n    activation (str or callable, optional): Activation function to use.\n        If you don't specify anything, no activation is applied (i.e., \"linear\" activation: `a(x) = x`).\n        Default is `None`.\n    use_bias (bool, optional): Whether the layer uses a bias vector.\n        Default is `True`.\n    kernel_initializer (str or callable, optional): Initializer for the `kernel` weights matrix.\n        Default is `\"glorot_uniform\"`.\n    bias_initializer (str or callable, optional): Initializer for the bias vector.\n        Default is `\"zeros\"`.\n    kernel_regularizer (str or callable, optional): Regularizer function applied to the `kernel` weights matrix.\n        Default is `None`.\n    bias_regularizer (str or callable, optional): Regularizer function applied to the bias vector.\n        Default is `None`.\n    activity_regularizer (str or callable, optional): Regularizer function applied to the output of the layer (its \"activation\").\n        Default is `None`.\n    kernel_constraint (str or callable, optional): Constraint function applied to the `kernel` weights matrix.\n        Default is `None`.\n    bias_constraint (str or callable, optional): Constraint function applied to the bias vector.\n        Default is `None`.\n    **kwargs: Additional keyword arguments passed to the base `Conv` class.\n\"\"\"", "```python\n\"\"\"\n3D transposed convolution layer (sometimes called deconvolution).\n\nThis layer performs the operation of transposed convolution, also known as\ndeconvolution, on a 3D input tensor. It is often used for upsampling in\n3D convolutional neural networks.\n\nArguments:\n    filters: Integer, the dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 3 integers, specifying the depth, height, and width of the 3D convolution window.\n    strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    output_padding: An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions.\n    data_format: A string, one of `\"channels_last\"` (default) or `\"channels_first\"`. The ordering of the dimensions in the inputs. `\"channels_last\"` corresponds to inputs with shape `(batch_size, depth, height, width, channels)` while `\"channels_first\"` corresponds to inputs with shape `(batch_size, channels, depth, height, width)`.\n    dilation_rate: An integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use. If you don't specify anything, no activation is applied (i.e., \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")..\n    kernel_constraint: Constraint function applied to the `kernel` weights matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\nInput shape:\n    5D tensor with shape: `(batch_size, channels, depth, rows, cols)` if `data_format='channels_first'`\n    or 5D tensor with shape: `(batch_size, depth, rows, cols, channels)` if `data_format='channels_last'`.\n\nOutput shape:\n    5D tensor with shape: `(batch_size, filters, new_depth, new_rows, new_cols)` if `data_format='channels_first'`\n    or 5D tensor with shape: `(batch_size, new_depth, new_rows, new_cols, filters)` if `data_format='channels_last'`.\n    `new_depth`, `new_rows`, and `new_cols` values might have changed due to padding.\n\"\"\"\n```", "\"\"\"\nA 1D layer that crops a specified number of units from the beginning and end of the input tensor.\n\nParameters:\n    cropping (tuple of int, optional): A tuple of two integers, where the first integer is the number of units to crop from the start of the input tensor, and the second integer is the number of units to crop from the end. Defaults to (1, 1).\n\nAttributes:\n    cropping (tuple of int): Normalized tuple of two integers representing the cropping amounts.\n    input_spec (InputSpec): Specifies the expected input shape, which must be a 3D tensor.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): Applies the cropping operation to the input tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"", "```python\n\"\"\"\nA 2D layer that crops the input tensor along the height and width dimensions.\n\nThe `Cropping2D` layer allows you to specify how many units should be trimmed from the top, bottom, left, and right of the input tensor. The cropping can be symmetric or asymmetric.\n\nParameters:\n    cropping (int or tuple of 2 ints or tuple of 2 tuples of 2 ints): \n        - If an int, the same symmetric cropping is applied to height and width.\n        - If a tuple of 2 ints, interpreted as two different symmetric cropping values for height and width.\n        - If a tuple of 2 tuples of 2 ints, interpreted as ((top_crop, bottom_crop), (left_crop, right_crop)).\n    data_format (str): One of `\"channels_first\"`, `\"channels_last\"`. Specifies the ordering of the dimensions in the inputs. `\"channels_last\"` corresponds to inputs with shape `(batch, height, width, channels)` while `\"channels_first\"` corresponds to inputs with shape `(batch, channels, height, width)`. Defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, it will be `\"channels_last\"`.\n\nRaises:\n    ValueError: If the `cropping` parameter is not an int, a tuple of 2 ints, or a tuple of 2 tuples of 2 ints.\n\"\"\"\n```", "```python\n\"\"\"\nA 3D layer that crops a volume (last 3 dimensions) of a 5D tensor.\n\nThe cropping is specified for the depth, height, and width dimensions.\nIt can be a symmetric crop or an asymmetric crop.\n\nParameters:\n    cropping: int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n        - If int, the same symmetric cropping is applied to depth, height, and width.\n        - If tuple of 3 ints, the symmetric cropping is applied to each dimension.\n        - If tuple of 3 tuples of 2 ints, the asymmetric cropping is applied to each dimension.\n    data_format: A string, one of 'channels_first' or 'channels_last'.\n        The ordering of the dimensions in the inputs.\n        'channels_first' corresponds to inputs with shape (batch_size, channels, depth, height, width),\n        while 'channels_last' corresponds to inputs with shape (batch_size, depth, height, width, channels).\n        It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n        If you never set it, it will be 'channels_last'.\n\nRaises:\n    ValueError: If `cropping` is not an int, a tuple of 3 ints, or a tuple of 3 tuples of 2 ints.\n\"\"\"\n```", "```python\n\"\"\"\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering algorithm.\n\nDBSCAN groups together points that are closely packed together, marking as outliers points that lie alone in low-density regions.\nThis class implements the DBSCAN algorithm with various parameters to control the clustering process.\n\nParameters:\n    eps (float): The maximum distance between two samples for them to be considered as in the same neighborhood.\n    min_samples (int): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n    metric (str or callable): The metric to use when calculating distance between instances in a feature array.\n    metric_params (dict, optional): Additional keyword arguments for the metric function.\n    algorithm (str): The algorithm to be used by the NearestNeighbors module to compute pointwise distances.\n    leaf_size (int): Leaf size passed to BallTree or KDTree.\n    p (float, optional): The power parameter for the Minkowski metric.\n    n_jobs (int, optional): The number of parallel jobs to run.\n\nAttributes:\n    core_sample_indices_ (ndarray): Indices of core samples.\n    components_ (ndarray): Copy of each core sample found by training.\n    labels_ (ndarray): Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\nMethods:\n    fit(X, y=None, sample_weight=None): Compute DBSCAN clustering.\n    fit_predict(X, y=None, sample_weight=None): Perform clustering on X and returns cluster labels.\n    _more_tags(): Get tags related to the estimator.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D depthwise convolution layer.\n\nThis layer performs a depthwise spatial convolution over an input tensor.\nDepthwise convolution is a type of convolution where each input channel is convolved with a separate kernel,\nand the outputs are concatenated along the channel axis.\n\nParameters:\n    kernel_size (int or tuple of 2 ints): Size of the convolution kernel.\n    strides (int or tuple of 2 ints): Strides of the convolution along the height and width.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive).\n    depth_multiplier (int): The number of depthwise convolution output channels for each input channel.\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n    dilation_rate (int or tuple of 2 ints): Dilation rate for each spatial dimension.\n    activation (str or callable): Activation function to use.\n    use_bias (bool): Whether the layer uses a bias vector.\n    depthwise_initializer (str or callable): Initializer for the depthwise kernel weights matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    depthwise_regularizer (str or callable): Regularizer function applied to the depthwise kernel weights matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\")..\n    depthwise_constraint (str or callable): Constraint function applied to the depthwise kernel weights matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n\nMethods:\n    build(input_shape): Builds the layer.\n    call(inputs): Applies the layer to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer.\n    get_config(): Returns the config of the layer.\n\nRaises:\n    ValueError: If the input shape is not compatible with the layer.\n\"\"\"\n```", "```python\n\"\"\"\nAn Embedding layer that turns positive integers (indexes) into dense vectors of fixed size.\n\nThis layer can only be used as the first layer in a model (you can't stack it on top of a non-embedding layer).\n\nArguments:\n    input_dim (int): Size of the vocabulary, i.e. maximum integer index + 1.\n    output_dim (int): Dimension of the dense embedding.\n    embeddings_initializer (str or keras.initializers.Initializer): Initializer for the `embeddings` matrix.\n    embeddings_regularizer (str or keras.regularizers.Regularizer): Regularizer function applied to the `embeddings` matrix.\n    activity_regularizer (str or keras.regularizers.Regularizer): Regularizer function applied to the output of the layer (its \"activation\").\n    embeddings_constraint (str or keras.constraints.Constraint): Constraint function applied to the `embeddings` matrix.\n    mask_zero (bool): Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is `True`, then all subsequent layers in the model need to support masking or an exception will be raised.\n    input_length (int): Length of input sequences, when it is constant. This argument is required if you are going to connect `Flatten` then `Dense` layers upstream (without it, the shape of the dense outputs cannot be computed).\n\nInput shape:\n    2D tensor with shape: `(batch_size, sequence_length)`.\n\nOutput shape:\n    3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n\"\"\"\n```", "```python\n\"\"\"\nThe Flask class is a WSGI application that provides the core functionality for building web applications.\nIt inherits from the App class and includes methods for handling requests, routing, and serving static files.\nThe class manages configuration, request and response objects, and provides a command-line interface for running the application.\n\nAttributes:\n    default_config (ImmutableDict): Default configuration settings for the Flask application.\n    request_class (type[Request]): The class used to create request objects.\n    response_class (type[Response]): The class used to create response objects.\n    session_interface (SessionInterface): The interface for handling sessions.\n\nMethods:\n    __init__: Initializes the Flask application with various configuration options.\n    get_send_file_max_age: Returns the maximum age for cache control headers on static files.\n    send_static_file: Sends a static file from the static folder.\n    open_resource: Opens a resource file relative to the application's root path.\n    open_instance_resource: Opens a resource file relative to the application's instance path.\n    create_jinja_environment: Creates a Jinja environment for rendering templates.\n    create_url_adapter: Creates a URL adapter for URL routing.\n    raise_routing_exception: Raises a routing exception if necessary.\n    update_template_context: Updates the template context with additional variables.\n    make_shell_context: Creates a shell context for interactive debugging.\n    run: Runs the Flask application on a local development server.\n    test_client: Creates a test client for testing the application.\n    test_cli_runner: Creates a CLI runner for testing the application's command-line interface.\n    handle_http_exception: Handles HTTP exceptions.\n    handle_user_exception: Handles user-defined exceptions.\n    handle_exception: Handles exceptions in general.\n    log_exception: Logs exceptions to the application's logger.\n    dispatch_request: Dispatches a request to the appropriate view function.\n    full_dispatch_request: Handles the full request dispatching process.\n    finalize_request: Finalizes the response before sending it to the client.\n    make_default_options_response: Creates a default response for OPTIONS requests.\n    ensure_sync: Ensures that a function is synchronous.\n    async_to_sync: Converts an asynchronous function to a synchronous one.\n    url_for: Generates a URL for a given endpoint.\n    make_response: Creates a response object from the return value of a view function.\n    preprocess_request: Preprocesses the request before dispatching it.\n    process_response: Processes the response before sending it to the client.\n    do_teardown_request: Performs teardown operations for the request context.\n    do_teardown_appcontext: Performs teardown operations for the application context.\n    app_context: Creates an application context.\n    request_context: Creates a request context.\n    test_request_context: Creates a test request context.\n    wsgi_app: The WSGI application callable.\n    __call__: The callable interface for the Flask application.\n\"\"\"\n```", "```python\n\"\"\"\nA transformer that applies a user-defined function to the input data.\n\nParameters\n----------\nfunc : callable, default=None\n    The function to be applied to the input data. If None, the identity function is used.\n\ninverse_func : callable, default=None\n    The function to be applied to the transformed data to invert the transformation. If None, the inverse transformation is not available.\n\nvalidate : bool, default=False\n    If True, the input data will be validated using the `check_array` function.\n\naccept_sparse : bool, default=False\n    If True, the transformer will accept sparse input data.\n\ncheck_inverse : bool, default=True\n    If True, the transformer will check if the inverse transformation is the inverse of the forward transformation.\n\nfeature_names_out : callable, {'one-to-one'}, default=None\n    A function that returns the output feature names. If 'one-to-one', the output feature names are the same as the input feature names.\n\nkw_args : dict, default=None\n    Additional keyword arguments to be passed to the `func`.\n\ninv_kw_args : dict, default=None\n    Additional keyword arguments to be passed to the `inverse_func`.\n\nMethods\n-------\nfit(X, y=None)\n    Fit the transformer to the input data.\n\ntransform(X)\n    Apply the transformation to the input data.\n\ninverse_transform(X)\n    Apply the inverse transformation to the input data.\n\nget_feature_names_out(input_features=None)\n    Get the output feature names.\n\nset_output(transform=None)\n    Set the output configuration for the transformer.\n\nNotes\n-----\nThe transformer can be used to apply any custom function to the input data, making it a flexible tool for data preprocessing.\n\"\"\"\n```", "```python\n\"\"\"\nGaussian Mixture Model\n\nA Gaussian Mixture Model (GMM) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. This class extends the BaseMixture class to implement the GMM algorithm.\n\nParameters:\n    n_components (int, default=1): The number of mixture components.\n    covariance_type (str, default='full'): The type of covariance parameters to use. Supported options are:\n        'full': each component has its own general covariance matrix.\n        'tied': all components share the same general covariance matrix.\n        'diag': each component has its own diagonal covariance matrix.\n        'spherical': each component has its own single variance.\n    tol (float, default=0.001): The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.\n    reg_covar (float, default=1e-06): Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.\n    max_iter (int, default=100): The number of EM iterations to perform.\n    n_init (int, default=1): The number of initializations to perform. The best results are kept.\n    init_params (str, default='kmeans'): The method used to initialize the weights, the means and the precisions. Supported options are:\n        'kmeans': responsibilities are initialized using kmeans.\n        'random': responsibilities are initialized randomly.\n    weights_init (array-like, default=None): The user-provided initial weights, defaults to None. If it is None, weights are initialized using the 'init_params'.\n    means_init (array-like, default=None): The user-provided initial means, defaults to None. If it is None, means are initialized using the 'init_params'.\n    precisions_init (array-like, default=None): The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it is None, precisions are initialized using the 'init_params'.\n    random_state (int, RandomState instance or None, default=None): Controls the random seed given to the method chosen to initialize the parameters (see `init_params`).\n    warm_start (bool, default=False): If 'warm_start' is True, the solution of the last fitting is used as initialization for the next call to fit. This can speed up convergence when fit is called several times on similar problems.\n    verbose (int, default=0): Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.\n    verbose_interval (int, default=10): Number of iteration done before the next print.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D global average pooling layer.\n\nThis layer computes the global average of the input tensor along the temporal dimension.\nIt is useful for reducing the temporal dimensionality of the input tensor to a fixed-size vector.\n\nArgs:\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape `(batch, steps, features)`\n        while `channels_first` corresponds to inputs with shape `(batch, features, steps)`.\n\nAttributes:\n    supports_masking (bool): Whether the layer supports masking.\n\nMethods:\n    call(inputs, mask=None): Computes the global average pooling of the input tensor.\n        If a mask is provided, it is applied to the input tensor before computing the average.\n    compute_mask(inputs, mask=None): Returns `None` as this layer does not support masking.\n\"\"\"\n```", "```python\n\"\"\"\nPerforms global average pooling operation for 2D inputs (e.g. images).\n\nThis layer computes the average of all elements in the spatial dimensions\n(height and width) of the input tensor. The result is a tensor with the same\nnumber of channels as the input, but with a spatial size of 1x1.\n\nArgs:\n    inputs: A 4D tensor with shape:\n            - If `data_format` is 'channels_last': `(batch_size, height, width, channels)`\n            - If `data_format` is 'channels_first': `(batch_size, channels, height, width)`\n\nReturns:\n    A 2D tensor with shape `(batch_size, channels)` if `keepdims` is False.\n    A 4D tensor with shape `(batch_size, 1, 1, channels)` if `keepdims` is True.\n\"\"\"\n```", "```python\n\"\"\"\nApplies global average pooling operation for 3D data.\n\nThis layer computes the average of all elements in the spatial dimensions (height, width, depth)\nof the input tensor. The `data_format` attribute determines the order of the dimensions in the input tensor.\n\nArgs:\n    inputs: A 5D tensor with shape:\n            - If `data_format` is 'channels_last': (batch_size, height, width, depth, channels)\n            - If `data_format` is 'channels_first': (batch_size, channels, height, width, depth)\n\nReturns:\n    A 2D tensor with shape:\n    - If `keepdims` is True: (batch_size, 1, 1, 1, channels) if `data_format` is 'channels_last',\n      or (batch_size, channels, 1, 1, 1) if `data_format` is 'channels_first'.\n    - If `keepdims` is False: (batch_size, channels)\n\"\"\"\n```", "```python\n\"\"\"\nApplies global max pooling operation for 1D temporal data.\n\nThis layer computes the maximum value along the temporal dimension (axis 1 if `data_format` is 'channels_last',\naxis 2 if `data_format` is 'channels_first') of the input tensor.\n\nArguments:\n    inputs: A 3D tensor with shape: `(batch_size, steps, features)` if `data_format` is 'channels_last',\n            or `(batch_size, features, steps)` if `data_format` is 'channels_first'.\n\nReturns:\n    A 2D tensor with shape: `(batch_size, features)` if `keepdims` is False,\n    or `(batch_size, 1, features)` if `keepdims` is True.\n\"\"\"\n```", "```python\n\"\"\"\nPerforms global max pooling operation for 2D inputs.\n\nThis class inherits from `GlobalPooling2D` and implements the `call` method to perform\na global max pooling operation on the input tensor. The pooling is performed across the\nspatial dimensions (height and width) of the input tensor, and the result can be configured\nto either keep or remove the reduced dimensions based on the `keepdims` attribute.\n\nArgs:\n    inputs: A 4D tensor with shape:\n            - If `data_format` is 'channels_last': (batch_size, height, width, channels)\n            - If `data_format` is 'channels_first': (batch_size, channels, height, width)\n\nReturns:\n    A 2D tensor with shape:\n    - If `keepdims` is True: (batch_size, 1, 1, channels) or (batch_size, channels, 1, 1)\n    - If `keepdims` is False: (batch_size, channels)\n\"\"\"\n```", "```python\n\"\"\"\nApplies global max pooling operation for 3D data.\n\nThis class extends `GlobalPooling3D` and performs a global max pooling operation\non the input tensor. The pooling is performed along the spatial dimensions\n(height, width, depth) of the input tensor. The behavior of the pooling operation\ncan be adjusted based on the `data_format` attribute, which specifies the\nordering of the dimensions in the input tensor.\n\nArgs:\n    inputs: A 5D tensor with shape:\n            - If `data_format` is 'channels_last': (batch_size, depth, height, width, channels)\n            - If `data_format` is 'channels_first': (batch_size, channels, depth, height, width)\n\nReturns:\n    A 2D tensor with shape:\n    - If `keepdims` is True: (batch_size, 1, 1, 1, channels) if 'channels_last',\n      or (batch_size, channels, 1, 1, 1) if 'channels_first'\n    - If `keepdims` is False: (batch_size, channels)\n\"\"\"\n```", "```python\n\"\"\"\nA 1D global pooling layer for temporal data.\n\nThis layer performs global pooling operations on 1D temporal data.\nIt supports both 'channels_first' and 'channels_last' data formats and\nprovides an option to keep the dimensions of the output tensor.\n\nParameters:\n    data_format (str): A string, one of 'channels_first' or 'channels_last'.\n        Specifies the ordering of the dimensions in the inputs.\n        'channels_last' corresponds to inputs with shape (batch, steps, features),\n        while 'channels_first' corresponds to inputs with shape (batch, features, steps).\n        Default is 'channels_last'.\n    keepdims (bool): A boolean, whether to keep the temporal dimension or not.\n        If `keepdims` is `True`, the output will have the same number of dimensions as the input.\n        Default is `False`.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The logic of the layer. This method should not be called directly.\n        It is intended to be overridden by subclasses.\n    get_config(): Returns the config of the layer.\n        A layer config is a Python dictionary (serializable) containing the configuration of a layer.\n        The same layer can be reinstantiated later (without its trained weights) from this configuration.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D global pooling layer.\n\nThis layer performs global pooling operations on 4D inputs (batch_size, height, width, channels) or\n(batch_size, channels, height, width) depending on the `data_format` parameter. The pooling operation\nreduces the spatial dimensions (height and width) to 1, optionally keeping the dimensions or flattening them.\n\nParameters:\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs.\n    keepdims (bool): A boolean, whether to keep the spatial dimensions or not. If `True`, the output will have the same number of dimensions as the input, with the spatial dimensions reduced to 1. If `False`, the spatial dimensions are removed.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The logic of the layer. This method should be overridden by subclasses.\n    get_config(): Returns the config of the layer.\n\nRaises:\n    NotImplementedError: The `call` method is not implemented in this base class and should be overridden by subclasses.\n\"\"\"\n```", "```python\n\"\"\"\nA 3D global pooling layer.\n\nThis layer performs global pooling operations on 3D input data. It supports both 'channels_last' and 'channels_first' data formats.\nThe `keepdims` parameter allows the output to retain the spatial dimensions or not.\n\nParameters:\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to inputs with shape `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n    keepdims (bool): A boolean, whether to keep the spatial dimensions or not. If `True`, the output will have the same number of dimensions as the input.\n        If `False`, the spatial dimensions will be collapsed into a single dimension.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The logic of the layer. This method should not be called directly.\n    get_config(): Returns the config of the layer.\n        A layer config is a Python dictionary (serializable) containing the configuration of a layer.\n        The same layer can be reinstantiated later (without its trained weights) from this configuration.\n\"\"\"\n```", "```python\n\"\"\"\nA class for splitting time series data into training and testing sets based on groups.\nThe splits can be either rolling or expanding, with options to specify the size of the train and test sets,\nthe number of splits, the gap between train and test sets, and the shift size for each split.\n\nParameters:\n    test_size (int): The size of the test set for each split.\n    train_size (int, optional): The size of the train set for each split. If not provided, n_splits must be specified.\n    n_splits (int, optional): The number of splits to generate. If not provided, train_size must be specified.\n    gap_size (int, default=0): The number of groups to skip between the train and test sets.\n    shift_size (int, default=1): The number of groups to shift for each subsequent split.\n    window_type (str, default='rolling'): The type of window to use for splitting. Can be 'rolling' or 'expanding'.\n\nMethods:\n    split(X, y=None, groups=None): Generate indices to split the data into training and test sets.\n    get_n_splits(X=None, y=None, groups=None): Return the number of splits.\n    _calculate_split_params(): Calculate the necessary parameters for splitting the data.\n\"\"\"\n```", "```python\n\"\"\"\nKmeans class for performing k-means clustering. Inherits from _BaseModel, _Cluster, and _IterativeModel.\n\nParameters:\n    k (int): The number of clusters to form.\n    max_iter (int, optional): Maximum number of iterations of the k-means algorithm for a single run. Default is 10.\n    convergence_tolerance (float, optional): Relative tolerance with regards to inertia to declare convergence. Default is 1e-05.\n    random_seed (int, optional): Seed used by the random number generator for reproducibility. Default is None.\n    print_progress (int, optional): If set to a positive integer, progress will be printed every `print_progress` iterations. Default is 0.\n\nAttributes:\n    k (int): Number of clusters.\n    max_iter (int): Maximum number of iterations.\n    convergence_tolerance (float): Convergence tolerance.\n    random_seed (int): Seed for random number generation.\n    print_progress (int): Frequency of progress printing.\n    _is_fitted (bool): Indicates if the model has been fitted.\n    iterations_ (int): Number of iterations performed during the last fit.\n    centroids_ (np.ndarray): Coordinates of cluster centers.\n    clusters_ (dict): Mapping of cluster indices to sample indices.\n\nMethods:\n    _fit(X, init_params=True): Fits the k-means model to the data X.\n    _get_cluster_idx(X, centroids): Yields the closest cluster index for each sample in X.\n    _predict(X): Predicts the closest cluster for each sample in X.\n\"\"\"\n```", "```python\n\"\"\"\nLabelBinarizer is a transformer that binarizes labels in a one-vs-all fashion.\n\nParameters:\n    neg_label (int, default=0): The label to use for negative samples.\n    pos_label (int, default=1): The label to use for positive samples.\n    sparse_output (bool, default=False): Whether the output should be a sparse matrix.\n\nAttributes:\n    classes_ (array of shape (n_classes,)): Holds the label for each class.\n    y_type_ (str): Represents the type of the target data as identified by `type_of_target`.\n    sparse_input_ (bool): Indicates whether the input data was sparse.\n\nMethods:\n    fit(y): Fit label binarizer.\n    fit_transform(y): Fit label binarizer and return the transformed labels.\n    transform(y): Transform labels to a one-vs-all binary representation.\n    inverse_transform(Y, threshold=None): Transform binary labels back to original labels.\n    _more_tags(): Returns a dictionary with metadata describing the estimator.\n\nRaises:\n    ValueError: If `neg_label` is not strictly less than `pos_label`.\n    ValueError: If sparse binarization is requested with non-zero `pos_label` or non-zero `neg_label`.\n    ValueError: If the target data is multioutput.\n    ValueError: If the input data has 0 samples.\n    ValueError: If the object was not fitted with multilabel input and multilabel data is provided.\n\"\"\"\n```", "\"\"\"\nA utility class to encode target labels with value between 0 and n_classes-1.\n\nThis class implements a label encoder that can be used to convert categorical labels into numerical format, which is often required for machine learning algorithms. It provides methods to fit the encoder to a set of labels, transform new labels into the encoded format, and inverse transform encoded labels back to their original form.\n\nParameters:\n    auto_wrap_output_keys (None): This parameter is not used in the provided class definition and seems to be a placeholder or an error.\n\nMethods:\n    fit(y): Fit label encoder.\n    fit_transform(y): Fit label encoder and return encoded labels.\n    transform(y): Transform labels to normalized encoding.\n    inverse_transform(y): Transform labels back to original encoding.\n    _more_tags(): Returns a dictionary with metadata tags for the estimator.\n\nAttributes:\n    classes_ (array of shape (n_classes,)): Holds the unique labels found in the data.\n\"\"\"", "```python\n\"\"\"\nA class for performing linear regression using various methods including direct, stochastic gradient descent (SGD),\nsingular value decomposition (SVD), and QR decomposition.\n\nParameters:\n    method (str): The method to use for fitting the model. Options are 'direct', 'sgd', 'svd', and 'qr'.\n                  Default is 'direct'.\n    eta (float): The learning rate for the SGD method. Default is 0.01.\n    epochs (int): The number of passes over the training dataset for the SGD method. Default is 50.\n    minibatches (int or None): The number of minibatches for the SGD method. If None, the entire dataset is used.\n                             Default is None.\n    random_seed (int or None): The seed for the random number generator for reproducibility. Default is None.\n    print_progress (int): Controls the verbosity of the training process. If set to 1, prints progress for each epoch.\n                        Default is 0.\n\nAttributes:\n    eta (float): The learning rate.\n    epochs (int): The number of epochs.\n    minibatches (int or None): The number of minibatches.\n    random_seed (int or None): The random seed.\n    print_progress (int): The verbosity level.\n    _is_fitted (bool): Indicates whether the model has been fitted.\n    method (str): The fitting method.\n    b_ (np.ndarray): The bias term.\n    w_ (np.ndarray): The weight vector.\n    cost_ (list): The cost values for each epoch during training.\n    init_time_ (float): The time at which training began.\n\nMethods:\n    _fit(X, y, init_params): Fits the model to the training data.\n    _normal_equation(X, y): Computes the weights and bias using the normal equation.\n    _net_input(X): Computes the net input (linear combination of weights and features).\n    _predict(X): Predicts the target values for the input data.\n    _sum_squared_error_cost(y, y_val): Computes the sum of squared errors cost.\n\"\"\"\n```", "```python\n\"\"\"\nLogistic Regression classifier.\n\nThis class implements a logistic regression model for binary classification. It inherits from\n_BaseModel, _IterativeModel, and _Classifier, and provides methods for fitting the model to data,\nmaking predictions, and computing probabilities.\n\nParameters:\n    eta (float): Learning rate (default: 0.01).\n    epochs (int): Number of passes over the training dataset (default: 50).\n    l2_lambda (float): Regularization strength (default: 0.0).\n    minibatches (int): Number of minibatches for stochastic gradient descent (default: 1).\n    random_seed (int): Random seed for reproducibility (default: None).\n    print_progress (int): Verbosity level (default: 0).\n\nAttributes:\n    eta (float): Learning rate.\n    epochs (int): Number of epochs.\n    l2_lambda (float): Regularization strength.\n    minibatches (int): Number of minibatches.\n    random_seed (int): Random seed.\n    print_progress (int): Verbosity level.\n    _is_fitted (bool): Flag indicating if the model is fitted.\n    b_ (np.ndarray): Bias term.\n    w_ (np.ndarray): Weights.\n    cost_ (list): List of cost values for each epoch.\n    init_time_ (float): Time when the model fitting started.\n\"\"\"\n```", "```python\n\"\"\"\nBase class for Keras loss functions.\n\nThis class provides a framework for defining custom loss functions in Keras. It includes methods for initializing the loss function,\ncomputing the loss given true and predicted values, and handling optional sample weights. Subclasses should implement the `call` method\nto define the specific loss computation.\n\nAttributes:\n    name (str): The name of the loss function.\n    reduction (str): The reduction method to apply to the loss values. Options include 'none', 'sum', and 'sum_over_batch_size'.\n    dtype (tf.DType): The data type used for the loss computation.\n\nMethods:\n    __call__(y_true, y_pred, sample_weight=None): Computes the loss for the given true and predicted values, optionally applying sample weights.\n    call(y_true, y_pred): Abstract method to be implemented by subclasses for computing the loss.\n    get_config(): Returns the configuration of the loss function as a dictionary.\n    from_config(cls, config): Creates a loss function instance from its configuration.\n    _obj_type(): Returns the type of the object, which is 'Loss'.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D max pooling layer.\n\nThis class performs max pooling on a 1D input tensor. Max pooling reduces the spatial dimensions of the input by taking the maximum value over a specified window (pool_size) and sliding it across the input with a specified stride.\n\nParameters:\n    pool_size (int): The size of the pooling window. Defaults to 2.\n    strides (int or tuple of int): The stride of the pooling window. If None, it is set to pool_size. Defaults to None.\n    padding (str): The padding method to use. Options are 'valid' (no padding) and 'same' (padding to maintain the input size). Defaults to 'valid'.\n    data_format (str): The data format of the input tensor. Options are 'channels_last' (default) and 'channels_first'. Defaults to 'channels_last'.\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"\n```", "```python\n\"\"\"\nMaxPooling2D layer for downsampling the input representation by taking the maximum value over a spatial neighborhood.\n\nParameters:\n    pool_size (tuple of int): Factors by which to downscale in each dimension. Default is (2, 2).\n    strides (tuple of int or None): Strides values. If None, it will default to `pool_size`. Default is None.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. Default is `\"valid\"`.\n    data_format (str or None): A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, it will be `\"channels_last\"`.\n    **kwargs: Additional keyword arguments passed to the base `Pooling2D` class.\n\"\"\"\n```", "```python\n\"\"\"\n3D Max Pooling layer.\n\nThis class applies max pooling operation over a 3D input tensor (e.g., spatial or spatio-temporal data).\n\nParameters:\n    pool_size (tuple of 3 ints): Size of the pooling window along each dimension (depth, height, width).\n    strides (tuple of 3 ints, optional): Strides of the pooling operation along each dimension. If None, it defaults to `pool_size`.\n    padding (str, optional): One of 'valid' or 'same' (case-insensitive). 'valid' means no padding, 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format (str, optional): One of 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape `(batch_size, depth, height, width, channels)` while 'channels_first' corresponds to inputs with shape `(batch_size, channels, depth, height, width)`.\n    **kwargs: Additional keyword arguments passed to the base class.\n\"\"\"\n```", "```python\n\"\"\"\nBase class for Keras metrics.\n\nThis class provides a framework for implementing custom metrics in Keras. It includes methods for updating the state of the metric, computing the result, and resetting the state. Metrics can be used to evaluate the performance of a model during training and evaluation.\n\nAttributes:\n    name (str): The name of the metric.\n    dtype (tf.DType): The data type of the metric's internal state.\n    _dtype_policy (tf.keras.mixed_precision.Policy): The policy for managing the dtype of the metric.\n    _metrics (list): A list of sub-metrics.\n    _variables (list): A list of variables used by the metric.\n    _tracker (Tracker): An object for tracking variables and sub-metrics.\n\nMethods:\n    reset_state(): Resets the state of the metric.\n    update_state(*args, **kwargs): Updates the state of the metric with new data.\n    stateless_update_state(metric_variables, *args, **kwargs): Updates the state of the metric in a stateless manner.\n    result(): Computes the result of the metric.\n    stateless_result(metric_variables): Computes the result of the metric in a stateless manner.\n    stateless_reset_state(): Resets the state of the metric in a stateless manner.\n    add_variable(shape, initializer, dtype=None, aggregation='sum', name=None): Adds a new variable to the metric.\n    add_weight(shape=(), initializer=None, dtype=None, name=None): Adds a new weight to the metric.\n    get_config(): Returns the configuration of the metric.\n    from_config(cls, config): Creates a metric from its configuration.\n    __call__(*args, **kwargs): Updates the state of the metric and returns the result.\n    __repr__(): Returns a string representation of the metric.\n    __str__(): Returns a string representation of the metric.\n\"\"\"\n```", "```python\n\"\"\"\nMultiLabelBinarizer is a transformer that transforms a list of sets of labels into a binary matrix indicating the presence or absence of each label.\n\nParameters:\n    classes (array-like, default=None): The set of all possible labels. If None, the set of all unique labels in the data is used.\n    sparse_output (bool, default=False): If True, the output will be a sparse matrix.\n\nAttributes:\n    classes_ (ndarray): The sorted unique list of all possible labels.\n    _cached_dict (dict): A dictionary mapping each label to its index in classes_.\n\nMethods:\n    fit(y): Fit the label sets y.\n    fit_transform(y): Fit to data, then transform it.\n    transform(y): Transform the label sets y to a binary matrix.\n    inverse_transform(yt): Transform the binary matrix yt back to label sets.\n    _build_cache(): Build a cache of label to index mappings.\n    _transform(y, class_mapping): Transform the label sets y to a binary matrix using the provided class mapping.\n    _more_tags(): Get metadata about the estimator.\n\nRaises:\n    ValueError: If the classes argument contains duplicate classes or if the label indicator contains values other than 0 or 1.\n\"\"\"\n```", "```python\n\"\"\"\nOneHotEncoder(_BaseEncoder)\n\nA class for encoding categorical features as a one-hot numeric array.\n\nParameters:\n    categories (str or list of array-like, default='auto'):\n        Categories (unique values) per feature. Can be:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : categories[i] holds the categories expected in the ith column.\n    drop (str, 'array-like', or None, default=None):\n        Specifies a methodology to use to drop one of the categories per feature.\n        Can be:\n        - None : retain all features (the default).\n        - 'first' : drop the first category in each feature.\n        - 'if_binary' : drop the first category if only two categories are present in the feature.\n        - array-like : drop categories in each feature that are listed in the array-like.\n    sparse_output (bool, default=True):\n        Will return sparse matrix if set True else will return an array.\n    dtype (numpy data type, default=np.float64):\n        Desired dtype of output.\n    handle_unknown (str, default='error'):\n        Specifies whether to raise an error or ignore if a unknown categorical feature is present during transform.\n        Can be:\n        - 'error' : raise an error.\n        - 'ignore' : ignore the unknown feature.\n        - 'infrequent_if_exist' : use the infrequent category if it exists.\n    min_frequency (int or float, default=None):\n        Specifies the minimum frequency below which a category will be considered infrequent.\n    max_categories (int, default=None):\n        Specifies the maximum number of categories to encode. If exceeded, the most frequent categories are kept.\n    feature_name_combiner (str or callable, default='concat'):\n        A callable or string that specifies how to combine the original feature name with the category name.\n        Can be:\n        - 'concat' : concatenate the original feature name and the category name with an underscore.\n        - callable : a function that takes two arguments (feature name and category name) and returns a string.\n\nMethods:\n    fit(X, y=None):\n        Fit OneHotEncoder to X.\n    transform(X):\n        Transform X using one-hot encoding.\n    inverse_transform(X):\n        Convert the data back to the original representation.\n    get_feature_names_out(input_features=None):\n        Get output feature names for transformation.\n\"\"\"\n```", "```python\n\"\"\"\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm\nthat can identify clusters of varying shapes and sizes in a dataset. It does not require specifying\nthe number of clusters a priori and can handle noise.\n\nParameters\n----------\nmin_samples : int or float, default=5\n    The number of samples in a neighborhood for a point to be considered as a core point.\n    If float, it is treated as a fraction of the dataset size.\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for them to be considered as in the same neighborhood.\n\nmetric : str or callable, default='minkowski'\n    The distance metric to use. It can be a string representing a known metric or a callable function.\n\np : float, default=2\n    The power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance\n    (l1), and euclidean_distance (l2) for p = 2.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncluster_method : {'dbscan', 'xi'}, default='xi'\n    The extraction method used to derive clusters from the reachability plot.\n\neps : float, default=None\n    The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n    Used only when cluster_method='dbscan'.\n\nxi : float, default=0.05\n    Determines the minimum steepness on the reachability plot that constitutes a cluster boundary.\n    Used only when cluster_method='xi'.\n\npredecessor_correction : bool, default=True\n    Correct clusters according to the predecessors. Used only when cluster_method='xi'.\n\nmin_cluster_size : int or float, default=None\n    Minimum number of samples in an optics cluster, expressed as an absolute number or a fraction of the dataset.\n    Used only when cluster_method='xi'.\n\nalgorithm : {'auto', 'brute', 'ball_tree', 'kd_tree'}, default='auto'\n    The algorithm to compute the nearest neighbors.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all processors.\n\nAttributes\n----------\nordering_ : array, shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : array, shape (n_samples,)\n    Distances at which each sample becomes a core point, indexed by object order.\n    Points which are not core have a distance of inf.\n\nreachability_ : array, shape (n_samples,)\n    Reachability distances per sample, indexed by object order.\n\npredecessor_ : array, shape (n_samples,)\n    Sample indices that each sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\nlabels_ : array, shape (n_samples,)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\ncluster_hierarchy_ : array, shape (n_clusters, 2)\n    The list of clusters in the form of [start, end] slices into the ``ordering_`` array.\n    Used only when cluster_method='xi'.\n\"\"\"\n```", "```python\n\"\"\"\nOrdinalEncoder(OneToOneFeatureMixin, _BaseEncoder)\n\nEncode categorical features as an integer array.\n\nThis encoder converts each category into a unique integer value. The categories\ncan be specified manually or automatically inferred from the training data. It also\nprovides options to handle unknown categories and missing values during both\nfitting and transformation.\n\nParameters\n----------\ncategories : 'auto' or list of array-like, default='auto'\n    Categories (unique values) per feature:\n\n    - 'auto' : Determine categories automatically from the training data.\n    - list : ``categories[i]`` holds the categories expected in the ith column.\n      The passed categories should not mix strings and numeric values, and should\n      be sorted in case of numeric values.\n\ndtype : number type, default=np.float64\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'use_encoded_value'}, default='error'\n    Specifies whether to raise an error or use the encoded value for unknown\n    categories.\n\nunknown_value : int or np.nan, default=None\n    When ``handle_unknown='use_encoded_value'``, this value is used for unknown\n    categories. It has to be distinct from the values used to encode any of the\n    categories in ``fit``. If ``np.nan`` is given, and ``dtype`` is a float, the\n    missing value will be used.\n\nencoded_missing_value : int or np.nan, default=np.nan\n    Value used to represent missing values in the encoded data.\n\nmin_frequency : int or float, default=None\n    Specifies the minimum frequency of categories to be encoded. If an integer,\n    categories with a smaller cardinality will be considered infrequent. If a float,\n    it represents the minimum frequency of categories. Categories with a frequency\n    lower than this threshold will be considered infrequent.\n\nmax_categories : int, default=None\n    Specifies the maximum number of categories to encode. Categories more frequent\n    than this threshold will be considered infrequent.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D pooling layer for temporal data.\n\nThis layer applies a pooling function to the input data, which is expected to be a 3D tensor.\nThe pooling operation is performed along the temporal dimension, and the layer supports different\npooling functions, pool sizes, strides, and padding options.\n\nParameters:\n    pool_function (function): The pooling function to apply (e.g., tf.nn.max_pool1d).\n    pool_size (int or tuple): The size of the pooling window.\n    strides (int or tuple): The stride of the pooling window. If None, it defaults to the pool_size.\n    padding (str): The padding method, either 'valid' or 'same'. Defaults to 'valid'.\n    data_format (str): The data format of the input tensor, either 'channels_first' or 'channels_last'. Defaults to 'channels_last'.\n    name (str): The name of the layer. Defaults to None.\n    **kwargs: Additional keyword arguments to be passed to the base class constructor.\n\nAttributes:\n    pool_function (function): The pooling function to apply.\n    pool_size (tuple): The normalized size of the pooling window.\n    strides (tuple): The normalized stride of the pooling window.\n    padding (str): The normalized padding method.\n    data_format (str): The normalized data format.\n    input_spec (InputSpec): The specification of the expected input shape.\n\nMethods:\n    call(inputs): Applies the pooling function to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D pooling layer for spatial data.\n\nThis layer performs pooling operations (e.g., max pooling, average pooling) on 2D inputs\nsuch as images. It supports different pooling functions, customizable pool sizes, strides,\npadding modes, and data formats.\n\nParameters:\n    pool_function (function): The pooling function to apply (e.g., tf.nn.max_pool).\n    pool_size (int or tuple of 2 ints): The size of the pooling window for each dimension of the input tensor.\n    strides (int or tuple of 2 ints): The stride of the pooling window for each dimension of the input tensor.\n    padding (str): The padding method, either 'valid' or 'same'.\n    data_format (str): The data format of the input tensor, either 'channels_first' or 'channels_last'.\n    name (str): The name of the layer.\n    **kwargs: Additional keyword arguments to be passed to the base class constructor.\n\nAttributes:\n    pool_function (function): The pooling function to apply.\n    pool_size (tuple of 2 ints): The normalized size of the pooling window.\n    strides (tuple of 2 ints): The normalized stride of the pooling window.\n    padding (str): The normalized padding method.\n    data_format (str): The normalized data format.\n    input_spec (InputSpec): Specifies the expected input shape and dtype.\n\nMethods:\n    call(inputs): Applies the pooling function to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "\"\"\"\nA 3D pooling layer for spatial data.\n\nThis layer performs pooling operations (e.g., max pooling, average pooling) on 3D input data.\nIt supports different pooling functions, customizable pool sizes, strides, and padding options.\n\nParameters:\n    pool_function (function): The pooling function to apply (e.g., tf.nn.max_pool3d).\n    pool_size (int or tuple of 3 ints): Size of the pooling window for each dimension of the input tensor.\n    strides (int or tuple of 3 ints): Stride of the pooling window for each dimension of the input tensor.\n    padding (str): One of 'valid' or 'same'. Specifies the padding algorithm.\n    data_format (str): One of 'channels_last' or 'channels_first'. Specifies the data format of the input tensor.\n    name (str): Name of the layer.\n    **kwargs: Additional keyword arguments to be passed to the base class constructor.\n\nAttributes:\n    pool_function (function): The pooling function to apply.\n    pool_size (tuple of 3 ints): Normalized size of the pooling window.\n    strides (tuple of 3 ints): Normalized stride of the pooling window.\n    padding (str): Normalized padding algorithm.\n    data_format (str): Normalized data format of the input tensor.\n    input_spec (InputSpec): Specifies the expected input shape.\n\nMethods:\n    call(inputs): Applies the pooling function to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"", "```python\n\"\"\"\nPrincipal Component Analysis (PCA) implementation.\n\nThis class performs dimensionality reduction using PCA. It supports two solvers: 'eigen' and 'svd'.\nThe 'eigen' solver computes the eigenvectors and eigenvalues of the covariance matrix,\nwhile the 'svd' solver uses Singular Value Decomposition on the data matrix.\n\nParameters:\n    n_components (int, optional): Number of principal components to keep. If None, all components are kept.\n    solver (str): Solver to use ('eigen' or 'svd'). Default is 'svd'.\n    whitening (bool): When True, the components are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Default is False.\n\nAttributes:\n    solver (str): The solver used for PCA.\n    n_components (int): Number of principal components to keep.\n    _is_fitted (bool): Indicates if the model has been fitted.\n    whitening (bool): Whether whitening is applied.\n    e_vals_ (array): Eigenvalues of the covariance matrix.\n    e_vecs_ (array): Eigenvectors of the covariance matrix.\n    w_ (array): Projection matrix.\n    e_vals_normalized_ (array): Normalized eigenvalues.\n    loadings_ (array): Loadings matrix.\n\nMethods:\n    fit(X, y=None): Fit the model to the data X.\n    transform(X): Apply dimensionality reduction to X.\n    _fit(X): Perform the actual fitting process.\n    _covariance_matrix(X): Compute the covariance matrix of X.\n    _decomposition(mat, n_samples): Decompose the matrix using the specified solver.\n    _loadings(): Compute the loadings matrix.\n    _projection_matrix(eig_vals, eig_vecs, whitening, n_components): Compute the projection matrix.\n\"\"\"\n```", "```python\n\"\"\"\nRMSprop optimizer.\n\nRMSprop is an adaptive learning rate method proposed by Geoffrey Hinton. It is\ndesigned to combine the advantages of two other extensions of stochastic gradient\ndescent: AdaGrad and Rprop. The gist of RMSprop is to:\n\n- Maintain a moving (discounted) average of the square of gradients\n- Divide the gradient by the root of this average\n\nThis implementation includes optional momentum and centered variants.\n\nArgs:\n    learning_rate: A `Tensor` or a floating point value. The learning rate.\n    rho: Discounting factor for the history/coming gradient. Must be between 0 and 1.\n    momentum: A scalar tensor or a floating point value. The momentum.\n    epsilon: Small constant for numerical stability.\n    centered: Boolean. If True, gradients are normalized by the estimated variance of the gradient.\n    name: Optional name prefix for the operations created when applying gradients.\n    **kwargs: Keyword arguments. Allowed to be one of `clipnorm` or `clipvalue`.\n        `clipnorm` (float) clips gradients by norm; `clipvalue` (float) clips\n        gradients by value.\n\"\"\"\n```", "```python\n\"\"\"\nSelfTrainingClassifier is a semi-supervised learning classifier that uses a base estimator to iteratively label unlabeled data.\nIt can use either a fixed threshold on prediction probabilities or select the top k best samples to label in each iteration.\nThe process continues until all samples are labeled or a maximum number of iterations is reached.\n\nParameters:\n    estimator (object, default=None): The base estimator to be used for self-training. Must implement the `fit` method.\n    base_estimator (object, default='deprecated'): Deprecated parameter. Use `estimator` instead.\n    threshold (float, default=0.75): The probability threshold for a sample to be labeled. Used if `criterion` is 'threshold'.\n    criterion (str, default='threshold'): The criterion to use for selecting samples to label. Options are 'threshold' or 'k_best'.\n    k_best (int, default=10): The number of top samples to label in each iteration. Used if `criterion` is 'k_best'.\n    max_iter (int, default=10): The maximum number of iterations to run the self-training process. If None, the process will continue until all samples are labeled.\n    verbose (bool, default=False): If True, print the number of new labels added in each iteration.\n\nAttributes:\n    estimator_ (object): The fitted base estimator.\n    transduction_ (ndarray): The label assignment for all samples.\n    labeled_iter_ (ndarray): The iteration at which each sample was labeled.\n    n_iter_ (int): The number of iterations run.\n    termination_condition_ (str): The reason for stopping the self-training process.\n    classes_ (ndarray): The classes seen during fit.\n\nMethods:\n    fit(X, y, **params): Fit the model to the data.\n    predict(X, **params): Predict the labels for the data.\n    predict_proba(X, **params): Predict the probability estimates for the data.\n    decision_function(X, **params): Compute the decision function for the data.\n    predict_log_proba(X, **params): Predict the log probability estimates for the data.\n    score(X, y, **params): Return the mean accuracy on the given test data and labels.\n    get_metadata_routing(): Get metadata routing of this object.\n\"\"\"\n```", "```python\n\"\"\"\nA separable convolution layer, inheriting from the Conv class.\n\nThis layer performs a depthwise spatial convolution followed by a pointwise convolution.\nThe depthwise convolution applies a single filter to each input channel separately,\nand the pointwise convolution mixes the outputs of the depthwise convolution across channels.\n\nParameters:\n    rank (int): The dimensionality of the convolution.\n    filters (int): The number of output filters in the convolution.\n    kernel_size (int or tuple): The size of the convolution window.\n    strides (int or tuple): The stride of the convolution.\n    padding (str): One of 'valid' or 'same' (case-insensitive).\n    data_format (str): A string, one of 'channels_last' (default) or 'channels_first'.\n    dilation_rate (int or tuple): The dilation rate to use for dilated convolution.\n    depth_multiplier (int): The number of depthwise convolution output channels for each input channel.\n    activation (str or callable): Activation function to use.\n    use_bias (bool): Whether the layer uses a bias vector.\n    depthwise_initializer (str or callable): Initializer for the depthwise kernel weights matrix.\n    pointwise_initializer (str or callable): Initializer for the pointwise kernel weights matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    depthwise_regularizer (str or callable): Regularizer function applied to the depthwise kernel weights matrix.\n    pointwise_regularizer (str or callable): Regularizer function applied to the pointwise kernel weights matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its 'activation').\n    depthwise_constraint (str or callable): Constraint function applied to the depthwise kernel weights matrix.\n    pointwise_constraint (str or callable): Constraint function applied to the pointwise kernel weights matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n    trainable (bool): Whether the layer's weights will be updated during training.\n    name (str): A string, the name of the layer.\n    **kwargs: Additional keyword arguments.\n\nAttributes:\n    depth_multiplier (int): The number of depthwise convolution output channels for each input channel.\n    depthwise_initializer (Initializer): Initializer for the depthwise kernel weights matrix.\n    pointwise_initializer (Initializer): Initializer for the pointwise kernel weights matrix.\n    bias_initializer (Initializer): Initializer for the bias vector.\n    depthwise_regularizer (Regularizer): Regularizer function applied to the depthwise kernel weights matrix.\n    pointwise_regularizer (Regularizer): Regularizer function applied to the pointwise kernel weights matrix.\n    bias_regularizer (Regularizer): Regularizer function applied to the bias vector.\n    activity_regularizer (Regularizer): Regularizer function applied to the output of the layer (its 'activation').\n    depthwise_constraint (Constraint): Constraint function applied to the depthwise kernel weights matrix.\n    pointwise_constraint (Constraint): Constraint function applied to the pointwise kernel weights matrix.\n    bias_constraint (Constraint): Constraint function applied to the bias vector.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D separable convolution layer.\n\nThis layer performs a depthwise spatial convolution followed by a pointwise convolution.\nIt is designed to be more efficient than a standard convolution layer by separating the\nspatial and channel-wise convolutions.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size (int or tuple): The size of the convolution window.\n    strides (int or tuple): The stride of the convolution.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding.\n                   `\"same\"` results in padding the input such that the output has the same length as the input.\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n                       The ordering of the dimensions in the inputs.\n                       `channels_last` corresponds to inputs with shape `(batch, steps, features)`\n                       while `channels_first` corresponds to inputs with shape `(batch, features, steps)`.\n    dilation_rate (int or tuple): The dilation rate to use for dilated convolution.\n    depth_multiplier (int): The number of depthwise convolution output channels for each input channel.\n                            The total number of depthwise convolution output channels will be equal to\n                            `filters_in * depth_multiplier`.\n    activation (str or callable): Activation function to use. If you don't specify anything, no activation is applied.\n    use_bias (bool): Whether the layer uses a bias vector.\n    depthwise_initializer (str or callable): Initializer for the depthwise kernel matrix.\n    pointwise_initializer (str or callable): Initializer for the pointwise kernel matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    depthwise_regularizer (str or callable): Regularizer function applied to the depthwise kernel matrix.\n    pointwise_regularizer (str or callable): Regularizer function applied to the pointwise kernel matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\")..\n    depthwise_constraint (str or callable): Constraint function applied to the depthwise kernel matrix.\n    pointwise_constraint (str or callable): Constraint function applied to the pointwise kernel matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n\nReturns:\n    A tensor of shape `(batch, new_steps, filters)` where `new_steps` is determined by the padding and stride settings.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D separable convolution layer.\n\nThis layer performs a depthwise spatial convolution followed by a pointwise convolution.\nThe depthwise convolution applies a single filter to each input channel separately,\nand the pointwise convolution applies a 1x1 convolution to combine the outputs of the depthwise convolution.\n\nParameters:\n    filters (int): The dimensionality of the output space (i.e., the number of output filters in the pointwise convolution).\n    kernel_size (int or tuple of 2 ints): The height and width of the 2D convolution window.\n    strides (tuple of 2 ints): The strides of the convolution along the height and width.\n    padding (str): One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding.\n                   `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input.\n                   When `padding=\"same\"` and `strides=1`, the output has the same height and width as the input.\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`.\n                       The ordering of the dimensions in the inputs.\n                       `channels_last` corresponds to inputs with shape `(batch, height, width, channels)`\n                       while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n                       It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n                       If you never set it, it will be `\"channels_last\"`.\n    dilation_rate (tuple of 2 ints): An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n                                   Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any `strides` value != 1.\n    depth_multiplier (int): The number of depthwise convolution output channels for each input channel.\n                            The total number of depthwise convolution output channels will be equal to `filters_in * depth_multiplier`.\n    activation (str or callable): Activation function to use. If you don't specify anything, no activation is applied (i.e., \"linear\" activation: `a(x) = x`).\n    use_bias (bool): Whether the layer uses a bias vector.\n    depthwise_initializer (str or callable): Initializer for the depthwise kernel matrix.\n    pointwise_initializer (str or callable): Initializer for the pointwise kernel matrix.\n    bias_initializer (str or callable): Initializer for the bias vector.\n    depthwise_regularizer (str or callable): Regularizer function applied to the depthwise kernel matrix.\n    pointwise_regularizer (str or callable): Regularizer function applied to the pointwise kernel matrix.\n    bias_regularizer (str or callable): Regularizer function applied to the bias vector.\n    activity_regularizer (str or callable): Regularizer function applied to the output of the layer (its \"activation\")..\n    depthwise_constraint (str or callable): Constraint function applied to the depthwise kernel matrix.\n    pointwise_constraint (str or callable): Constraint function applied to the pointwise kernel matrix.\n    bias_constraint (str or callable): Constraint function applied to the bias vector.\n\nReturns:\n    A tensor of shape `(batch, new_height, new_width, filters)` if `data_format='channels_last'`,\n    or `(batch, filters, new_height, new_width)` if `data_format='channels_first'`.\n\"\"\"\n```", "```python\n\"\"\"\nSequentialFeatureSelector is a class for feature selection using sequential algorithms.\nIt can perform both forward and backward selection, with optional floating selection.\nThe class uses cross-validation to evaluate the performance of feature subsets and\nselects the best subset based on a specified scoring metric.\n\nParameters:\n    estimator (object): A supervised learning estimator with a `fit` method.\n    k_features (int, tuple, or str): Number of features to select. Can be an integer,\n        a tuple of two integers (min, max), or a string ('best' or 'parsimonious').\n    forward (bool): If True, perform forward selection. If False, perform backward selection.\n    floating (bool): If True, perform floating selection.\n    verbose (int): Controls the verbosity of the output.\n    scoring (str, callable, or None): A string or a scorer callable object/function with signature\n        `scorer(estimator, X, y)`.\n    cv (int, cross-validation generator, or an iterable): Determines the cross-validation splitting strategy.\n    n_jobs (int): Number of jobs to run in parallel.\n    pre_dispatch (str or int): Controls the number of jobs that get dispatched during parallel execution.\n    clone_estimator (bool): If True, the estimator will be cloned during fitting.\n    fixed_features (tuple or None): Features that must be included in the selected feature set.\n    feature_groups (list of lists or None): Groups of features that must be included or excluded together.\n\nAttributes:\n    estimator_ (object): The fitted estimator.\n    subsets_ (dict): A dictionary containing the selected feature subsets and their corresponding scores.\n    k_feature_idx_ (tuple): Indices of the selected features.\n    k_feature_names_ (list): Names of the selected features.\n    k_score_ (float): Score of the selected feature subset.\n    fitted (bool): Indicates whether the selector has been fitted.\n    interrupted_ (bool): Indicates whether the fitting process was interrupted.\n    _TESTING_INTERRUPT_MODE (bool): Used for testing purposes to simulate an interruption.\n\"\"\"\n```", "```python\n\"\"\"\nStochastic Gradient Descent (SGD) optimizer.\n\nThis class implements the Stochastic Gradient Descent optimization algorithm,\noptionally with momentum and Nesterov acceleration.\n\nParameters:\n    learning_rate (float): Learning rate. Defaults to 0.01.\n    momentum (float): Momentum factor. Defaults to 0.0.\n    nesterov (bool): Whether to apply Nesterov momentum. Defaults to False.\n    name (str): Name of the optimizer. Defaults to 'SGD'.\n    **kwargs: Additional keyword arguments passed to the base class.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer aggregates gradients.\n\nMethods:\n    _create_slots(var_list): Creates slots for variables if momentum is enabled.\n    _prepare_local(var_device, var_dtype, apply_state): Prepares local state for the optimizer.\n    _resource_apply_dense(grad, var, apply_state): Applies gradients to variables for dense updates.\n    _resource_apply_sparse_duplicate_indices(grad, var, indices, **kwargs): Applies gradients to variables for sparse updates with duplicate indices.\n    _resource_apply_sparse(grad, var, indices, apply_state): Applies gradients to variables for sparse updates.\n    get_config(): Returns the configuration of the optimizer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nSoftmax Regression classifier.\n\nParameters:\n    eta (float): Learning rate (default: 0.01).\n    epochs (int): Number of passes over the training dataset (default: 50).\n    l2 (float): L2 regularization parameter (default: 0.0).\n    minibatches (int): Number of minibatches for gradient descent (default: 1).\n    n_classes (int): Number of unique classes (default: None, will be inferred from data).\n    random_seed (int): Random seed for reproducibility (default: None).\n    print_progress (int): Verbosity level (0: no output, 1: epoch-wise, 2: iter-wise) (default: 0).\n\nAttributes:\n    eta (float): Learning rate.\n    epochs (int): Number of epochs.\n    l2 (float): L2 regularization parameter.\n    minibatches (int): Number of minibatches.\n    n_classes (int): Number of unique classes.\n    random_seed (int): Random seed.\n    print_progress (int): Verbosity level.\n    _is_fitted (bool): Flag indicating if the model is fitted.\n    w_ (np.ndarray): Weights after fitting.\n    b_ (np.ndarray): Biases after fitting.\n    cost_ (list): Cost values after each epoch.\n    init_time_ (float): Time when fitting started.\n\nMethods:\n    _net_input(X): Compute the net input.\n    _softmax_activation(z): Apply the softmax activation function.\n    _cross_entropy(output, y_target): Compute the cross-entropy loss.\n    _cost(cross_entropy): Compute the cost with L2 regularization.\n    _to_classlabels(z): Convert softmax output to class labels.\n    _forward(X): Perform forward propagation.\n    _backward(X, y_true, y_probas): Perform backward propagation.\n    _fit(X, y, init_params): Fit the model to the training data.\n    predict_proba(X): Predict class probabilities for X.\n    _predict(X): Predict class labels for X.\n\"\"\"\n```", "```python\n\"\"\"\nTargetEncoder is a transformer that encodes categorical features based on the target variable.\nIt uses a target-based encoding scheme with optional smoothing and cross-validation to prevent overfitting.\nThe encoder supports different types of target variables, including continuous, binary, and multiclass.\n\nParameters:\n    categories (str or list of lists, default='auto'):\n        Specifies the categories for each feature. If 'auto', categories are determined from the training data.\n    target_type (str, default='auto'):\n        Specifies the type of the target variable. Options are 'auto', 'continuous', 'binary', and 'multiclass'.\n    smooth (str or float, default='auto'):\n        Specifies the smoothing parameter. If 'auto', the smoothing parameter is determined based on the target variance.\n    cv (int, default=5):\n        Specifies the number of cross-validation folds.\n    shuffle (bool, default=True):\n        Specifies whether to shuffle the data before splitting into folds.\n    random_state (int, RandomState instance, or None, default=None):\n        Controls the randomness of the cross-validation shuffling.\n\nMethods:\n    fit(X, y):\n        Fits the encoder to the training data.\n    fit_transform(X, y):\n        Fits the encoder to the training data and returns the encoded features.\n    transform(X):\n        Transforms the input features using the fitted encoder.\n    get_feature_names_out(input_features=None):\n        Returns the output feature names.\n    _more_tags():\n        Returns a dictionary of metadata tags for the encoder.\n\"\"\"\n```", "```python\n\"\"\"\nTransactionEncoder is a transformer that encodes a list of transactions into a binary matrix.\nEach transaction is a list of items, and the resulting matrix has a column for each unique item\nacross all transactions. The value in each cell is True if the item is present in the transaction,\nand False otherwise.\n\nParameters:\n    None\n\nAttributes:\n    columns_ (list): A sorted list of unique items found in the transactions.\n    columns_mapping_ (dict): A mapping from each unique item to its corresponding column index.\n\nMethods:\n    fit(X): Fit the encoder to a list of transactions, determining the unique items and their order.\n    transform(X, sparse=False): Transform a list of transactions into a binary matrix.\n    inverse_transform(array): Convert a binary matrix back into a list of transactions.\n    fit_transform(X, sparse=False): Fit the encoder and transform the transactions in one step.\n    get_feature_names_out(): Get the feature names (unique items) corresponding to the columns of the matrix.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D upsampling layer for temporal data.\n\nThis layer repeats each temporal step `size` times along the time axis.\nThe input should be a 3D tensor with shape `(batch_size, steps, features)`.\n\nParameters:\n    size (int): The upsampling factor. Defaults to 2.\n\nAttributes:\n    input_spec (InputSpec): Specifies the expected input shape, which should be a 3D tensor.\n    size (int): The upsampling factor.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The forward pass of the layer, which repeats elements along the time axis.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D upsampling layer.\n\nThis layer upsamples the input by a factor of (size[0], size[1]) along the spatial dimensions.\nIt supports two interpolation methods: 'nearest' and 'bilinear'.\n\nArguments:\n    size: Tuple of 2 integers. The upsampling factors for rows and columns.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape `(batch, height, width, channels)`\n        while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n        If you never set it, it will be \"channels_last\".\n    interpolation: A string, one of `nearest` or `bilinear`.\n        The method used for upsampling.\n\nInput shape:\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`: `(batch_size, rows, cols, channels)`\n    - If `data_format` is `\"channels_first\"`: `(batch_size, channels, rows, cols)`\n\nOutput shape:\n    4D tensor with shape:\n    - If `data_format` is `\"channels_last\"`: `(batch_size, upsampled_rows, upsampled_cols, channels)`\n    - If `data_format` is `\"channels_first\"`: `(batch_size, channels, upsampled_rows, upsampled_cols)`\n\"\"\"\n```", "```python\n\"\"\"\nA 3D upsampling layer.\n\nThis layer upsamples a 5D tensor (batch_size, channels, depth, height, width) by a factor of (size[0], size[1], size[2]).\nThe upsampling is performed by repeating the rows, columns, and depth slices of the input tensor.\n\nParameters:\n    size (tuple of 3 int): The upsampling factors for depth, height, and width. Default is (2, 2, 2).\n    data_format (str): One of \"channels_first\" or \"channels_last\". Specifies the ordering of the dimensions in the inputs.\n                       \"channels_first\" corresponds to inputs with shape (batch_size, channels, depth, height, width),\n                       while \"channels_last\" corresponds to inputs with shape (batch_size, depth, height, width, channels).\n                       Default is None, which will use the Keras backend's default data format.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The logic of the layer, returns the upsampled tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nA 1D layer that adds rows of zeros at the top and bottom of the input tensor.\n\nParameters:\n    padding (int or tuple of int): Integer or tuple of 2 integers, specifying the number of zeros to add at the start and end of the temporal dimension (axis 1). If a single integer is provided, the same padding is applied to both sides.\n\nAttributes:\n    padding (tuple of int): Normalized tuple of 2 integers representing the padding for the start and end of the temporal dimension.\n    input_spec (InputSpec): Specifies the expected input shape, which must be a 3D tensor.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): Applies zero padding to the input tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D layer that pads the input tensor with zeros along the height and width dimensions.\n\nParameters:\n    padding (int or tuple): If an integer, the same symmetric padding is applied to height and width.\n                            If a tuple of 2 integers, interpreted as two different symmetric padding values for height and width.\n                            If a tuple of 2 tuples of 2 integers, interpreted as ((top_pad, bottom_pad), (left_pad, right_pad)).\n    data_format (str): One of \"channels_last\" (default) or \"channels_first\". The ordering of the dimensions in the inputs.\n                       \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width).\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): Applies the zero-padding transformation to the input tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\n3D Zero Padding layer for a neural network.\n\nThis layer adds rows and columns of zeros at the top, bottom, left, right, front, and back of a 3D input tensor.\n\nParameters:\n    padding (int or tuple of 3 ints or tuple of 3 tuples of 2 ints): \n        - If int, the same symmetric padding is applied to width, height, and depth.\n        - If tuple of 3 ints, the symmetric padding is applied to each dimension.\n        - If tuple of 3 tuples of 2 ints, asymmetric padding is applied to each dimension.\n    data_format (str): One of 'channels_first' or 'channels_last'. Specifies the ordering of the dimensions in the inputs.\n        - 'channels_first': channels dimension comes first, e.g. (batch_size, channels, depth, height, width)\n        - 'channels_last': channels dimension comes last, e.g. (batch_size, depth, height, width, channels)\n    **kwargs: Additional keyword arguments passed to the base class.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): Applies the zero padding to the input tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"\n```", "```python\n\"\"\"\nBase class for encoders that transform categorical features into numerical format.\n\nThis class provides the foundational methods for checking input data, fitting the encoder to the data,\ntransforming the data, and handling infrequent categories. It inherits from `TransformerMixin` and\n`BaseEstimator` to integrate seamlessly with scikit-learn's pipeline and model evaluation tools.\n\nAttributes:\n    categories_ (list of arrays): The unique categories for each feature learned during the fit process.\n    n_features_in_ (int): The number of features seen during the fit process.\n    infrequent_categories_ (list of arrays): The infrequent categories for each feature, if enabled.\n    _infrequent_enabled (bool): Indicates whether infrequent category handling is enabled.\n    _infrequent_indices (list of arrays): Indices of infrequent categories for each feature.\n    _default_to_infrequent_mappings (list of arrays): Mappings from original category indices to infrequent category indices.\n\nMethods:\n    _check_X(X, force_all_finite=True): Validates and preprocesses the input data.\n    _fit(X, handle_unknown='error', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False): Fits the encoder to the input data.\n    _transform(X, handle_unknown='error', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None): Transforms the input data using the fitted encoder.\n    _check_infrequent_enabled(): Checks if infrequent category handling is enabled based on the encoder's parameters.\n    _identify_infrequent(category_count, n_samples, col_idx): Identifies infrequent categories for a given feature.\n    _fit_infrequent_category_mapping(n_samples, category_counts, missing_indices): Fits the mapping for infrequent categories.\n    _map_infrequent_categories(X_int, X_mask, ignore_category_indices): Maps infrequent categories in the transformed data.\n    _more_tags(): Returns metadata about the estimator, including supported data types and NaN handling.\n\"\"\"\n```"]