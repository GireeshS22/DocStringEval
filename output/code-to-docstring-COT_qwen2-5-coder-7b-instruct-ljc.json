["```python\nclass Adamax(optimizer_v2.OptimizerV2):\n    \"\"\"\n    Adamax optimizer for TensorFlow.\n\n    Adamax is a variant of the Adam algorithm that uses the infinity norm for\n    parameter updates, making it suitable for sparse data and large datasets.\n\n    Args:\n        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.001.\n        beta_1 (float, optional): The exponential decay rate for the first moment estimates. Defaults to 0.9.\n        beta_2 (float, optional): The exponential decay rate for the second moment estimates. Defaults to 0.999.\n        epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-07.\n        name (str, optional): The name of the optimizer. Defaults to 'Adamax'.\n\n    Attributes:\n        _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports gradient aggregation.\n\n    Methods:\n        _create_slots(var_list): Creates slots for the optimizer.\n        _prepare_local(var_device, var_dtype, apply_state): Prepares local variables for the optimizer.\n        _resource_apply_dense(grad, var, apply_state=None): Applies the optimizer to a dense variable.\n        _resource_apply_sparse(grad, var, indices, apply_state=None): Applies the optimizer to a sparse variable.\n        get_config(): Returns a dictionary containing the configuration of the optimizer.\n    \"\"\"\n```", "```python\nclass AgglomerationTransform(TransformerMixin):\n    \"\"\"\n    AgglomerationTransform is a transformer that aggregates data points based on their labels using a specified pooling function.\n\n    Parameters\n    ----------\n    pooling_func : callable, default=np.mean\n        The function to use for pooling the data points. Default is the mean.\n\n    Attributes\n    ----------\n    labels_ : array-like\n        The labels assigned to each data point.\n\n    Methods\n    -------\n    transform(X)\n        Aggregates the data points based on their labels using the specified pooling function.\n\n    inverse_transform(X=None, *, Xt=None)\n        Reverses the transformation by mapping the aggregated data back to the original data points.\n    \"\"\"\n\n    __metadata_request__inverse_transform = {'Xt': metadata_routing.UNUSED}\n\n    def transform(self, X):\n        \"\"\"\n        Aggregates the data points based on their labels using the specified pooling function.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data to be transformed.\n\n        Returns\n        -------\n        nX : array-like of shape (n_samples, n_clusters)\n            The transformed data, where each row corresponds to a cluster and each column corresponds to a feature.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if self.pooling_func == np.mean and (not issparse(X)):\n            size = np.bincount(self.labels_)\n            n_samples = X.shape[0]\n            nX = np.array([np.bincount(self.labels_, X[i, :]) / size for i in range(n_samples)])\n        else:\n            nX = [self.pooling_func(X[:, self.labels_ == l], axis=1) for l in np.unique(self.labels_)]\n            nX = np.array(nX).T\n        return nX\n\n    def inverse_transform(self, X=None, *, Xt=None):\n        \"\"\"\n        Reverses the transformation by mapping the aggregated data back to the original data points.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_clusters), optional\n            The transformed data to be inverse transformed. If not provided, the inverse transformation is applied to the data used during the last fit.\n        Xt : array-like of shape (n_samples, n_features), optional\n            The original data used during the last fit. If not provided, the inverse transformation is applied to the data used during the last fit.\n\n        Returns\n        -------\n        X : array-like of shape (n_samples, n_features)\n            The original data points.\n        \"\"\"\n        X = _deprecate_Xt_in_inverse_transform(X, Xt)\n        check_is_fitted(self)\n        unil, inverse = np.unique(self.labels_, return_inverse=True)\n        return X[..., inverse]\n```", "```python\nclass AveragePooling1D(Pooling1D):\n    \"\"\"\n    Applies average pooling along the time dimension of the input.\n\n    Args:\n        pool_size (int, tuple of int): The size of the pooling window. If a tuple is provided, it should be of length 1.\n        strides (int, tuple of int, optional): The strides of the pooling window. If a tuple is provided, it should be of length 1. Defaults to None, which uses the same stride as the pool_size.\n        padding (str, optional): The padding method to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The format of the input data. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        A 3D tensor with the same number of channels as the input, but with reduced spatial dimensions.\n    \"\"\"\n    def __init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs):\n        super(AveragePooling1D, self).__init__(functools.partial(backend.pool2d, pool_mode='avg'), pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\nclass AveragePooling2D(Pooling2D):\n    \"\"\"\n    Applies average pooling over input data.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window (height, width). Defaults to (2, 2).\n        strides (tuple of int, optional): The strides of the pooling window (height, width). If None, the strides are set to the pool_size. Defaults to None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The format of the input data. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The output tensor after applying average pooling.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        super(AveragePooling2D, self).__init__(nn.avg_pool, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\nclass AveragePooling3D(Pooling3D):\n    \"\"\"\n    Applies a 3D average pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window in the format (depth, height, width). Default is (2, 2, 2).\n        strides (tuple of int, optional): The strides of the pooling window in the format (depth, height, width). If None, the strides are set to the pool_size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_last' or 'channels_first'. Default is None.\n        **kwargs: Additional keyword arguments to pass to the parent class.\n\n    Returns:\n        Tensor: The output tensor after applying the 3D average pooling operation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        super(AveragePooling3D, self).__init__(nn.avg_pool3d, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\nclass BayesianGaussianMixture(BaseMixture):\n    \"\"\"\n    Bayesian Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    n_components : int, default=1\n        The number of mixture components.\n\n    covariance_type : {'spherical', 'tied', 'diag', 'full'}, default='full'\n        The type of covariance parameters to use. Must be one of:\n\n        - 'spherical' : each component has its own diagonal covariance matrix with all values equal.\n        - 'tied' : all components share the same diagonal covariance matrix.\n        - 'diag' : each component has its own diagonal covariance matrix.\n        - 'full' : each component has its own full covariance matrix.\n\n    tol : float, default=1e-3\n        The convergence threshold. EM iterations will stop when the lower bound\n        increase is less than this threshold.\n\n    reg_covar : float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance matrices.\n\n    max_iter : int, default=100\n        The number of EM iterations to perform.\n\n    n_init : int, default=1\n        The number of times the EM algorithm will be run with different\n        initializations. The best result will be kept.\n\n    init_params : {'kmeans', 'random'}, default='kmeans'\n        The method used to initialize the weights, the means and the\n        precisions.\n\n        - 'kmeans' : responsibilities are initialized using kmeans.\n        - 'random' : responsibilities are initialized randomly.\n\n    weight_concentration_prior_type : {'dirichlet_process', 'dirichlet_distribution'}, default='dirichlet_process'\n        The type of prior to use for the weight concentration parameters.\n\n        - 'dirichlet_process' : the weight concentration parameters are drawn from a Dirichlet process.\n        - 'dirichlet_distribution' : the weight concentration parameters are drawn from a Dirichlet distribution.\n\n    weight_concentration_prior : float or None, default=None\n        The prior on the weight concentration parameters. If None, it is set to 1.0 / n_components.\n\n    mean_precision_prior : float or None, default=None\n        The prior on the mean precision parameters. If None, it is set to 1.0.\n\n    mean_prior : array-like or None, default=None\n        The prior on the means. If None, it is set to the mean of the data.\n\n    degrees_of_freedom_prior : float or None, default=None\n        The prior on the degrees of freedom parameters. If None, it is set to the number of features.\n\n    covariance_prior : array-like or None, default=None\n        The prior on the covariance parameters. If None, it is set to the covariance of the data.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to the method for reproducibility.\n\n    warm_start : bool, default=False\n        If 'warm_start' is True, the solution of the previous call to fit is used as\n        initialization.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints the current iteration number.\n        If greater than 1 then it prints the log probability and the time needed\n        for each iteration.\n\n    verbose_interval : int, default=10\n        Interval between verbose logs.\n\n    Attributes\n    ----------\n    weights_ : array-like of shape (n_components,)\n        The weights of each mixture component.\n\n    means_ : array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component. The shape depends on `covariance_type`:\n\n        - 'spherical' : (n_components,)\n        - 'tied' : (n_features, n_features)\n        - 'diag' : (n_components, n_features)\n        - 'full' : (n_components, n_features, n_features)\n\n    precisions_ : array-like\n        The precision matrices for each mixture component. The shape depends on `covariance_type`:\n\n        - 'spherical' : (n_components,)\n        - 'tied' : (n_features, n_features)\n        - 'diag' : (n_components, n_features)\n        - 'full' : (n_components, n_features, n_features)\n\n    precisions_cholesky_ : array-like\n        The Cholesky decomposition of the precision matrices for each mixture component. The shape depends on `covariance_type`:\n\n        - 'spherical' : (n_components,)\n        - 'tied' : (n_features, n_features)\n        - 'diag' : (n_components, n_features)\n        - 'full' : (n_components, n_features, n", "```python\nclass Conv(Layer):\n    \"\"\"\n    A class representing a convolutional layer.\n\n    Args:\n        rank (int): The rank of the convolution, e.g., 1 for 1D, 2 for 2D, etc.\n        filters (int): The number of output filters in the convolution.\n        kernel_size (int or tuple of ints): The size of the convolution kernel.\n        strides (int or tuple of ints, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_first' or 'channels_last'. Defaults to None.\n        dilation_rate (int or tuple of ints, optional): The dilation rate of the convolution. Defaults to 1.\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the output activity. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias vector. Defaults to None.\n        trainable (bool, optional): Whether the layer is trainable. Defaults to True.\n        name (str, optional): The name of the layer. Defaults to None.\n        conv_op (callable, optional): The convolution operation to use. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Raises:\n        ValueError: If the number of filters is negative.\n        ValueError: If the number of filters is not evenly divisible by the number of groups.\n        ValueError: If the kernel size contains 0(s).\n        ValueError: If the strides contain 0(s).\n        ValueError: If causal padding is used with a layer other than Conv1D or SeparableConv1D.\n\n    Attributes:\n        rank (int): The rank of the convolution.\n        filters (int): The number of output filters in the convolution.\n        groups (int): The number of groups for grouped convolution.\n        kernel_size (tuple of ints): The size of the convolution kernel.\n        strides (tuple of ints): The strides of the convolution.\n        padding (str): The type of padding to use.\n        data_format (str): The data format to use.\n        dilation_rate (tuple of ints): The dilation rate of the convolution.\n        activation (callable): The activation function to use.\n        use_bias (bool): Whether to use a bias vector.\n        kernel_initializer (callable): The initializer for the kernel weights.\n        bias_initializer (callable): The initializer for the bias vector.\n        kernel_regularizer (callable): The regularizer for the kernel weights.\n        bias_regularizer (callable): The regularizer for the bias vector.\n        activity_regularizer (callable): The regularizer for the output activity.\n        kernel_constraint (callable): The constraint for the kernel weights.\n        bias_constraint (callable): The constraint for the bias vector.\n        input_spec (InputSpec): The input specification for the layer.\n        _is_causal (bool): Whether causal padding is used.\n        _channels_first (bool): Whether the data format is channels_first.\n        _tf_data_format (str): The TensorFlow data format.\n        _convolution_op (callable): The convolution operation to use.\n        built (bool): Whether the layer has been built.\n    \"\"\"\n```", "```python\nclass Conv1D(Conv):\n    \"\"\"\n    1D convolution layer for processing sequences.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (int): The size of the convolution kernel.\n        strides (int, optional): The stride of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The format of the input data. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        dilation_rate (int, optional): The dilation rate for atrous convolution. Defaults to 1.\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str, optional): The activation function to use. Can be 'relu', 'sigmoid', etc. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the output activity. Defaults to None.\n        kernel_constraint (str, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias vector. Defaults to None.\n\n    Returns:\n        A 1D convolution layer.\n    \"\"\"\n```", "```python\nclass Conv1DTranspose(Conv1D):\n    \"\"\"\n    Transposed 1D convolution layer.\n\n    This layer is used to perform transposed 1D convolution, which is useful for upsampling in neural networks.\n\n    Args:\n        filters (int): The dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (int or tuple): The size of the convolution kernel.\n        strides (int or tuple, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        output_padding (int or tuple, optional): Additional padding to add to the output. Defaults to None.\n        data_format (str, optional): The data format to use. Can be 'channels_first' or 'channels_last'. Defaults to None.\n        dilation_rate (int or tuple, optional): The dilation rate to use. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer to use for the kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer to use for the bias. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer to use for the kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer to use for the bias. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer to use for the activity. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint to use for the kernel. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint to use for the bias. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Raises:\n        ValueError: If the padding is not 'valid' or 'same'.\n        ValueError: If the output padding is greater than or equal to the stride.\n\n    Returns:\n        A tensor representing the output of the transposed 1D convolution.\n    \"\"\"\n```", "```python\nclass Conv2D(Conv):\n    \"\"\"\n    A 2D convolutional layer for neural networks.\n\n    Args:\n        filters (int): The number of filters in the convolutional layer.\n        kernel_size (tuple): The size of the convolutional kernel.\n        strides (tuple, optional): The strides of the convolution operation. Defaults to (1, 1).\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The format of the input data. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (tuple, optional): The dilation rate of the convolution operation. Defaults to (1, 1).\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias term. Defaults to True.\n        kernel_initializer (str, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias term. Defaults to 'zeros'.\n        kernel_regularizer (str, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias term. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the layer activity. Defaults to None.\n        kernel_constraint (str, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias term. Defaults to None.\n\n    Returns:\n        A 2D convolutional layer.\n    \"\"\"\n```", "```python\nclass Conv2DTranspose(Conv2D):\n    \"\"\"\n    Transposed 2D convolution layer, also known as deconvolution or fractionally strided convolution.\n\n    Args:\n        filters (int): The dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (tuple of int): A tuple of integers specifying the height and width of the 2D convolution window.\n        strides (tuple of int, optional): A tuple of integers specifying the strides of the convolution along the height and width. Defaults to (1, 1).\n        padding (str, optional): One of 'valid' or 'same'. 'valid' means no padding is added, and 'same' means padding is added so that the output has the same spatial dimensions as the input. Defaults to 'valid'.\n        output_padding (tuple of int, optional): A tuple of integers specifying the additional padding added to one side of the output. Defaults to None.\n        data_format (str, optional): One of 'channels_first' or 'channels_last'. 'channels_first' means the input and output tensors have shape (batch_size, channels, height, width), and 'channels_last' means they have shape (batch_size, height, width, channels). Defaults to None.\n        dilation_rate (tuple of int, optional): A tuple of integers specifying the dilation rate of the convolution along the height and width. Defaults to (1, 1).\n        activation (str or callable, optional): The activation function to use. If None, no activation is applied. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer to use for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer to use for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer to use for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer to use for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer to use for the output of the layer. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint to use for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint to use for the bias vector. Defaults to None.\n\n    Returns:\n        A 4D tensor of shape (batch_size, height, width, filters) if data_format is 'channels_last', or (batch_size, channels, height, width) if data_format is 'channels_first'.\n    \"\"\"\n```", "```python\nclass Conv3D(Conv):\n    \"\"\"\n    A 3D convolutional layer.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (tuple of int): The size of the convolution kernel.\n        strides (tuple of int, optional): The strides of the convolution. Defaults to (1, 1, 1).\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (tuple of int, optional): The dilation rate of the convolution. Defaults to (1, 1, 1).\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Can be a string name or a callable. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias term. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer for the kernel weights. Can be a string name or a callable. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias term. Can be a string name or a callable. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer for the kernel weights. Can be a string name or a callable. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias term. Can be a string name or a callable. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the layer activity. Can be a string name or a callable. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint for the kernel weights. Can be a string name or a callable. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias term. Can be a string name or a callable. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the base class.\n    \"\"\"\n```", "```python\nclass Conv3DTranspose(Conv3D):\n    \"\"\"\n    A 3D transposed convolution layer, also known as a deconvolution layer.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (tuple): The size of the convolution kernel.\n        strides (tuple, optional): The strides of the convolution. Defaults to (1, 1, 1).\n        padding (str, optional): The type of padding to use. Defaults to 'valid'.\n        output_padding (tuple, optional): Additional padding to add to the output. Defaults to None.\n        data_format (str, optional): The data format to use. Defaults to None.\n        dilation_rate (tuple, optional): The dilation rate of the convolution. Defaults to (1, 1, 1).\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the activity. Defaults to None.\n        kernel_constraint (str, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias vector. Defaults to None.\n\n    Returns:\n        A 3D tensor representing the output of the convolution.\n    \"\"\"\n```", "```python\nclass Cropping1D(Layer):\n    \"\"\"\n    Crops the input tensor along the time dimension (axis 1).\n\n    Args:\n        cropping (tuple of int): A tuple of two integers specifying the number of elements to crop from the start and end of the time dimension. Defaults to (1, 1).\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Returns:\n        Tensor: The cropped tensor with the specified number of elements removed from the start and end of the time dimension.\n\n    Example:\n        >>> cropping_layer = Cropping1D(cropping=(2, 3))\n        >>> input_tensor = tf.constant([[[1, 2, 3, 4, 5, 6, 7, 8]]], dtype=tf.float32)\n        >>> output_tensor = cropping_layer(input_tensor)\n        >>> print(output_tensor)\n        tf.Tensor([[[3 4 5]]], shape=(1, 1, 3), dtype=float32)\n    \"\"\"\n    def __init__(self, cropping=(1, 1), **kwargs):\n        super(Cropping1D, self).__init__(**kwargs)\n        self.cropping = conv_utils.normalize_tuple(cropping, 2, 'cropping')\n        self.input_spec = InputSpec(ndim=3)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if input_shape[1] is not None:\n            length = input_shape[1] - self.cropping[0] - self.cropping[1]\n        else:\n            length = None\n        return tensor_shape.TensorShape([input_shape[0], length, input_shape[2]])\n\n    def call(self, inputs):\n        if self.cropping[1] == 0:\n            return inputs[:, self.cropping[0]:, :]\n        else:\n            return inputs[:, self.cropping[0]:-self.cropping[1], :]\n\n    def get_config(self):\n        config = {'cropping': self.cropping}\n        base_config = super(Cropping1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass Cropping2D(Layer):\n    \"\"\"\n    Crops the input tensor along the spatial dimensions (height and width).\n\n    Args:\n        cropping (int, tuple of 2 ints, or tuple of 2 tuples of 2 ints): \n            - If an int, crops symmetrically on both sides of the input tensor.\n            - If a tuple of 2 ints, crops symmetrically on both sides of the height and width dimensions.\n            - If a tuple of 2 tuples of 2 ints, crops asymmetrically on the height and width dimensions.\n        data_format (str, optional): \n            - Specifies the data format of the input tensor. \n            - Can be 'channels_first' or 'channels_last'. \n            - Defaults to 'None', which uses the default data format of the backend.\n\n    Returns:\n        Tensor: The cropped tensor.\n\n    Raises:\n        ValueError: If the `cropping` argument is not in the expected format.\n    \"\"\"\n    def __init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs):\n        super(Cropping2D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if isinstance(cropping, int):\n            self.cropping = ((cropping, cropping), (cropping, cropping))\n        elif hasattr(cropping, '__len__'):\n            if len(cropping) != 2:\n                raise ValueError('`cropping` should have two elements. Found: ' + str(cropping))\n            height_cropping = conv_utils.normalize_tuple(cropping[0], 2, '1st entry of cropping')\n            width_cropping = conv_utils.normalize_tuple(cropping[1], 2, '2nd entry of cropping')\n            self.cropping = (height_cropping, width_cropping)\n        else:\n            raise ValueError('`cropping` should be either an int, a tuple of 2 ints (symmetric_height_crop, symmetric_width_crop), or a tuple of 2 tuples of 2 ints ((top_crop, bottom_crop), (left_crop, right_crop)). Found: ' + str(cropping))\n        self.input_spec = InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], input_shape[2] - self.cropping[0][0] - self.cropping[0][1] if input_shape[2] else None, input_shape[3] - self.cropping[1][0] - self.cropping[1][1] if input_shape[3] else None])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1] - self.cropping[0][0] - self.cropping[0][1] if input_shape[1] else None, input_shape[2] - self.cropping[1][0] - self.cropping[1][1] if input_shape[2] else None, input_shape[3]])\n\n    def call(self, inputs):\n        if self.data_format == 'channels_first':\n            if self.cropping[0][1] == self.cropping[1][1] == 0:\n                return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:]\n            elif self.cropping[0][1] == 0:\n                return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:-self.cropping[1][1]]\n            elif self.cropping[1][1] == 0:\n                return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:]\n            return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:-self.cropping[1][1]]\n        else:\n            if self.cropping[0][1] == self.cropping[1][1] == 0:\n                return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:, :]\n            elif self.cropping[0][1] == 0:\n                return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:-self.cropping[1][1], :]\n            elif self.cropping[1][1] == 0:\n                return inputs[:, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:, :]\n            return inputs[:, self.cropping[", "```python\nclass Cropping3D(Layer):\n    \"\"\"\n    Crops 3D input tensors.\n\n    Args:\n        cropping (int, tuple of 3 ints, or tuple of 3 tuples of 2 ints): Amount of cropping at each dimension.\n            If an integer, the same amount of cropping is applied to all dimensions.\n            If a tuple of 3 ints, it specifies the symmetric cropping for each dimension.\n            If a tuple of 3 tuples of 2 ints, it specifies the cropping for each dimension as (left, right).\n        data_format (str, optional): The data format of the input tensor ('channels_first' or 'channels_last'). Defaults to None.\n\n    Returns:\n        Tensor: Cropped 3D tensor.\n\n    Raises:\n        ValueError: If `cropping` is not an int, a tuple of 3 ints, or a tuple of 3 tuples of 2 ints.\n    \"\"\"\n    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs):\n        super(Cropping3D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if isinstance(cropping, int):\n            self.cropping = ((cropping, cropping), (cropping, cropping), (cropping, cropping))\n        elif hasattr(cropping, '__len__'):\n            if len(cropping) != 3:\n                raise ValueError('`cropping` should have 3 elements. Found: ' + str(cropping))\n            dim1_cropping = conv_utils.normalize_tuple(cropping[0], 2, '1st entry of cropping')\n            dim2_cropping = conv_utils.normalize_tuple(cropping[1], 2, '2nd entry of cropping')\n            dim3_cropping = conv_utils.normalize_tuple(cropping[2], 2, '3rd entry of cropping')\n            self.cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n        else:\n            raise ValueError('`cropping` should be either an int, a tuple of 3 ints (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), or a tuple of 3 tuples of 2 ints ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim2_crop)). Found: ' + str(cropping))\n        self.input_spec = InputSpec(ndim=5)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            if input_shape[2] is not None:\n                dim1 = input_shape[2] - self.cropping[0][0] - self.cropping[0][1]\n            else:\n                dim1 = None\n            if input_shape[3] is not None:\n                dim2 = input_shape[3] - self.cropping[1][0] - self.cropping[1][1]\n            else:\n                dim2 = None\n            if input_shape[4] is not None:\n                dim3 = input_shape[4] - self.cropping[2][0] - self.cropping[2][1]\n            else:\n                dim3 = None\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], dim1, dim2, dim3])\n        elif self.data_format == 'channels_last':\n            if input_shape[1] is not None:\n                dim1 = input_shape[1] - self.cropping[0][0] - self.cropping[0][1]\n            else:\n                dim1 = None\n            if input_shape[2] is not None:\n                dim2 = input_shape[2] - self.cropping[1][0] - self.cropping[1][1]\n            else:\n                dim2 = None\n            if input_shape[3] is not None:\n                dim3 = input_shape[3] - self.cropping[2][0] - self.cropping[2][1]\n            else:\n                dim3 = None\n            return tensor_shape.TensorShape([input_shape[0], dim1, dim2, dim3, input_shape[4]])\n\n    def call(self, inputs):\n        if self.data_format == 'channels_first':\n            if self.cropping[0][1] == self.cropping[1][1] == self.cropping[2][1] == 0:\n                return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:, self.cropping[2][0]:]\n            elif self.cropping[0][1] == self.cropping[1", "```python\nclass DBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"\n    DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together points that are packed closely together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for them to be considered as in the same neighborhood.\n\n    min_samples : int, default=5\n        The number of samples in a neighborhood for a point to be considered as a core point.\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in X. If metric is a string, it must be one of the options allowed by `sklearn.metrics.pairwise_distances`. If metric is callable, it should take two arrays representing samples and return a distance matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used to compute the nearest neighbors. If 'auto', the algorithm will be chosen based on the values of n_samples and n_features.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory usage.\n\n    p : float, default=None\n        The power parameter for the Minkowski metric. If None, then the Euclidean metric is used.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n\n    Attributes\n    ----------\n    core_sample_indices_ : array-like, shape (n_core_samples,)\n        Indices of core samples.\n\n    labels_ : array-like, shape (n_samples,)\n        Cluster labels for each point in the dataset. -1 means that the point is an outlier.\n\n    components_ : array-like, shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    Methods\n    -------\n    fit(X, y=None, sample_weight=None)\n        Compute DBSCAN clustering.\n\n    fit_predict(X, y=None, sample_weight=None)\n        Compute DBSCAN clustering and predict cluster labels for the samples.\n\n    _more_tags()\n        Additional tags for estimator.\n    \"\"\"\n```", "```python\nclass DepthwiseConv2D(Conv2D):\n    \"\"\"\n    Depthwise separable convolution layer.\n\n    This layer performs a depthwise spatial convolution followed by a pointwise spatial convolution.\n\n    Args:\n        kernel_size (tuple): A tuple of integers specifying the height and width of the convolution kernel.\n        strides (tuple, optional): A tuple of integers specifying the strides of the convolution. Defaults to (1, 1).\n        padding (str, optional): One of 'valid' or 'same'. Specifies the type of padding to use. Defaults to 'valid'.\n        depth_multiplier (int, optional): The number of depthwise convolution output channels for each input channel. Defaults to 1.\n        data_format (str, optional): One of 'channels_first' or 'channels_last'. Specifies the data format to use. Defaults to None.\n        dilation_rate (tuple, optional): A tuple of integers specifying the dilation rate of the convolution. Defaults to (1, 1).\n        activation (str or callable, optional): Activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str or callable, optional): Initializer for the depthwise kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): Initializer for the bias vector. Defaults to 'zeros'.\n        depthwise_regularizer (str or callable, optional): Regularizer for the depthwise kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): Regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): Regularizer for the output activity. Defaults to None.\n        depthwise_constraint (str or callable, optional): Constraint for the depthwise kernel. Defaults to None.\n        bias_constraint (str or callable, optional): Constraint for the bias vector. Defaults to None.\n\n    Returns:\n        A tensor representing the output of the depthwise separable convolution.\n    \"\"\"\n```", "```python\nclass Embedding(Layer):\n    \"\"\"\n    Creates an embedding layer that maps integer indices to dense vectors of fixed size.\n\n    Args:\n        input_dim (int): Size of the vocabulary, i.e., maximum integer index + 1.\n        output_dim (int): Dimension of the dense embedding.\n        embeddings_initializer (str or Initializer, optional): Initializer for the embeddings matrix. Defaults to 'uniform'.\n        embeddings_regularizer (str or Regularizer, optional): Regularizer function applied to the embeddings matrix. Defaults to None.\n        activity_regularizer (str or Regularizer, optional): Regularizer function applied to the output of the layer. Defaults to None.\n        embeddings_constraint (str or Constraint, optional): Constraint function applied to the embeddings matrix. Defaults to None.\n        mask_zero (bool, optional): If True, any input value greater than or equal to `input_dim` is masked as zero. Defaults to False.\n        input_length (int or list, optional): Length of input sequences. If a list, it must match the number of dimensions in the input shape. Defaults to None.\n\n    Raises:\n        ValueError: If `input_dim` or `output_dim` is not a positive integer.\n\n    Attributes:\n        input_dim (int): Size of the vocabulary.\n        output_dim (int): Dimension of the dense embedding.\n        embeddings_initializer (Initializer): Initializer for the embeddings matrix.\n        embeddings_regularizer (Regularizer): Regularizer function applied to the embeddings matrix.\n        activity_regularizer (Regularizer): Regularizer function applied to the output of the layer.\n        embeddings_constraint (Constraint): Constraint function applied to the embeddings matrix.\n        mask_zero (bool): If True, any input value greater than or equal to `input_dim` is masked as zero.\n        input_length (int or list): Length of input sequences.\n        embeddings (Variable): Embeddings matrix.\n\n    Methods:\n        build(input_shape): Builds the layer by creating the embeddings matrix.\n        compute_mask(inputs, mask): Computes the mask for the output.\n        compute_output_shape(input_shape): Computes the output shape of the layer.\n        call(inputs): Applies the embedding layer to the input.\n        get_config(): Returns a dictionary containing the configuration of the layer.\n    \"\"\"\n```", "```python\nclass Flask(App):\n    \"\"\"\n    A WSGI web application class that provides a simple framework for building web applications.\n\n    The Flask class is a subclass of the `App` class and provides additional functionality for handling requests, serving static files, and managing templates.\n\n    Attributes:\n        default_config (ImmutableDict): A dictionary containing default configuration values for the Flask application.\n        request_class (type[Request]): The class used to create request objects.\n        response_class (type[Response]): The class used to create response objects.\n        session_interface (SessionInterface): The session interface used to manage user sessions.\n\n    Methods:\n        __init__(self, import_name: str, static_url_path: str | None=None, static_folder: str | os.PathLike[str] | None='static', static_host: str | None=None, host_matching: bool=False, subdomain_matching: bool=False, template_folder: str | os.PathLike[str] | None='templates', instance_path: str | None=None, instance_relative_config: bool=False, root_path: str | None=None): Initializes a new Flask application.\n        get_send_file_max_age(self, filename: str | None) -> int | None: Returns the maximum age for sending a file.\n        send_static_file(self, filename: str) -> Response: Sends a static file to the client.\n        open_resource(self, resource: str, mode: str='rb', encoding: str | None=None) -> t.IO[t.AnyStr]: Opens a resource file.\n        open_instance_resource(self, resource: str, mode: str='rb', encoding: str | None='utf-8') -> t.IO[t.AnyStr]: Opens an instance resource file.\n        create_jinja_environment(self) -> Environment: Creates a Jinja2 environment for rendering templates.\n        create_url_adapter(self, request: Request | None) -> MapAdapter | None: Creates a URL adapter for the application.\n        raise_routing_exception(self, request: Request) -> t.NoReturn: Raises a routing exception if necessary.\n        update_template_context(self, context: dict[str, t.Any]) -> None: Updates the template context with additional variables.\n        make_shell_context(self) -> dict[str, t.Any]: Creates a shell context for interactive debugging.\n        run(self, host: str | None=None, port: int | None=None, debug: bool | None=None, load_dotenv: bool=True, **options: t.Any) -> None: Runs the Flask application.\n        test_client(self, use_cookies: bool=True, **kwargs: t.Any) -> FlaskClient: Creates a test client for testing the application.\n        test_cli_runner(self, **kwargs: t.Any) -> FlaskCliRunner: Creates a test CLI runner for testing the application.\n        handle_http_exception(self, e: HTTPException) -> HTTPException | ft.ResponseReturnValue: Handles HTTP exceptions.\n        handle_user_exception(self, e: Exception) -> HTTPException | ft.ResponseReturnValue: Handles user exceptions.\n        handle_exception(self, e: Exception) -> Response: Handles exceptions.\n        log_exception(self, exc_info: tuple[type, BaseException, TracebackType] | tuple[None, None, None]) -> None: Logs an exception.\n        dispatch_request(self) -> ft.ResponseReturnValue: Dispatches a request to the appropriate view function.\n        full_dispatch_request(self) -> Response: Dispatches a request and finalizes the response.\n        finalize_request(self, rv: ft.ResponseReturnValue | HTTPException, from_error_handler: bool=False) -> Response: Finalizes a response.\n        make_default_options_response(self) -> Response: Creates a default OPTIONS response.\n        ensure_sync(self, func: t.Callable[..., t.Any]) -> t.Callable[..., t.Any]: Ensures a function is synchronous.\n        async_to_sync(self, func: t.Callable[..., t.Coroutine[t.Any, t.Any, t.Any]]) -> t.Callable[..., t.Any]: Converts an asynchronous function to synchronous.\n        url_for(self, /, endpoint: str, *, _anchor: str | None=None, _method: str | None=None, _scheme: str | None=None, _external: bool | None=None, **values: t.Any) -> str: Generates a URL for a given endpoint.\n        make_response(self, rv: ft.ResponseReturnValue) -> Response: Creates a response object.\n        preprocess_request(self) -> ft.ResponseReturnValue | None: Preprocesses a request.\n        process_response(self, response: Response) -> Response: Processes a response.\n        do_teardown_request(self, exc: BaseException | None=_sentinel) -> None: Teardowns a request.\n        do_teardown_appcontext(self, exc: BaseException | None=_sentinel) -> None: Teardowns the application context.\n        app_context(self) -> AppContext: Creates an application context.\n        request_context(self,", "```python\nclass FunctionTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    A transformer that applies a custom function to the input data and optionally its inverse.\n\n    Parameters\n    ----------\n    func : callable or None, default=None\n        The function to apply to the input data. If None, the identity function is used.\n    inverse_func : callable or None, default=None\n        The inverse function to apply to the transformed data. If None, the inverse of `func` is not checked.\n    validate : bool, default=False\n        Whether to validate the input data.\n    accept_sparse : bool, default=False\n        Whether to accept sparse input data.\n    check_inverse : bool, default=True\n        Whether to check if the provided functions are strictly inverse of each other.\n    feature_names_out : callable, 'one-to-one', or None, default=None\n        The function to generate output feature names. If 'one-to-one', the output feature names are the same as the input feature names.\n    kw_args : dict or None, default=None\n        Additional keyword arguments to pass to `func`.\n    inv_kw_args : dict or None, default=None\n        Additional keyword arguments to pass to `inverse_func`.\n\n    Attributes\n    ----------\n    n_features_in_ : int\n        The number of input features.\n    feature_names_in_ : array-like of shape (n_features_in,)\n        Names of input features.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit the transformer to the input data.\n    transform(X)\n        Apply the transformer to the input data.\n    inverse_transform(X)\n        Apply the inverse transformer to the input data.\n    get_feature_names_out(input_features=None)\n        Get the output feature names.\n    set_output(transform=None)\n        Set the output format for the transformer.\n\n    Notes\n    -----\n    The `check_inverse` parameter is only supported when all the elements in `X` are numerical.\n    \"\"\"\n```", "```python\nclass GaussianMixture(BaseMixture):\n    \"\"\"\n    Gaussian Mixture Model (GMM) with support for various covariance types.\n\n    Parameters\n    ----------\n    n_components : int, default=1\n        The number of mixture components.\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n        Type of covariance parameters to use. Must be one of:\n        - 'full': each component has its own general covariance matrix.\n        - 'tied': all components share the same general covariance matrix.\n        - 'diag': each component has its own diagonal covariance matrix.\n        - 'spherical': each component has its own single variance.\n\n    tol : float, default=1e-3\n        The convergence threshold. EM iterations will stop when the\n        lower bound average gain is below this threshold.\n\n    reg_covar : float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance\n        matrices to improve numerical stability.\n\n    max_iter : int, default=100\n        The number of EM iterations to perform.\n\n    n_init : int, default=1\n        The number of times the EM algorithm will be run with different\n        initializations. The best result will be kept.\n\n    init_params : {'kmeans', 'random'}, default='kmeans'\n        The method used to initialize the weights, means and precisions.\n        Must be one of:\n        - 'kmeans': responsibilities are initialized using kmeans.\n        - 'random': responsibilities are initialized randomly.\n\n    weights_init : array-like of shape (n_components,), default=None\n        The user-provided initial weights, if any.\n\n    means_init : array-like of shape (n_components, n_features), default=None\n        The user-provided initial means, if any.\n\n    precisions_init : array-like, default=None\n        The user-provided initial precisions, if any. The shape depends on\n        `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to the method for reproducibility.\n\n    warm_start : bool, default=False\n        If 'true', the solution of the last fitting is used as initialization\n        for the next call.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints the current iteration\n        number and the lower bound for each iteration.\n\n    verbose_interval : int, default=10\n        Number of iterations between each verbose output.\n\n    Attributes\n    ----------\n    weights_ : array-like of shape (n_components,)\n        The weights of each mixture component.\n\n    means_ : array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component. The shape depends on\n        `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    precisions_ : array-like\n        The precision matrices for each mixture component. The shape depends on\n        `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    precisions_cholesky_ : array-like\n        The Cholesky decomposition of the precision matrices for each\n        mixture component. The shape depends on `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    n_features_in_ : int\n        Number of features seen during fit.\n\n    feature_names_in_ : ndarray of shape (n_features,)\n        Names of features seen during fit. Defined only if `input_features`\n        is given, otherwise set to `None`.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Estimate model parameters with the EM algorithm.\n\n    fit_predict(X, y=None)\n        Estimate model parameters using X and predict the labels for X.\n\n    predict(X)\n        Predict the labels for the data samples in X using the trained model.\n\n    predict_proba(X)\n        Evaluate the", "```python\nclass GlobalAveragePooling1D(GlobalPooling1D):\n    \"\"\"\n    Global average pooling layer for 1D data.\n\n    This layer computes the average value of each feature map across the spatial dimensions.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments to be passed to the base class.\n\n    Attributes:\n        supports_masking (bool): Indicates whether the layer supports masking.\n\n    Methods:\n        call(inputs, mask=None): Computes the output tensor by applying global average pooling.\n        compute_mask(inputs, mask=None): Returns None, as this layer does not support masking.\n    \"\"\"\n```", "```python\nclass GlobalAveragePooling2D(GlobalPooling2D):\n    \"\"\"\n    Applies global average pooling on a 4D tensor.\n\n    This layer computes the average value of each feature map in the input tensor,\n    resulting in a 2D tensor with the same number of channels as the input but with\n    a height and width of 1.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either\n            'channels_last' (default) or 'channels_first'. If 'channels_last', the input\n            tensor should have the shape (batch_size, height, width, channels). If\n            'channels_first', the input tensor should have the shape (batch_size, channels,\n            height, width).\n        keepdims (bool, optional): If True, the spatial dimensions are retained with\n            length 1. If False, the spatial dimensions are removed. Default is False.\n\n    Returns:\n        A 4D tensor with the same number of channels as the input but with a height\n        and width of 1.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n        else:\n            return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n```", "```python\nclass GlobalAveragePooling3D(GlobalPooling3D):\n    \"\"\"\n    Applies global average pooling on a 3D input (spatial dimensions).\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, retains the spatial dimensions of the input tensor. Defaults to False.\n\n    Returns:\n        Tensor: The output tensor after applying global average pooling.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n        else:\n            return backend.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n```", "```python\nclass GlobalMaxPooling1D(GlobalPooling1D):\n    \"\"\"\n    Applies global maximum pooling operation for 1D inputs.\n\n    This layer reduces the spatial dimensions of the input by taking the maximum value along the specified axis.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor ('channels_last' or 'channels_first'). Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, retains the reduced dimensions with length 1. Defaults to False.\n\n    Returns:\n        Tensor: The output tensor after applying global maximum pooling.\n    \"\"\"\n    def call(self, inputs):\n        steps_axis = 1 if self.data_format == 'channels_last' else 2\n        return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n```", "```python\nclass GlobalMaxPooling2D(GlobalPooling2D):\n    \"\"\"\n    Applies global maximum pooling operation on the input tensor.\n\n    This layer reduces the spatial dimensions of the input tensor to a single point by computing the maximum value along the spatial dimensions (height and width).\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. If 'channels_last', the input tensor is expected to have the shape (batch_size, height, width, channels). If 'channels_first', the input tensor is expected to have the shape (batch_size, channels, height, width).\n        keepdims (bool, optional): If True, the spatial dimensions are kept with size 1. If False, the spatial dimensions are removed. Default is False.\n\n    Returns:\n        Tensor: The output tensor after applying global maximum pooling, with reduced spatial dimensions.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n        else:\n            return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n```", "```python\nclass GlobalMaxPooling3D(GlobalPooling3D):\n    \"\"\"\n    Applies global maximum pooling operation for 3D data.\n\n    This layer reduces the spatial dimensions of the input tensor to a single point by computing the maximum value along the specified axes.\n\n    Args:\n        data_format (str, optional): Specifies the data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. The 'channels_last' format assumes the input tensor has the shape (batch_size, depth, height, width, channels), while 'channels_first' assumes the shape (batch_size, channels, depth, height, width).\n        keepdims (bool, optional): If True, the spatial dimensions are retained with length 1. If False, the spatial dimensions are removed. Default is False.\n\n    Returns:\n        Tensor: The output tensor after applying global maximum pooling.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n        else:\n            return backend.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n```", "```python\nclass GlobalPooling1D(Layer):\n    \"\"\"\n    Global 1D pooling layer for 3D input tensors.\n\n    This layer performs global pooling on the input tensor along the spatial dimension (axis 2),\n    reducing the spatial dimensions to a single value. The output tensor will have the same number of\n    channels as the input tensor, but the spatial dimensions will be reduced to 1.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_first' or 'channels_last'.\n            Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, the spatial dimensions will be kept as 1 in the output tensor.\n            If False, the spatial dimensions will be removed from the output tensor. Defaults to False.\n\n    Returns:\n        Tensor: The output tensor after global pooling.\n\n    Raises:\n        NotImplementedError: This method is not implemented and should be overridden by subclasses.\n    \"\"\"\n\n    def __init__(self, data_format='channels_last', keepdims=False, **kwargs):\n        super(GlobalPooling1D, self).__init__(**kwargs)\n        self.input_spec = InputSpec(ndim=3)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.keepdims = keepdims\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            if self.keepdims:\n                return tensor_shape.TensorShape([input_shape[0], input_shape[1], 1])\n            else:\n                return tensor_shape.TensorShape([input_shape[0], input_shape[1]])\n        elif self.keepdims:\n            return tensor_shape.TensorShape([input_shape[0], 1, input_shape[2]])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], input_shape[2]])\n\n    def call(self, inputs):\n        raise NotImplementedError\n\n    def get_config(self):\n        config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n        base_config = super(GlobalPooling1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass GlobalPooling2D(Layer):\n    \"\"\"\n    Global 2D pooling layer that reduces the spatial dimensions of the input tensor to a single point.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. Defaults to None.\n        keepdims (bool, optional): If True, the spatial dimensions are kept as 1x1, otherwise they are removed. Defaults to False.\n\n    Attributes:\n        data_format (str): The normalized data format of the input tensor.\n        input_spec (InputSpec): Specifies the expected input shape of the layer.\n        keepdims (bool): Whether to keep the spatial dimensions as 1x1.\n\n    Methods:\n        compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n        call(inputs): Defines the computation performed by the layer on the input tensor. Raises NotImplementedError.\n        get_config(): Returns a dictionary containing the configuration of the layer.\n    \"\"\"\n```", "```python\nclass GlobalPooling3D(Layer):\n    \"\"\"\n    Applies global pooling on a 3D input tensor.\n\n    This layer reduces the spatial dimensions of the input tensor to a single point by performing a pooling operation over the entire spatial extent of the input.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. If 'channels_last', the input shape is expected to be (batch_size, depth, height, width, channels). If 'channels_first', the input shape is expected to be (batch_size, channels, depth, height, width).\n        keepdims (bool, optional): If True, the spatial dimensions of the output tensor will be kept as 1, resulting in an output shape of (batch_size, 1, 1, 1, channels) if 'channels_last' or (batch_size, channels, 1, 1, 1) if 'channels_first'. If False, the spatial dimensions will be removed, resulting in an output shape of (batch_size, channels) if 'channels_last' or (batch_size, channels) if 'channels_first'. Default is False.\n\n    Returns:\n        Tensor: The output tensor after applying global pooling.\n\n    Raises:\n        NotImplementedError: This method is not implemented and should be overridden by subclasses.\n    \"\"\"\n\n    def __init__(self, data_format=None, keepdims=False, **kwargs):\n        super(GlobalPooling3D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=5)\n        self.keepdims = keepdims\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_last':\n            if self.keepdims:\n                return tensor_shape.TensorShape([input_shape[0], 1, 1, 1, input_shape[4]])\n            else:\n                return tensor_shape.TensorShape([input_shape[0], input_shape[4]])\n        elif self.keepdims:\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], 1, 1, 1])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1]])\n\n    def call(self, inputs):\n        raise NotImplementedError\n\n    def get_config(self):\n        config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n        base_config = super(GlobalPooling3D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    A class for splitting time series data into training and testing sets based on groups.\n\n    Parameters\n    ----------\n    test_size : int\n        The size of the test set.\n    train_size : int, optional\n        The size of the training set. If not specified, it will be calculated based on the other parameters.\n    n_splits : int, optional\n        The number of splits to generate. If not specified, it will be calculated based on the other parameters.\n    gap_size : int, default=0\n        The number of samples to leave between the end of the training set and the start of the test set.\n    shift_size : int, default=1\n        The number of samples to shift the training and test sets in each split.\n    window_type : str, default='rolling'\n        The type of window to use for splitting. Can be either 'rolling' or 'expanding'.\n\n    Raises\n    ------\n    ValueError\n        If both `train_size` and `n_splits` are not specified.\n        If `window_type` is not 'rolling' or 'expanding'.\n        If `train_size` is specified with an expanding window.\n        If the groups are not consecutive.\n        If there is not enough data to split the number of groups for the specified number of splits.\n\n    Attributes\n    ----------\n    test_size : int\n        The size of the test set.\n    train_size : int\n        The size of the training set.\n    n_splits : int\n        The number of splits to generate.\n    gap_size : int\n        The number of samples to leave between the end of the training set and the start of the test set.\n    shift_size : int\n        The number of samples to shift the training and test sets in each split.\n    window_type : str\n        The type of window to use for splitting.\n    _n_groups : int\n        The number of groups.\n    _train_start_idx : int\n        The starting index of the training set.\n\n    Methods\n    -------\n    split(X, y=None, groups=None)\n        Generate indices to split data into training and test sets.\n    get_n_splits(X=None, y=None, groups=None)\n        Return the number of splits.\n    _calculate_split_params()\n        Calculate the parameters for splitting the data.\n    \"\"\"\n```", "```python\nclass Kmeans(_BaseModel, _Cluster, _IterativeModel):\n    \"\"\"\n    K-means clustering algorithm implementation.\n\n    Parameters\n    ----------\n    k : int\n        The number of clusters to form as well as the number of centroids to generate.\n    max_iter : int, optional (default=10)\n        Maximum number of iterations of the k-means algorithm for a single run.\n    convergence_tolerance : float, optional (default=1e-05)\n        Tolerance for stopping criteria.\n    random_seed : int, optional (default=None)\n        Seed for the random number generator to ensure reproducibility.\n    print_progress : int, optional (default=0)\n        If greater than 0, print progress every `print_progress` iterations.\n\n    Attributes\n    ----------\n    centroids_ : array-like, shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n    clusters_ : dict\n        Dictionary where keys are cluster indices and values are lists of sample indices belonging to each cluster.\n    iterations_ : int\n        Number of iterations run.\n    _is_fitted : bool\n        Whether the model has been fitted.\n\n    Methods\n    -------\n    fit(X, init_params=True)\n        Compute k-means clustering.\n    _get_cluster_idx(X, centroids)\n        Assign samples to the closest cluster.\n    _predict(X)\n        Predict the closest cluster for each sample in X.\n    \"\"\"\n```", "```python\nclass LabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Transforms multiclass and multilabel classification problems into a binary label format.\n\n    Parameters\n    ----------\n    neg_label : int, default=0\n        The label for the negative class.\n    pos_label : int, default=1\n        The label for the positive class.\n    sparse_output : bool, default=False\n        If True, the output will be in sparse format.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        The classes encountered during fit.\n    y_type_ : str\n        The type of target data encountered during fit.\n    sparse_input_ : bool\n        Whether the input data was in sparse format.\n\n    Methods\n    -------\n    fit(y)\n        Fit the LabelBinarizer to the target data.\n    fit_transform(y)\n        Fit the LabelBinarizer to the target data and return the transformed data.\n    transform(y)\n        Transform the target data.\n    inverse_transform(Y, threshold=None)\n        Invert the transformation and return the original target data.\n\n    Raises\n    ------\n    ValueError\n        If `neg_label` is not strictly less than `pos_label`.\n        If `sparse_output` is True and `pos_label` is not 0 or `neg_label` is not 0.\n        If the target data is multioutput.\n        If the target data has 0 samples.\n        If the object was not fitted with multilabel input.\n    \"\"\"\n```", "```python\nclass LabelEncoder(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Encodes target labels with values between 0 and n_classes-1.\n\n    This transformer converts a collection of label strings to a sequence of\n    integers, where each integer corresponds to a unique label.\n\n    Parameters\n    ----------\n    None\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        A list of unique labels encountered during fitting.\n\n    Methods\n    -------\n    fit(y)\n        Fit label encoder.\n\n    fit_transform(y)\n        Fit label encoder and return encoded labels.\n\n    transform(y)\n        Transform labels to normalized encoding.\n\n    inverse_transform(y)\n        Transform labels back to original encoding.\n\n    _more_tags()\n        Additional tags for the transformer.\n    \"\"\"\n\n    def fit(self, y):\n        \"\"\"\n        Fit label encoder.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target labels to encode.\n\n        Returns\n        -------\n        self : LabelEncoder\n            Fitted label encoder.\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        self.classes_ = _unique(y)\n        return self\n\n    def fit_transform(self, y):\n        \"\"\"\n        Fit label encoder and return encoded labels.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target labels to encode.\n\n        Returns\n        -------\n        y : array of shape (n_samples,)\n            Encoded labels.\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        self.classes_, y = _unique(y, return_inverse=True)\n        return y\n\n    def transform(self, y):\n        \"\"\"\n        Transform labels to normalized encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target labels to encode.\n\n        Returns\n        -------\n        y : array of shape (n_samples,)\n            Encoded labels.\n        \"\"\"\n        check_is_fitted(self)\n        y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n        if _num_samples(y) == 0:\n            return np.array([])\n        return _encode(y, uniques=self.classes_)\n\n    def inverse_transform(self, y):\n        \"\"\"\n        Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Encoded labels.\n\n        Returns\n        -------\n        y : array of shape (n_samples,)\n            Original labels.\n        \"\"\"\n        check_is_fitted(self)\n        y = column_or_1d(y, warn=True)\n        if _num_samples(y) == 0:\n            return np.array([])\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if len(diff):\n            raise ValueError('y contains previously unseen labels: %s' % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n    def _more_tags(self):\n        \"\"\"\n        Additional tags for the transformer.\n\n        Returns\n        -------\n        dict\n            Additional tags.\n        \"\"\"\n        return {'X_types': ['1dlabels']}\n```", "```python\nclass LinearRegression(_BaseModel, _IterativeModel, _Regressor):\n    \"\"\"\n    A class for implementing linear regression models.\n\n    Parameters\n    ----------\n    method : str, optional (default='direct')\n        The method to use for fitting the model. Supported methods are 'direct', 'sgd', 'svd', and 'qr'.\n    eta : float, optional (default=0.01)\n        The learning rate for stochastic gradient descent (SGD) method.\n    epochs : int, optional (default=50)\n        The number of epochs to run for the iterative methods.\n    minibatches : int or None, optional (default=None)\n        The number of minibatches to use for SGD. If None, the entire dataset is used.\n    random_seed : int or None, optional (default=None)\n        The seed for the random number generator for reproducibility.\n    print_progress : int, optional (default=0)\n        The frequency of printing progress during training. If 0, no progress is printed.\n\n    Attributes\n    ----------\n    b_ : array-like, shape (1,)\n        The bias term of the model.\n    w_ : array-like, shape (n_features,)\n        The weight vector of the model.\n    cost_ : list\n        The list of cost values at each epoch during training.\n\n    Methods\n    -------\n    fit(X, y, init_params=True)\n        Fit the linear regression model to the training data.\n\n    predict(X)\n        Predict the target values for the input data.\n\n    _normal_equation(X, y)\n        Compute the parameters using the normal equation.\n\n    _net_input(X)\n        Compute the net input for the input data.\n\n    _sum_squared_error_cost(y, y_val)\n        Compute the sum of squared error cost.\n    \"\"\"\n```", "```python\nclass LogisticRegression(_BaseModel, _IterativeModel, _Classifier):\n    \"\"\"\n    Logistic Regression classifier using gradient descent.\n\n    Parameters\n    ----------\n    eta : float, optional (default=0.01)\n        Learning rate for gradient descent.\n    epochs : int, optional (default=50)\n        Number of iterations to perform gradient descent.\n    l2_lambda : float, optional (default=0.0)\n        Regularization parameter for L2 regularization.\n    minibatches : int, optional (default=1)\n        Number of minibatches to use for gradient descent.\n    random_seed : int, optional (default=None)\n        Seed for random number generator for reproducibility.\n    print_progress : int, optional (default=0)\n        Frequency of progress printing during training.\n\n    Attributes\n    ----------\n    w_ : array-like, shape (n_features,)\n        Weights after fitting.\n    b_ : array-like, shape (1,)\n        Bias after fitting.\n    cost_ : list\n        List of costs computed during training.\n    init_time_ : float\n        Time taken to initialize the model.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the model to the training data.\n    predict(X)\n        Predict class labels for samples in X.\n    predict_proba(X)\n        Predict class probabilities for samples in X.\n    \"\"\"\n\n    def __init__(self, eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0):\n        _BaseModel.__init__(self)\n        _IterativeModel.__init__(self)\n        _Classifier.__init__(self)\n        self.eta = eta\n        self.epochs = epochs\n        self.l2_lambda = l2_lambda\n        self.minibatches = minibatches\n        self.random_seed = random_seed\n        self.print_progress = print_progress\n        self._is_fitted = False\n\n    def _forward(self, X):\n        \"\"\"\n        Compute the forward pass of the logistic regression model.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        a : array-like, shape (n_samples,)\n            Predicted probabilities.\n        \"\"\"\n        z = self._net_input(X)\n        a = self._sigmoid_activation(z)\n        return a\n\n    def _backward(self, X, y_true, y_probas):\n        \"\"\"\n        Compute the gradients of the loss function with respect to the weights and bias.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y_true : array-like, shape (n_samples,)\n            True binary labels.\n        y_probas : array-like, shape (n_samples,)\n            Predicted probabilities.\n\n        Returns\n        -------\n        grad_loss_wrt_w : array-like, shape (n_features,)\n            Gradient of the loss with respect to the weights.\n        grad_loss_wrt_b : float\n            Gradient of the loss with respect to the bias.\n        \"\"\"\n        grad_loss_wrt_out = y_true - y_probas\n        grad_loss_wrt_w = -X.T @ grad_loss_wrt_out.reshape(-1, 1)\n        grad_loss_wrt_b = -np.sum(grad_loss_wrt_out)\n        return (grad_loss_wrt_w, grad_loss_wrt_b)\n\n    def _fit(self, X, y, init_params=True):\n        \"\"\"\n        Fit the model to the training data using gradient descent.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n        y : array-like, shape (n_samples,)\n            True binary labels.\n        init_params : bool, optional (default=True)\n            Whether to initialize the parameters.\n\n        Returns\n        -------\n        self : LogisticRegression\n            Fitted model.\n        \"\"\"\n        self._check_target_array(y, allowed={(0, 1)})\n        if init_params:\n            self.b_, self.w_ = self._init_params(weights_shape=(X.shape[1], 1), bias_shape=(1,), random_seed=self.random_seed)\n            self.cost_ = []\n        self.init_time_ = time()\n        rgen = np.random.RandomState(self.random_seed)\n        for i in range(self.epochs):\n            for idx in self._yield_minibatches_idx(rgen=rgen, n_batches=self.minibatches, data_ary=y, shuffle=True):\n                y_val = self._forward(X[idx])\n                grad_loss_wrt_w, grad_loss_wrt_b = self._backward(X[idx], y_true=y[idx], y_probas=y_val)\n                l2_reg = self.l2_lambda * self.w_\n                self.w_ += self.eta * (-grad_loss_wrt_w", "```python\n@keras_export(['keras.Loss', 'keras.losses.Loss'])\nclass Loss(KerasSaveable):\n    \"\"\"\n    Base class for all losses in Keras.\n\n    Args:\n        name (str, optional): The name of the loss function. If not provided, it will be automatically generated.\n        reduction (str, optional): The type of reduction to apply to the loss. Can be 'sum_over_batch_size' or 'none'. Defaults to 'sum_over_batch_size'.\n        dtype (str, optional): The data type of the loss function. If not provided, it will default to the backend's floatx value.\n\n    Attributes:\n        name (str): The name of the loss function.\n        reduction (str): The type of reduction to apply to the loss.\n        dtype (str): The data type of the loss function.\n\n    Methods:\n        __call__(y_true, y_pred, sample_weight=None): Computes the loss value for the given true and predicted values.\n        call(y_true, y_pred): Computes the loss value for the given true and predicted values. This method should be overridden by subclasses.\n        get_config(): Returns a dictionary containing the configuration of the loss function.\n        from_config(config): Creates a new instance of the loss function from the given configuration.\n        _obj_type(): Returns the type of the object as a string.\n    \"\"\"\n```", "```python\nclass MaxPooling1D(Pooling1D):\n    \"\"\"\n    Applies a 1D max pooling operation to the input.\n\n    Args:\n        pool_size (int, tuple of int): The size of the pooling window. If a tuple is provided, it should be of length 1.\n        strides (int, tuple of int, optional): The strides of the pooling window. If a tuple is provided, it should be of length 1. Defaults to the value of `pool_size`.\n        padding (str, optional): The padding method to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        A 1D tensor after applying the max pooling operation.\n    \"\"\"\n    def __init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs):\n        super(MaxPooling1D, self).__init__(functools.partial(backend.pool2d, pool_mode='max'), pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\nclass MaxPooling2D(Pooling2D):\n    \"\"\"\n    Applies a 2D max pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window (height, width). Default is (2, 2).\n        strides (tuple of int, optional): The strides of the pooling window (height, width). If None, the strides are set to the pool size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_last' or 'channels_first'. Default is None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The result of the max pooling operation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        super(MaxPooling2D, self).__init__(nn.max_pool, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\nclass MaxPooling3D(Pooling3D):\n    \"\"\"\n    Applies a 3D max pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window in the format (depth, height, width). Default is (2, 2, 2).\n        strides (tuple of int, optional): The strides of the pooling window in the format (depth, height, width). If None, the strides are set to the pool_size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_first' or 'channels_last'. Default is None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The result of the max pooling operation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        super(MaxPooling3D, self).__init__(nn.max_pool3d, pool_size=pool_size, strides=strides, padding=padding, data_format=data_format, **kwargs)\n```", "```python\n@keras_export(['keras.Metric', 'keras.metrics.Metric'])\nclass Metric(KerasSaveable):\n    \"\"\"\n    Base class for all metrics.\n\n    Args:\n        dtype: The data type of the metric result.\n        name: The name of the metric.\n\n    Attributes:\n        name: The name of the metric.\n        dtype: The data type of the metric result.\n        _dtype_policy: The data type policy for the metric.\n        _dtype: The computed data type of the metric.\n        _metrics: A list of nested metrics.\n        _variables: A list of variables tracked by the metric.\n        _tracker: A tracker for variables and metrics.\n\n    Methods:\n        reset_state: Resets the state of the metric.\n        update_state: Updates the state of the metric with new values.\n        stateless_update_state: Updates the state of the metric in a stateless manner.\n        result: Computes the result of the metric.\n        stateless_result: Computes the result of the metric in a stateless manner.\n        stateless_reset_state: Resets the state of the metric in a stateless manner.\n        dtype: Returns the data type of the metric result.\n        _obj_type: Returns the object type of the metric.\n        add_variable: Adds a variable to the metric.\n        add_weight: Adds a weight to the metric.\n        variables: Returns a list of variables tracked by the metric.\n        __call__: Updates the state of the metric and returns the result.\n        get_config: Returns the configuration of the metric.\n        from_config: Creates a metric from a configuration.\n        __setattr__: Sets an attribute and tracks it if necessary.\n        _check_super_called: Checks if the superclass constructor was called.\n        __repr__: Returns a string representation of the metric.\n        __str__: Returns a string representation of the metric.\n    \"\"\"\n```", "```python\nclass MultiLabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Transforms a collection of label sets into a binary matrix representation.\n\n    Parameters\n    ----------\n    classes : array-like, optional\n        An array of unique class labels. If not provided, the classes will be inferred from the data.\n    sparse_output : bool, default=False\n        If True, the output will be a sparse matrix. Otherwise, it will be a dense matrix.\n\n    Attributes\n    ----------\n    classes_ : array\n        An array of unique class labels used for binarization.\n    _cached_dict : dict, optional\n        A cached dictionary for class to index mapping.\n\n    Methods\n    -------\n    fit(y)\n        Fit the MultiLabelBinarizer to the data.\n    fit_transform(y)\n        Fit the MultiLabelBinarizer to the data and transform it.\n    transform(y)\n        Transform the data using the fitted MultiLabelBinarizer.\n    inverse_transform(yt)\n        Transform the binary matrix back to the original label sets.\n    _more_tags()\n        Additional tags for the estimator.\n    \"\"\"\n\n    _parameter_constraints: dict = {'classes': ['array-like', None], 'sparse_output': ['boolean']}\n\n    def __init__(self, *, classes=None, sparse_output=False):\n        self.classes = classes\n        self.sparse_output = sparse_output\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, y):\n        \"\"\"\n        Fit the MultiLabelBinarizer to the data.\n\n        Parameters\n        ----------\n        y : array-like\n            A collection of label sets.\n\n        Returns\n        -------\n        self : MultiLabelBinarizer\n            The fitted MultiLabelBinarizer.\n        \"\"\"\n        self._cached_dict = None\n        if self.classes is None:\n            classes = sorted(set(itertools.chain.from_iterable(y)))\n        elif len(set(self.classes)) < len(self.classes):\n            raise ValueError('The classes argument contains duplicate classes. Remove these duplicates before passing them to MultiLabelBinarizer.')\n        else:\n            classes = self.classes\n        dtype = int if all((isinstance(c, int) for c in classes)) else object\n        self.classes_ = np.empty(len(classes), dtype=dtype)\n        self.classes_[:] = classes\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, y):\n        \"\"\"\n        Fit the MultiLabelBinarizer to the data and transform it.\n\n        Parameters\n        ----------\n        y : array-like\n            A collection of label sets.\n\n        Returns\n        -------\n        yt : array\n            The binary matrix representation of the label sets.\n        \"\"\"\n        if self.classes is not None:\n            return self.fit(y).transform(y)\n        self._cached_dict = None\n        class_mapping = defaultdict(int)\n        class_mapping.default_factory = class_mapping.__len__\n        yt = self._transform(y, class_mapping)\n        tmp = sorted(class_mapping, key=class_mapping.get)\n        dtype = int if all((isinstance(c, int) for c in tmp)) else object\n        class_mapping = np.empty(len(tmp), dtype=dtype)\n        class_mapping[:] = tmp\n        self.classes_, inverse = np.unique(class_mapping, return_inverse=True)\n        yt.indices = np.asarray(inverse[yt.indices], dtype=yt.indices.dtype)\n        if not self.sparse_output:\n            yt = yt.toarray()\n        return yt\n\n    def transform(self, y):\n        \"\"\"\n        Transform the data using the fitted MultiLabelBinarizer.\n\n        Parameters\n        ----------\n        y : array-like\n            A collection of label sets.\n\n        Returns\n        -------\n        yt : array\n            The binary matrix representation of the label sets.\n        \"\"\"\n        check_is_fitted(self)\n        class_to_index = self._build_cache()\n        yt = self._transform(y, class_to_index)\n        if not self.sparse_output:\n            yt = yt.toarray()\n        return yt\n\n    def _build_cache(self):\n        \"\"\"\n        Build a cache for class to index mapping.\n\n        Returns\n        -------\n        class_to_index : dict\n            A dictionary mapping classes to indices.\n        \"\"\"\n        if self._cached_dict is None:\n            self._cached_dict = dict(zip(self.classes_, range(len(self.classes_))))\n        return self._cached_dict\n\n    def _transform(self, y, class_mapping):\n        \"\"\"\n        Transform the label sets into a binary matrix.\n\n        Parameters\n        ----------\n        y : array-like\n            A collection of label sets.\n        class_mapping : dict\n            A dictionary mapping classes to indices.\n\n        Returns\n        -------\n        yt : csr_matrix\n            The binary matrix representation of the label sets.\n        \"\"\"\n        indices = array.array('i')\n        indptr = array.array('i', [0])\n", "```python\nclass OneHotEncoder(_BaseEncoder):\n    \"\"\"\n    Encodes categorical features as a one-hot numeric array.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list of lists/arrays of dtype object, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list : Categories given explicitly as a list of arrays. Each array\n          contains the categories for the corresponding feature.\n\n    drop : {'first', 'if_binary'} or array-like of shape (n_features,), default=None\n        Specifies a methodology to drop one of the categories per feature. This\n        is useful in the case of collinearity. By default, no categories are\n        dropped. The following options are available for ``drop``:\n\n        - 'first' : Drop the first category in each feature. If only one\n          category is present, the feature will be dropped entirely.\n        - 'if_binary' : Drop the first category if the feature contains\n          only two categories. If more than two categories are present, no\n          categories are dropped.\n\n    sparse_output : bool, default=True\n        Whether the output should be in sparse format.\n\n    dtype : dtype, default=np.float64\n        Desired dtype of output.\n\n    handle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, default='error'\n        Specifies how to handle unknown categories:\n\n        - 'error' : Raise an error when an unknown category is encountered.\n        - 'ignore' : Ignore unknown categories and treat them as if they were\n          not present in the training data.\n        - 'infrequent_if_exist' : Treat unknown categories as infrequent if\n          they exist in the training data.\n\n    min_frequency : int or float, default=None\n        Minimum frequency of a category to be considered frequent. If a category\n        appears less than this frequency, it will be treated as infrequent.\n\n    max_categories : int, default=None\n        Maximum number of categories to be considered. If the number of unique\n        categories in a feature exceeds this value, the feature will be dropped.\n\n    feature_name_combiner : {'concat'} or callable, default='concat'\n        Method to combine feature names with category names. If 'concat', the\n        feature name and category name are concatenated with an underscore. If a\n        callable, it should take two arguments (feature name and category name)\n        and return a string.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature.\n\n    drop_idx_ : array of shape (n_features,)\n        Indices of categories to drop for each feature.\n\n    n_features_out_ : int\n        Number of output features.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit OneHotEncoder to X.\n\n    transform(X)\n        Transform X using one-hot encoding.\n\n    inverse_transform(X)\n        Transform X back to the original space.\n\n    get_feature_names_out(input_features=None)\n        Get output feature names for transformation.\n\n    _map_drop_idx_to_infrequent(feature_idx, drop_idx)\n        Map drop index to infrequent category if enabled.\n\n    _set_drop_idx()\n        Set drop indices for each feature.\n\n    _compute_transformed_categories(i, remove_dropped=True)\n        Compute transformed categories for feature i.\n\n    _remove_dropped_categories(categories, i)\n        Remove dropped categories from feature i.\n\n    _compute_n_features_outs()\n        Compute the number of output features for each feature.\n\n    _fit(X, handle_unknown, force_all_finite)\n        Fit the OneHotEncoder to X.\n\n    _transform(X, handle_unknown, force_all_finite, warn_on_unknown)\n        Transform X using the fitted OneHotEncoder.\n\n    _check_get_feature_name_combiner()\n        Check and return the feature name combiner.\n    \"\"\"\n```", "```python\nclass OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"\n    Perform OPTICS (Ordering Points To Identify the Clustering Structure) clustering.\n\n    OPTICS is an algorithm for clustering based on density. It is particularly useful for discovering clusters of arbitrary shapes and sizes in data.\n\n    Parameters\n    ----------\n    min_samples : int or float, default=5\n        The number of samples in a neighborhood for a point to be considered as a core point. If float, it represents a fraction of the total number of samples.\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n\n    metric : str or callable, default='minkowski'\n        The distance metric to use. It can be a string or a callable. Some valid string values are 'euclidean', 'manhattan', 'chebyshev', 'minkowski', etc.\n\n    p : float, default=2\n        The power parameter for the Minkowski metric. When p = 1, it is equivalent to the Manhattan distance, and when p = 2, it is equivalent to the Euclidean distance.\n\n    metric_params : dict or None, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The method to use for clustering. It can be 'dbscan' or 'xi'.\n\n    eps : float or None, default=None\n        The maximum distance between two samples for one to be considered as in the neighborhood of the other. If None, it is set to max_eps.\n\n    xi : float, default=0.05\n        The xi parameter for the xi method. It controls the density threshold for forming clusters.\n\n    predecessor_correction : bool, default=True\n        Whether to use predecessor correction for the xi method.\n\n    min_cluster_size : int, float, or None, default=None\n        The minimum number of samples in a cluster. If float, it represents a fraction of the total number of samples.\n\n    algorithm : str, default='auto'\n        The algorithm to use for computing the OPTICS graph. It can be 'auto', 'brute', 'ball_tree', or 'kd_tree'.\n\n    leaf_size : int, default=30\n        The leaf size for the ball tree or kd tree algorithm.\n\n    memory : str or HasMethods('cache'), default=None\n        Used to cache the computation of the OPTICS graph.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for the computation of the OPTICS graph.\n\n    Attributes\n    ----------\n    labels_ : array of shape (n_samples,)\n        The cluster labels for each sample.\n\n    ordering_ : array of shape (n_samples,)\n        The ordering of the samples based on their density.\n\n    core_distances_ : array of shape (n_samples,)\n        The core distances of the samples.\n\n    reachability_ : array of shape (n_samples,)\n        The reachability distances of the samples.\n\n    predecessor_ : array of shape (n_samples,)\n        The predecessor of each sample in the ordering.\n\n    cluster_hierarchy_ : dict\n        The hierarchical clustering structure for the xi method.\n    \"\"\"\n```", "```python\nclass OrdinalEncoder(OneToOneFeatureMixin, _BaseEncoder):\n    \"\"\"\n    Encodes categorical features as ordinal integers.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list of lists, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list : List of categories for each feature. Each list should be ordered\n          such that the first element is the most frequent value.\n\n    dtype : type, default=np.float64\n        Desired dtype of output.\n\n    handle_unknown : {'error', 'use_encoded_value'}, default='error'\n        Strategy to deal with unknown categories:\n\n        - 'error' : Raise an error when an unknown category is encountered during\n          transform.\n        - 'use_encoded_value' : Assign a value to unknown categories.\n\n    unknown_value : int or np.nan, default=None\n        Value to assign to unknown categories. Only used if\n        `handle_unknown` is 'use_encoded_value'.\n\n    encoded_missing_value : int or np.nan, default=np.nan\n        Value to encode missing values. If `dtype` is not a float dtype and\n        `encoded_missing_value` is np.nan, a ValueError will be raised.\n\n    min_frequency : int or None, default=None\n        Minimum frequency of categories to be included. Categories with a\n        frequency lower than this value will be considered as 'infrequent'.\n\n    max_categories : int or None, default=None\n        Maximum number of categories to be included. If the number of unique\n        categories exceeds this value, the least frequent categories will be\n        considered as 'infrequent'.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        Categories of each feature.\n\n    encoded_missing_value_ : int or np.nan\n        Value used to encode missing values.\n\n    unknown_value_ : int or np.nan\n        Value used to encode unknown categories.\n\n    min_frequency_ : int or None\n        Minimum frequency of categories to be included.\n\n    max_categories_ : int or None\n        Maximum number of categories to be included.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit the OrdinalEncoder to X.\n\n    transform(X)\n        Transform X using the fitted OrdinalEncoder.\n\n    inverse_transform(X)\n        Convert the encoded data back to the original representation.\n\n    Notes\n    -----\n    The `encoded_missing_value` and `unknown_value` parameters should be chosen\n    carefully to avoid conflicts with the encoded values of known categories.\n    \"\"\"\n```", "```python\nclass Pooling1D(Layer):\n    \"\"\"\n    Applies a 1D pooling operation to the input.\n\n    Args:\n        pool_function (callable): The pooling function to use (e.g., `tf.nn.max_pool1d`).\n        pool_size (int or tuple): The size of the pooling window.\n        strides (int or tuple): The strides of the pooling operation.\n        padding (str, optional): The type of padding to use ('valid' or 'same'). Defaults to 'valid'.\n        data_format (str, optional): The data format of the input ('channels_last' or 'channels_first'). Defaults to 'channels_last'.\n        name (str, optional): The name of the layer. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor after applying the pooling operation.\n\n    Raises:\n        ValueError: If the data format is not supported.\n    \"\"\"\n    def __init__(self, pool_function, pool_size, strides, padding='valid', data_format='channels_last', name=None, **kwargs):\n        super(Pooling1D, self).__init__(name=name, **kwargs)\n        if data_format is None:\n            data_format = backend.image_data_format()\n        if strides is None:\n            strides = pool_size\n        self.pool_function = pool_function\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=3)\n\n    def call(self, inputs):\n        pad_axis = 2 if self.data_format == 'channels_last' else 3\n        inputs = array_ops.expand_dims(inputs, pad_axis)\n        outputs = self.pool_function(inputs, self.pool_size + (1,), strides=self.strides + (1,), padding=self.padding, data_format=self.data_format)\n        return array_ops.squeeze(outputs, pad_axis)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            steps = input_shape[2]\n            features = input_shape[1]\n        else:\n            steps = input_shape[1]\n            features = input_shape[2]\n        length = conv_utils.conv_output_length(steps, self.pool_size[0], self.padding, self.strides[0])\n        if self.data_format == 'channels_first':\n            return tensor_shape.TensorShape([input_shape[0], features, length])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], length, features])\n\n    def get_config(self):\n        config = {'strides': self.strides, 'pool_size': self.pool_size, 'padding': self.padding, 'data_format': self.data_format}\n        base_config = super(Pooling1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass Pooling2D(Layer):\n    \"\"\"\n    Applies a 2D pooling operation to the input tensor.\n\n    Args:\n        pool_function (callable): The pooling function to apply, such as `tf.nn.max_pool` or `tf.nn.avg_pool`.\n        pool_size (tuple of int): The size of the pooling window (height, width).\n        strides (tuple of int): The strides of the pooling window (height, width).\n        padding (str, optional): The type of padding to use ('valid' or 'same'). Defaults to 'valid'.\n        data_format (str, optional): The data format of the input tensor ('channels_first' or 'channels_last'). Defaults to the backend's default data format.\n        name (str, optional): The name of the layer. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor after applying the pooling operation.\n\n    Raises:\n        ValueError: If an invalid padding type is provided.\n    \"\"\"\n    def __init__(self, pool_function, pool_size, strides, padding='valid', data_format=None, name=None, **kwargs):\n        super(Pooling2D, self).__init__(name=name, **kwargs)\n        if data_format is None:\n            data_format = backend.image_data_format()\n        if strides is None:\n            strides = pool_size\n        self.pool_function = pool_function\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=4)\n\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            pool_shape = (1,) + self.pool_size + (1,)\n            strides = (1,) + self.strides + (1,)\n        else:\n            pool_shape = (1, 1) + self.pool_size\n            strides = (1, 1) + self.strides\n        outputs = self.pool_function(inputs, ksize=pool_shape, strides=strides, padding=self.padding.upper(), data_format=conv_utils.convert_data_format(self.data_format, 4))\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            rows = input_shape[2]\n            cols = input_shape[3]\n        else:\n            rows = input_shape[1]\n            cols = input_shape[2]\n        rows = conv_utils.conv_output_length(rows, self.pool_size[0], self.padding, self.strides[0])\n        cols = conv_utils.conv_output_length(cols, self.pool_size[1], self.padding, self.strides[1])\n        if self.data_format == 'channels_first':\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], rows, cols])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], rows, cols, input_shape[3]])\n\n    def get_config(self):\n        config = {'pool_size': self.pool_size, 'padding': self.padding, 'strides': self.strides, 'data_format': self.data_format}\n        base_config = super(Pooling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass Pooling3D(Layer):\n    \"\"\"\n    Applies a 3D pooling operation to the input tensor.\n\n    Args:\n        pool_function (callable): The pooling function to use (e.g., `tf.nn.max_pool3d`).\n        pool_size (tuple of int): The size of the pooling window (depth, height, width).\n        strides (tuple of int): The strides of the pooling window (depth, height, width).\n        padding (str, optional): The padding method ('valid' or 'same'). Defaults to 'valid'.\n        data_format (str, optional): The data format ('channels_last' or 'channels_first'). Defaults to 'channels_last'.\n        name (str, optional): The name of the layer. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor after applying the 3D pooling operation.\n\n    Raises:\n        ValueError: If an invalid padding or data format is provided.\n    \"\"\"\n    def __init__(self, pool_function, pool_size, strides, padding='valid', data_format='channels_last', name=None, **kwargs):\n        super(Pooling3D, self).__init__(name=name, **kwargs)\n        if data_format is None:\n            data_format = backend.image_data_format()\n        if strides is None:\n            strides = pool_size\n        self.pool_function = pool_function\n        self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n        self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=5)\n\n    def call(self, inputs):\n        pool_shape = (1,) + self.pool_size + (1,)\n        strides = (1,) + self.strides + (1,)\n        if self.data_format == 'channels_first':\n            inputs = array_ops.transpose(inputs, (0, 2, 3, 4, 1))\n        outputs = self.pool_function(inputs, ksize=pool_shape, strides=strides, padding=self.padding.upper())\n        if self.data_format == 'channels_first':\n            outputs = array_ops.transpose(outputs, (0, 4, 1, 2, 3))\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            len_dim1 = input_shape[2]\n            len_dim2 = input_shape[3]\n            len_dim3 = input_shape[4]\n        else:\n            len_dim1 = input_shape[1]\n            len_dim2 = input_shape[2]\n            len_dim3 = input_shape[3]\n        len_dim1 = conv_utils.conv_output_length(len_dim1, self.pool_size[0], self.padding, self.strides[0])\n        len_dim2 = conv_utils.conv_output_length(len_dim2, self.pool_size[1], self.padding, self.strides[1])\n        len_dim3 = conv_utils.conv_output_length(len_dim3, self.pool_size[2], self.padding, self.strides[2])\n        if self.data_format == 'channels_first':\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], len_dim1, len_dim2, len_dim3])\n        else:\n            return tensor_shape.TensorShape([input_shape[0], len_dim1, len_dim2, len_dim3, input_shape[4]])\n\n    def get_config(self):\n        config = {'pool_size': self.pool_size, 'padding': self.padding, 'strides': self.strides, 'data_format': self.data_format}\n        base_config = super(Pooling3D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass PrincipalComponentAnalysis(_BaseModel):\n    \"\"\"\n    Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a new coordinate system such that the first coordinate (or principal component) has the largest possible variance, and each subsequent coordinate is orthogonal to the previous ones and has the next largest variance.\n\n    Parameters\n    ----------\n    n_components : int or None, optional (default=None)\n        Number of components to keep. If None, all components are kept.\n\n    solver : str, optional (default='svd')\n        Solver to use for computing the PCA. Valid options are 'eigen' and 'svd'.\n\n    whitening : bool, optional (default=False)\n        If True, the data is whitened (i.e., the components are uncorrelated and have unit variance).\n\n    Attributes\n    ----------\n    n_components_ : int\n        The estimated number of components.\n\n    explained_variance_ : array-like, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n    explained_variance_ratio_ : array-like, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    components_ : array-like, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of maximum variance in the data.\n\n    mean_ : array-like, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import PCA\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.components_)\n    [[-0.70710678 -0.70710678]\n     [ 0.70710678 -0.70710678]]\n    >>> print(pca.explained_variance_)\n    [2. 0.]\n    >>> print(pca.explained_variance_ratio_)\n    [1. 0.]\n    \"\"\"\n\n    def __init__(self, n_components=None, solver='svd', whitening=False):\n        valid_solver = {'eigen', 'svd'}\n        if solver not in valid_solver:\n            raise AttributeError(f'Must be in {valid_solver}. Found {solver}')\n        self.solver = solver\n        if n_components is not None and n_components < 1:\n            raise AttributeError('n_components must be > 1 or None')\n        self.n_components = n_components\n        self._is_fitted = False\n        self.whitening = whitening\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the PCA model to the data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,), optional (default=None)\n            Target values. Not used in PCA.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._is_fitted = False\n        self._check_arrays(X=X)\n        self._fit(X=X)\n        self._is_fitted = True\n        return self\n\n    def _fit(self, X):\n        n_samples = X.shape[0]\n        n_features = X.shape[1]\n        if self.n_components is None or self.n_components > n_features:\n            n_components = n_features\n        else:\n            n_components = self.n_components\n        if self.solver == 'eigen':\n            cov_mat = self._covariance_matrix(X)\n            self.e_vals_, self.e_vecs_ = self._decomposition(cov_mat, n_samples)\n        elif self.solver == 'svd':\n            self.e_vals_, self.e_vecs_ = self._decomposition(X, n_samples)\n        self.w_ = self._projection_matrix(eig_vals=self.e_vals_, eig_vecs=self.e_vecs_, whitening=self.whitening, n_components=n_components)\n        tot = np.sum(self.e_vals_)\n        self.e_vals_normalized_ = np.array([i / tot for i in sorted(self.e_vals_, reverse=True)])\n        self.loadings_ = self._loadings()\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transform the data using the PCA model.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_transformed : array-like, shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        self._check_arrays(X=X)\n        if not hasattr", "```python\nclass RMSprop(optimizer_v2.OptimizerV2):\n    \"\"\"\n    RMSprop optimizer.\n\n    RMSprop is an optimizer that uses the root mean square (RMS) of recent\n    gradients to scale the learning rate. It is a simple extension of stochastic\n    gradient descent (SGD) that adapts the learning rate for each parameter.\n\n    Args:\n        learning_rate (float, optional): The learning rate. Defaults to 0.001.\n        rho (float, optional): The decay rate for the moving average of squared gradients. Defaults to 0.9.\n        momentum (float, optional): The momentum factor. Defaults to 0.0.\n        epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-07.\n        centered (bool, optional): If True, use the centered version of RMSprop. Defaults to False.\n        name (str, optional): The name of the optimizer. Defaults to 'RMSprop'.\n\n    Attributes:\n        _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports aggregated gradients.\n\n    Methods:\n        __init__(self, learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs):\n            Initializes the RMSprop optimizer.\n\n        _create_slots(self, var_list):\n            Creates slots for the optimizer.\n\n        _prepare_local(self, var_device, var_dtype, apply_state):\n            Prepares local variables for the optimizer.\n\n        _resource_apply_dense(self, grad, var, apply_state=None):\n            Applies the optimizer to a dense variable.\n\n        _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n            Applies the optimizer to a sparse variable.\n\n        set_weights(self, weights):\n            Sets the weights of the optimizer.\n\n        get_config(self):\n            Returns the configuration of the optimizer.\n    \"\"\"\n```", "```python\nclass SelfTrainingClassifier(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"\n    A self-training classifier that iteratively trains on a growing set of labeled data.\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        An estimator object implementing the `fit` method. If `None`, `base_estimator` must be provided.\n    base_estimator : object, default='deprecated'\n        A deprecated estimator object implementing the `fit` method. Use `estimator` instead.\n    threshold : float, default=0.75\n        The threshold for selecting samples based on their predicted probability.\n    criterion : str, default='threshold'\n        The criterion for selecting samples. Options are 'threshold' or 'k_best'.\n    k_best : int, default=10\n        The number of samples to select based on the highest predicted probability when `criterion='k_best'`.\n    max_iter : int or None, default=10\n        The maximum number of iterations to perform. If `None`, there is no limit.\n    verbose : bool, default=False\n        Whether to print verbose output during training.\n\n    Attributes\n    ----------\n    estimator_ : object\n        The fitted estimator.\n    transduction_ : array-like of shape (n_samples,)\n        The transduction of the labels.\n    labeled_iter_ : array-like of shape (n_samples,)\n        The iteration at which each sample was labeled.\n    n_iter_ : int\n        The number of iterations performed.\n    termination_condition_ : str\n        The reason for termination. Options are 'no_change', 'max_iter', and 'all_labeled'.\n    classes_ : array-like of shape (n_classes,)\n        The classes of the target.\n\n    Methods\n    -------\n    fit(X, y, **params)\n        Fit the self-training classifier to the data.\n    predict(X, **params)\n        Predict the labels for the given data.\n    predict_proba(X, **params)\n        Predict the probabilities of the classes for the given data.\n    decision_function(X, **params)\n        Compute the decision function for the given data.\n    predict_log_proba(X, **params)\n        Predict the log-probabilities of the classes for the given data.\n    score(X, y, **params)\n        Return the mean accuracy on the given test data and labels.\n    get_metadata_routing()\n        Get the metadata routing configuration for this estimator.\n    \"\"\"\n```", "```python\nclass SeparableConv(Conv):\n    \"\"\"\n    A separable convolution layer that performs a depthwise spatial convolution followed by a pointwise spatial convolution.\n\n    Args:\n        rank (int): An integer, the rank of the convolution, e.g., 1 for 1D, 2 for 2D, 3 for 3D.\n        filters (int): An integer, the dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (tuple or list): A tuple or list of integers specifying the spatial dimensions of the filters.\n        strides (tuple or list, optional): A tuple or list of integers specifying the strides of the convolution. Defaults to 1.\n        padding (str, optional): A string, one of 'valid' or 'same'. The type of padding algorithm to use. Defaults to 'valid'.\n        data_format (str, optional): A string, one of 'channels_last' or 'channels_first'. The data format of the input and output tensors. Defaults to None.\n        dilation_rate (tuple or list, optional): A tuple or list of integers specifying the dilation rate of the convolution. Defaults to 1.\n        depth_multiplier (int, optional): An integer, the number of depthwise convolution outputs per input filter. Defaults to 1.\n        activation (str or callable, optional): A string or callable, the activation function to use. Defaults to None.\n        use_bias (bool, optional): A boolean, whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str or callable, optional): A string or callable, the initializer for the depthwise kernel. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str or callable, optional): A string or callable, the initializer for the pointwise kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): A string or callable, the initializer for the bias. Defaults to 'zeros'.\n        depthwise_regularizer (str or callable, optional): A string or callable, the regularizer for the depthwise kernel. Defaults to None.\n        pointwise_regularizer (str or callable, optional): A string or callable, the regularizer for the pointwise kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): A string or callable, the regularizer for the bias. Defaults to None.\n        activity_regularizer (str or callable, optional): A string or callable, the regularizer for the output activity. Defaults to None.\n        depthwise_constraint (str or callable, optional): A string or callable, the constraint for the depthwise kernel. Defaults to None.\n        pointwise_constraint (str or callable, optional): A string or callable, the constraint for the pointwise kernel. Defaults to None.\n        bias_constraint (str or callable, optional): A string or callable, the constraint for the bias. Defaults to None.\n        trainable (bool, optional): A boolean, whether the layer's variables are trainable. Defaults to True.\n        name (str, optional): A string, the name of the layer. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Raises:\n        ValueError: If the channel dimension of the inputs is not defined.\n\n    Returns:\n        A tensor representing the output of the separable convolution layer.\n    \"\"\"\n    def __init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, trainable=True, name=None, **kwargs):\n        super(SeparableConv, self).__init__(rank=rank, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, data_format=data_format, dilation_rate=dilation_rate, activation=activations.get(activation), use_bias=use_bias, bias_initializer=initializers.get(bias_initializer), bias_regularizer=regularizers.get(bias_regularizer), activity_regularizer=regularizers.get(activity_regularizer), bias_constraint=bias_constraint, trainable=trainable, name=name, **kwargs)\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_initializer = initializers.get(depthwise_initializer)\n        self.pointwise_initializer = initializers.get(pointwise_initializer)\n        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n        self.depthwise_constraint = constraints.get(depthwise_constraint)\n        self.pointwise_constraint = constraints.get(pointwise_constraint)\n\n    def build(self, input_shape):\n        input_shape = tensor_shape.Tensor", "```python\nclass SeparableConv1D(SeparableConv):\n    \"\"\"\n    A 1D separable convolution layer.\n\n    Args:\n        filters (int): The number of filters to use.\n        kernel_size (int): The size of the convolution kernel.\n        strides (int, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (int, optional): The dilation rate to use. Defaults to 1.\n        depth_multiplier (int, optional): The depth multiplier for the depthwise convolution. Defaults to 1.\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias term. Defaults to True.\n        depthwise_initializer (str, optional): The initializer for the depthwise kernel. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str, optional): The initializer for the pointwise kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias term. Defaults to 'zeros'.\n        depthwise_regularizer (str, optional): The regularizer for the depthwise kernel. Defaults to None.\n        pointwise_regularizer (str, optional): The regularizer for the pointwise kernel. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias term. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the activity. Defaults to None.\n        depthwise_constraint (str, optional): The constraint for the depthwise kernel. Defaults to None.\n        pointwise_constraint (str, optional): The constraint for the pointwise kernel. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias term. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The output tensor after applying the separable convolution.\n    \"\"\"\n    def __init__(self, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs):\n        super().__init__(rank=1, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, data_format=data_format, dilation_rate=dilation_rate, depth_multiplier=depth_multiplier, activation=activations.get(activation), use_bias=use_bias, depthwise_initializer=initializers.get(depthwise_initializer), pointwise_initializer=initializers.get(pointwise_initializer), bias_initializer=initializers.get(bias_initializer), depthwise_regularizer=regularizers.get(depthwise_regularizer), pointwise_regularizer=regularizers.get(pointwise_regularizer), bias_regularizer=regularizers.get(bias_regularizer), activity_regularizer=regularizers.get(activity_regularizer), depthwise_constraint=constraints.get(depthwise_constraint), pointwise_constraint=constraints.get(pointwise_constraint), bias_constraint=constraints.get(bias_constraint), **kwargs)\n\n    def call(self, inputs):\n        if self.padding == 'causal':\n            inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs))\n        if self.data_format == 'channels_last':\n            strides = (1,) + self.strides * 2 + (1,)\n            spatial_start_dim = 1\n        else:\n            strides = (1, 1) + self.strides * 2\n            spatial_start_dim = 2\n        inputs = array_ops.expand_dims(inputs, spatial_start_dim)\n        depthwise_kernel = array_ops.expand_dims(self.depthwise_kernel, 0)\n        pointwise_kernel = array_ops.expand_dims(self.pointwise_kernel, 0)\n        dilation_rate = (1,) + self.dilation_rate\n        if self.padding == 'causal':\n            op_padding = 'valid'\n        else:\n            op_padding = self.padding\n        outputs = nn.separable_conv2d(inputs, depthwise_kernel, pointwise_kernel, strides=strides, padding=op_padding.upper(), rate=dilation_rate, data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n        if self.use_bias:\n            outputs = nn.bias_add(outputs, self.bias, data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n        outputs = array_ops.squeeze(outputs, [spatial_start_dim])\n        if self.activation is not None:\n            return self.activation", "```python\nclass SeparableConv2D(SeparableConv):\n    \"\"\"\n    A 2D separable convolution layer.\n\n    Args:\n        filters (int): The number of output filters in the convolution.\n        kernel_size (tuple): A tuple of integers specifying the height and width of the 2D convolution window.\n        strides (tuple, optional): A tuple of integers specifying the strides of the convolution. Defaults to (1, 1).\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (tuple, optional): A tuple of integers specifying the dilation rate of the convolution. Defaults to (1, 1).\n        depth_multiplier (int, optional): The number of depthwise convolution output channels for each input channel. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Can be a string name or a callable. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str or callable, optional): The initializer for the depthwise convolution kernel. Can be a string name or a callable. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str or callable, optional): The initializer for the pointwise convolution kernel. Can be a string name or a callable. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias vector. Can be a string name or a callable. Defaults to 'zeros'.\n        depthwise_regularizer (str or callable, optional): The regularizer for the depthwise convolution kernel. Can be a string name or a callable. Defaults to None.\n        pointwise_regularizer (str or callable, optional): The regularizer for the pointwise convolution kernel. Can be a string name or a callable. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias vector. Can be a string name or a callable. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the output activity. Can be a string name or a callable. Defaults to None.\n        depthwise_constraint (str or callable, optional): The constraint for the depthwise convolution kernel. Can be a string name or a callable. Defaults to None.\n        pointwise_constraint (str or callable, optional): The constraint for the pointwise convolution kernel. Can be a string name or a callable. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias vector. Can be a string name or a callable. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor after applying the separable convolution.\n    \"\"\"\n    def __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs):\n        super(SeparableConv2D, self).__init__(rank=2, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, data_format=data_format, dilation_rate=dilation_rate, depth_multiplier=depth_multiplier, activation=activations.get(activation), use_bias=use_bias, depthwise_initializer=initializers.get(depthwise_initializer), pointwise_initializer=initializers.get(pointwise_initializer), bias_initializer=initializers.get(bias_initializer), depthwise_regularizer=regularizers.get(depthwise_regularizer), pointwise_regularizer=regularizers.get(pointwise_regularizer), bias_regularizer=regularizers.get(bias_regularizer), activity_regularizer=regularizers.get(activity_regularizer), depthwise_constraint=constraints.get(depthwise_constraint), pointwise_constraint=constraints.get(pointwise_constraint), bias_constraint=constraints.get(bias_constraint), **kwargs)\n\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            strides = (1,) + self.strides + (1,)\n        else:\n            strides = (1, 1) + self.strides\n        outputs = nn.separable_conv2d(inputs, self.depthwise_kernel, self.pointwise_kernel, strides=strides, padding=self.padding.upper(), rate=self.dilation_rate, data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n        if self.use_bias:\n            outputs = nn.bias_add(outputs, self.bias, data_format=conv_utils.convert_data", "```python\nclass SequentialFeatureSelector(_BaseXComposition, MetaEstimatorMixin):\n    \"\"\"\n    Sequentially selects features based on a scoring criterion.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A supervised learning estimator with a `fit` method that provides\n        information about feature importance either through a `coef_` attribute or\n        through a `feature_importances_` attribute.\n\n    k_features : int or tuple, default=1\n        The number of features to select. If an integer, it specifies the exact\n        number of features to select. If a tuple, it specifies a range (min, max)\n        of features to consider. If 'best', it selects the best features based\n        on the scoring criterion. If 'parsimonious', it selects the most\n        parsimonious set of features.\n\n    forward : bool, default=True\n        If True, the selection process starts with an empty set of features and\n        adds features one by one. If False, the selection process starts with all\n        features and removes features one by one.\n\n    floating : bool, default=False\n        If True, the selection process allows for the removal of features that\n        were previously added. If False, the selection process only adds features.\n\n    verbose : int, default=0\n        Controls the verbosity of the output. If 0, no output is generated. If 1,\n        a progress bar is displayed. If greater than 1, detailed output is\n        displayed.\n\n    scoring : str or callable, default=None\n        A string or callable to evaluate the quality of the selected features.\n        If None, the scoring is inferred from the estimator type.\n\n    cv : int, cross-validation generator or an iterable, default=5\n        Determines the cross-validation splitting strategy. If None, no\n        cross-validation is performed.\n\n    n_jobs : int, default=1\n        The number of jobs to run in parallel. If -1, all processors are used.\n\n    pre_dispatch : str or int, default='2*n_jobs'\n        Controls the number of jobs that are dispatched in parallel. If an\n        integer, it specifies the exact number of jobs. If a string, it specifies\n        a fraction of the total number of jobs.\n\n    clone_estimator : bool, default=True\n        If True, the estimator is cloned before fitting. If False, the original\n        estimator is used.\n\n    fixed_features : array-like, default=None\n        An array of indices or names of features that should not be removed\n        during the selection process.\n\n    feature_groups : list of lists, default=None\n        A list of lists where each sublist contains indices or names of features\n        that should be treated as a single group.\n\n    Attributes\n    ----------\n    k_feature_idx_ : array-like\n        Indices of the selected features.\n\n    k_feature_names_ : array-like\n        Names of the selected features.\n\n    k_score_ : float\n        The score of the selected features.\n\n    subsets_ : dict\n        A dictionary containing the subsets of features and their corresponding\n        scores.\n\n    Methods\n    -------\n    fit(X, y, groups=None, **fit_params)\n        Fit the SequentialFeatureSelector to the data.\n\n    transform(X)\n        Transform the data by selecting the features.\n\n    fit_transform(X, y, groups=None, **fit_params)\n        Fit the SequentialFeatureSelector to the data and transform it.\n\n    get_metric_dict(confidence_interval=0.95)\n        Get a dictionary containing the subsets of features and their\n        corresponding scores, including confidence intervals.\n\n    _calc_confidence(ary, confidence=0.95)\n        Calculate the confidence interval for the scores.\n\n    _check_fitted()\n        Check if the SequentialFeatureSelector has been fitted.\n    \"\"\"\n```", "```python\nclass SGD(optimizer_v2.OptimizerV2):\n    \"\"\"\n    Stochastic Gradient Descent (SGD) optimizer.\n\n    Args:\n        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.01.\n        momentum (float, optional): The momentum factor. Defaults to 0.0.\n        nesterov (bool, optional): Whether to use Nesterov momentum. Defaults to False.\n        name (str, optional): The name of the optimizer. Defaults to 'SGD'.\n        **kwargs: Additional keyword arguments.\n\n    Attributes:\n        _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports gradient aggregation.\n\n    Methods:\n        __init__: Initializes the SGD optimizer.\n        _create_slots: Creates slots for momentum if momentum is enabled.\n        _prepare_local: Prepares local variables for the optimizer.\n        _resource_apply_dense: Applies the SGD update to a dense variable.\n        _resource_apply_sparse_duplicate_indices: Applies the SGD update to a sparse variable with duplicate indices.\n        _resource_apply_sparse: Applies the SGD update to a sparse variable.\n        get_config: Returns the configuration of the optimizer.\n    \"\"\"\n```", "```python\nclass SoftmaxRegression(_BaseModel, _IterativeModel, _Classifier, _MultiClass):\n    \"\"\"\n    Implements a Softmax Regression classifier using gradient descent.\n\n    Parameters\n    ----------\n    eta : float, optional (default=0.01)\n        Learning rate (between 0.0 and 1.0).\n    epochs : int, optional (default=50)\n        Number of passes over the training dataset.\n    l2 : float, optional (default=0.0)\n        L2 regularization parameter.\n    minibatches : int, optional (default=1)\n        Number of minibatches for gradient descent.\n    n_classes : int, optional (default=None)\n        Number of classes. If None, it will be inferred from the data.\n    random_seed : int, optional (default=None)\n        Seed for random number generation.\n    print_progress : int, optional (default=0)\n        If greater than 0, prints progress every `print_progress` epochs.\n\n    Attributes\n    ----------\n    w_ : array-like, shape = [n_features, n_classes]\n        Weights after fitting.\n    b_ : array-like, shape = [n_classes,]\n        Bias after fitting.\n    cost_ : list\n        List of costs computed during training.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the model to the training data.\n\n    predict_proba(X)\n        Return probability estimates for the test data.\n\n    predict(X)\n        Predict class labels for the test data.\n\n    _net_input(X)\n        Calculate the net input for the input data.\n\n    _softmax_activation(z)\n        Apply the softmax activation function.\n\n    _cross_entropy(output, y_target)\n        Calculate the cross-entropy loss.\n\n    _cost(cross_entropy)\n        Calculate the total cost including L2 regularization.\n\n    _to_classlabels(z)\n        Convert the output to class labels.\n\n    _forward(X)\n        Perform the forward pass through the network.\n\n    _backward(X, y_true, y_probas)\n        Perform the backward pass to compute gradients.\n\n    _fit(X, y, init_params=True)\n        Fit the model using gradient descent.\n\n    _predict(X)\n        Predict class labels for the test data using the trained model.\n    \"\"\"\n```", "```python\nclass TargetEncoder(OneToOneFeatureMixin, _BaseEncoder):\n    \"\"\"\n    Encodes categorical features based on the target variable using a target encoding approach.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list, default='auto'\n        Specifies how to handle categorical features. 'auto' means the categories are determined automatically.\n    target_type : {'auto', 'continuous', 'binary', 'multiclass'}, default='auto'\n        Specifies the type of target variable. 'auto' means the type is inferred from the data.\n    smooth : {'auto'} or float, default='auto'\n        Smoothing parameter to handle zero counts. 'auto' means the smoothing is determined automatically.\n    cv : int, default=5\n        Number of folds for cross-validation.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting into folds.\n    random_state : int or None, default=None\n        Seed for random number generation.\n\n    Attributes\n    ----------\n    encodings_ : array-like\n        Encodings for each feature.\n    target_mean_ : array-like\n        Mean of the target variable for each class.\n    classes_ : array-like\n        Classes for multiclass target.\n    n_features_in_ : int\n        Number of input features.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the encoder to the data.\n    fit_transform(X, y)\n        Fit the encoder to the data and transform it.\n    transform(X)\n        Transform the data using the fitted encoder.\n    get_feature_names_out(input_features=None)\n        Get the output feature names.\n\n    Notes\n    -----\n    This encoder is particularly useful for handling categorical features in regression and classification tasks.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        'categories': [StrOptions({'auto'}), list],\n        'target_type': [StrOptions({'auto', 'continuous', 'binary', 'multiclass'})],\n        'smooth': [StrOptions({'auto'}), Interval(Real, 0, None, closed='left')],\n        'cv': [Interval(Integral, 2, None, closed='left')],\n        'shuffle': ['boolean'],\n        'random_state': ['random_state']\n    }\n\n    def __init__(self, categories='auto', target_type='auto', smooth='auto', cv=5, shuffle=True, random_state=None):\n        self.categories = categories\n        self.smooth = smooth\n        self.target_type = target_type\n        self.cv = cv\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y):\n        \"\"\"\n        Fit the encoder to the data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n        y : array-like of shape (n_samples,)\n            Target variable.\n\n        Returns\n        -------\n        self : TargetEncoder\n            Fitted encoder.\n        \"\"\"\n        self._fit_encodings_all(X, y)\n        return self\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit the encoder to the data and transform it.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n        y : array-like of shape (n_samples,)\n            Target variable.\n\n        Returns\n        -------\n        X_out : array-like of shape (n_samples, n_features * n_categories)\n            Transformed data.\n        \"\"\"\n        from ..model_selection import KFold, StratifiedKFold\n        X_ordinal, X_known_mask, y_encoded, n_categories = self._fit_encodings_all(X, y)\n        if self.target_type_ == 'continuous':\n            cv = KFold(self.cv, shuffle=self.shuffle, random_state=self.random_state)\n        else:\n            cv = StratifiedKFold(self.cv, shuffle=self.shuffle, random_state=self.random_state)\n        if self.target_type_ == 'multiclass':\n            X_out = np.empty((X_ordinal.shape[0], X_ordinal.shape[1] * len(self.classes_)), dtype=np.float64)\n        else:\n            X_out = np.empty_like(X_ordinal, dtype=np.float64)\n        for train_idx, test_idx in cv.split(X, y):\n            X_train, y_train = (X_ordinal[train_idx, :], y_encoded[train_idx])\n            y_train_mean = np.mean(y_train, axis=0)\n            if self.target_type_ == 'multiclass':\n                encodings = self._fit_encoding_multiclass(X_train, y_train, n_categories, y_train_mean)\n            else:\n                encodings = self._fit_encoding_binary_or_continuous(X_train, y_train, n_categories, y_train_mean)\n            self._transform_X_ordinal", "```python\nclass TransactionEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encodes a dataset of transactions into a binary matrix where each row represents a transaction and each column represents a unique item.\n\n    Attributes:\n        columns_ (list): List of unique items in the dataset.\n        columns_mapping_ (dict): Dictionary mapping each unique item to its corresponding column index.\n\n    Methods:\n        fit(X): Fits the encoder to the dataset, determining the unique items and their column indices.\n        transform(X, sparse=False): Transforms the dataset into a binary matrix. If `sparse=True`, returns a sparse matrix.\n        inverse_transform(array): Transforms the binary matrix back into a list of transactions.\n        fit_transform(X, sparse=False): Fits the encoder to the dataset and then transforms it.\n        get_feature_names_out(): Returns the feature names (unique items) of the encoded dataset.\n    \"\"\"\n```", "```python\nclass UpSampling1D(Layer):\n    \"\"\"\n    Upsamples a 1D input sequence by repeating each element a specified number of times along the time axis.\n\n    Args:\n        size (int, optional): The factor by which to upsample the input sequence. Defaults to 2.\n\n    Returns:\n        Tensor: The upsampled 1D sequence.\n\n    Example:\n        >>> upsample = UpSampling1D(size=3)\n        >>> input_seq = tf.constant([[[1], [2], [3]]])\n        >>> output_seq = upsample(input_seq)\n        >>> print(output_seq)\n        tf.Tensor([[[1], [1], [1], [2], [2], [2], [3], [3], [3]]], shape=(1, 9, 1), dtype=int32)\n    \"\"\"\n    def __init__(self, size=2, **kwargs):\n        super(UpSampling1D, self).__init__(**kwargs)\n        self.size = int(size)\n        self.input_spec = InputSpec(ndim=3)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        size = self.size * input_shape[1] if input_shape[1] is not None else None\n        return tensor_shape.TensorShape([input_shape[0], size, input_shape[2]])\n\n    def call(self, inputs):\n        output = backend.repeat_elements(inputs, self.size, axis=1)\n        return output\n\n    def get_config(self):\n        config = {'size': self.size}\n        base_config = super(UpSampling1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass UpSampling2D(Layer):\n    \"\"\"\n    Upsamples the input by a factor of `size` in both dimensions.\n\n    Args:\n        size (tuple of int): The upsampling factor in the height and width dimensions. Defaults to `(2, 2)`.\n        data_format (str, optional): The data format of the input. Can be either 'channels_first' or 'channels_last'. If not specified, the default data format is used.\n        interpolation (str): The interpolation method to use. Can be either 'nearest' or 'bilinear'. Defaults to 'nearest'.\n\n    Raises:\n        ValueError: If `interpolation` is not one of 'nearest' or 'bilinear'.\n\n    Returns:\n        Tensor: The upsampled tensor.\n    \"\"\"\n    def __init__(self, size=(2, 2), data_format=None, interpolation='nearest', **kwargs):\n        super(UpSampling2D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n        if interpolation not in {'nearest', 'bilinear'}:\n            raise ValueError('`interpolation` argument should be one of `\"nearest\"` or `\"bilinear\"`.')\n        self.interpolation = interpolation\n        self.input_spec = InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], height, width])\n        else:\n            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n            return tensor_shape.TensorShape([input_shape[0], height, width, input_shape[3]])\n\n    def call(self, inputs):\n        return backend.resize_images(inputs, self.size[0], self.size[1], self.data_format, interpolation=self.interpolation)\n\n    def get_config(self):\n        config = {'size': self.size, 'data_format': self.data_format, 'interpolation': self.interpolation}\n        base_config = super(UpSampling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass UpSampling3D(Layer):\n    \"\"\"\n    Upsamples the input volume by a factor of `size` in each dimension.\n\n    Args:\n        size (tuple of int): The upsampling factor for each dimension (depth, height, width). Default is (2, 2, 2).\n        data_format (str, optional): The data format of the input and output tensors. Can be 'channels_first' or 'channels_last'. Default is None, which uses the default data format of the backend.\n\n    Returns:\n        Tensor: The upsampled volume.\n\n    Raises:\n        ValueError: If `size` is not a tuple of three integers.\n        ValueError: If `data_format` is not 'channels_first' or 'channels_last'.\n\n    Example:\n        >>> upsample = UpSampling3D(size=(2, 2, 2), data_format='channels_first')\n        >>> input_tensor = tf.random.normal((1, 3, 16, 16, 16))\n        >>> output_tensor = upsample(input_tensor)\n        >>> output_tensor.shape\n        (1, 3, 32, 32, 32)\n    \"\"\"\n    def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.size = conv_utils.normalize_tuple(size, 3, 'size')\n        self.input_spec = InputSpec(ndim=5)\n        super(UpSampling3D, self).__init__(**kwargs)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            dim1 = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n            dim2 = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n            dim3 = self.size[2] * input_shape[4] if input_shape[4] is not None else None\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], dim1, dim2, dim3])\n        else:\n            dim1 = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n            dim2 = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n            dim3 = self.size[2] * input_shape[3] if input_shape[3] is not None else None\n            return tensor_shape.TensorShape([input_shape[0], dim1, dim2, dim3, input_shape[4]])\n\n    def call(self, inputs):\n        return backend.resize_volumes(inputs, self.size[0], self.size[1], self.size[2], self.data_format)\n\n    def get_config(self):\n        config = {'size': self.size, 'data_format': self.data_format}\n        base_config = super(UpSampling3D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass ZeroPadding1D(Layer):\n    \"\"\"\n    Adds zero padding to the input tensor along the time dimension.\n\n    Args:\n        padding (int or tuple of int, optional): The amount of padding to add on both sides of the time dimension. \n            If a single integer is provided, it is used as padding on both sides. If a tuple is provided, it should \n            contain two integers representing the padding on the left and right sides, respectively. Defaults to 1.\n        **kwargs: Additional keyword arguments passed to the base Layer class.\n\n    Returns:\n        Tensor: The padded tensor with the same number of channels as the input tensor.\n\n    Raises:\n        ValueError: If the input tensor has fewer than 3 dimensions.\n\n    Example:\n        >>> padding_layer = ZeroPadding1D(padding=(2, 3))\n        >>> input_tensor = tf.constant([[[1], [2], [3]]], dtype=tf.float32)\n        >>> output_tensor = padding_layer(input_tensor)\n        >>> print(output_tensor)\n        tf.Tensor([[[0. 0. 0. 1. 2. 3. 0. 0.]]], shape=(1, 5, 1), dtype=float32)\n    \"\"\"\n    def __init__(self, padding=1, **kwargs):\n        super(ZeroPadding1D, self).__init__(**kwargs)\n        self.padding = conv_utils.normalize_tuple(padding, 2, 'padding')\n        self.input_spec = InputSpec(ndim=3)\n\n    def compute_output_shape(self, input_shape):\n        if input_shape[1] is not None:\n            length = input_shape[1] + self.padding[0] + self.padding[1]\n        else:\n            length = None\n        return tensor_shape.TensorShape([input_shape[0], length, input_shape[2]])\n\n    def call(self, inputs):\n        return backend.temporal_padding(inputs, padding=self.padding)\n\n    def get_config(self):\n        config = {'padding': self.padding}\n        base_config = super(ZeroPadding1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass ZeroPadding2D(Layer):\n    \"\"\"\n    Adds zero padding to the input tensor.\n\n    Args:\n        padding (int, tuple of 2 ints, or tuple of 2 tuples of 2 ints): \n            - If an int, the same symmetric padding is applied to all sides.\n            - If a tuple of 2 ints, interpreted as (symmetric_height_pad, symmetric_width_pad).\n            - If a tuple of 2 tuples of 2 ints, interpreted as ((top_pad, bottom_pad), (left_pad, right_pad)).\n        data_format (str, optional): \n            - 'channels_first' or 'channels_last'. Defaults to the default image data format configured in Keras.\n\n    Returns:\n        Tensor: The padded tensor with the same number of dimensions as the input tensor.\n\n    Raises:\n        ValueError: If `padding` is not an int, a tuple of 2 ints, or a tuple of 2 tuples of 2 ints.\n    \"\"\"\n    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n        super(ZeroPadding2D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if isinstance(padding, int):\n            self.padding = ((padding, padding), (padding, padding))\n        elif hasattr(padding, '__len__'):\n            if len(padding) != 2:\n                raise ValueError('`padding` should have two elements. Found: ' + str(padding))\n            height_padding = conv_utils.normalize_tuple(padding[0], 2, '1st entry of padding')\n            width_padding = conv_utils.normalize_tuple(padding[1], 2, '2nd entry of padding')\n            self.padding = (height_padding, width_padding)\n        else:\n            raise ValueError('`padding` should be either an int, a tuple of 2 ints (symmetric_height_pad, symmetric_width_pad), or a tuple of 2 tuples of 2 ints ((top_pad, bottom_pad), (left_pad, right_pad)). Found: ' + str(padding))\n        self.input_spec = InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            if input_shape[2] is not None:\n                rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n            else:\n                rows = None\n            if input_shape[3] is not None:\n                cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n            else:\n                cols = None\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], rows, cols])\n        elif self.data_format == 'channels_last':\n            if input_shape[1] is not None:\n                rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n            else:\n                rows = None\n            if input_shape[2] is not None:\n                cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n            else:\n                cols = None\n            return tensor_shape.TensorShape([input_shape[0], rows, cols, input_shape[3]])\n\n    def call(self, inputs):\n        return backend.spatial_2d_padding(inputs, padding=self.padding, data_format=self.data_format)\n\n    def get_config(self):\n        config = {'padding': self.padding, 'data_format': self.data_format}\n        base_config = super(ZeroPadding2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```", "```python\nclass ZeroPadding3D(Layer):\n    \"\"\"\n    Adds zero padding to the 3D input tensor.\n\n    Args:\n        padding (int, tuple of 3 ints, or tuple of 3 tuples of 2 ints): \n            - If an int, the same symmetric padding is applied to all three dimensions.\n            - If a tuple of 3 ints, it specifies the padding for each dimension as (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad).\n            - If a tuple of 3 tuples of 2 ints, it specifies the padding for each dimension as ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)).\n        data_format (str, optional): \n            - Specifies the data format of the input tensor. Can be either 'channels_first' or 'channels_last'. Defaults to None, which uses the default data format of the backend.\n\n    Returns:\n        Tensor: The padded 3D tensor.\n\n    Raises:\n        ValueError: \n            - If `padding` is not an int, a tuple of 3 ints, or a tuple of 3 tuples of 2 ints.\n            - If `padding` is a tuple of 3 ints and its length is not 3.\n    \"\"\"\n    def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n        super(ZeroPadding3D, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        if isinstance(padding, int):\n            self.padding = ((padding, padding), (padding, padding), (padding, padding))\n        elif hasattr(padding, '__len__'):\n            if len(padding) != 3:\n                raise ValueError('`padding` should have 3 elements. Found: ' + str(padding))\n            dim1_padding = conv_utils.normalize_tuple(padding[0], 2, '1st entry of padding')\n            dim2_padding = conv_utils.normalize_tuple(padding[1], 2, '2nd entry of padding')\n            dim3_padding = conv_utils.normalize_tuple(padding[2], 2, '3rd entry of padding')\n            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n        else:\n            raise ValueError('`padding` should be either an int, a tuple of 3 ints (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), or a tuple of 3 tuples of 2 ints ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim2_pad)). Found: ' + str(padding))\n        self.input_spec = InputSpec(ndim=5)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.data_format == 'channels_first':\n            if input_shape[2] is not None:\n                dim1 = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n            else:\n                dim1 = None\n            if input_shape[3] is not None:\n                dim2 = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n            else:\n                dim2 = None\n            if input_shape[4] is not None:\n                dim3 = input_shape[4] + self.padding[2][0] + self.padding[2][1]\n            else:\n                dim3 = None\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], dim1, dim2, dim3])\n        elif self.data_format == 'channels_last':\n            if input_shape[1] is not None:\n                dim1 = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n            else:\n                dim1 = None\n            if input_shape[2] is not None:\n                dim2 = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n            else:\n                dim2 = None\n            if input_shape[3] is not None:\n                dim3 = input_shape[3] + self.padding[2][0] + self.padding[2][1]\n            else:\n                dim3 = None\n            return tensor_shape.TensorShape([input_shape[0], dim1, dim2, dim3, input_shape[4]])\n\n    def call(self, inputs):\n        return backend.spatial_3d_padding(inputs, padding=self.padding, data_format=self.data_format)\n\n    def get_config(self):\n        config = {'padding': self.padding, 'data_format': self.data_format}\n        base_config", "```python\nclass _BaseEncoder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Base class for encoding categorical features.\n\n    This class provides methods for checking and transforming input data, as well as handling infrequent categories.\n\n    Methods\n    -------\n    _check_X(X, force_all_finite=True)\n        Checks and prepares the input data for encoding.\n\n    _fit(X, handle_unknown='error', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False)\n        Fits the encoder to the input data, determining the categories and their counts.\n\n    _transform(X, handle_unknown='error', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None)\n        Transforms the input data using the fitted categories.\n\n    infrequent_categories_\n        Returns the infrequent categories for each feature.\n\n    _check_infrequent_enabled()\n        Checks if infrequent category handling is enabled based on the parameters.\n\n    _identify_infrequent(category_count, n_samples, col_idx)\n        Identifies infrequent categories based on the specified frequency threshold.\n\n    _fit_infrequent_category_mapping(n_samples, category_counts, missing_indices)\n        Fits the mapping for infrequent categories.\n\n    _map_infrequent_categories(X_int, X_mask, ignore_category_indices)\n        Maps infrequent categories to a default value.\n\n    _more_tags()\n        Returns additional tags for the transformer.\n    \"\"\"\n```"]