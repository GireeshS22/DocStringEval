["  \"\"\"Adamax optimizer.\n\nThis optimizer applies the Adamax algorithm to the variables in order to minimize the loss. Adamax is a variant of the Adam optimizer that uses the infinity norm instead of the Euclidean norm for normalizing the gradient.\n\nAdamax is similar to Adam, but it uses a different decay rate for the moment estimates. Adamax uses a decay rate of 1 - beta^t, whereas Adam uses a decay rate of 1 - beta^t * (1 - beta). This means that Adamax has a smaller effect on the gradients as they are updated, which can help to prevent the gradients from exploding.\n\nAdamax also uses a different update rule for the moment estimates, which is given by:\n\nm_t = beta_1 * m_{t-1} + (1 - beta_1) * g_t\nv_t = max(beta_2 * v_{t-1}, abs(g_t))\n\nwhere m_t is the moment estimate, v_t is the second moment estimate, g_t is the gradient, beta_1 is the decay rate for the moment estimates, and beta_2 is the decay rate for the second moment estimates.\n\nThe Adamax optimizer uses the following hyperparameters:\n\n* learning_rate: The learning rate for the optimizer.\n* beta_1: The decay rate for the moment estimates.\n* beta_2: The decay rate for the second moment estimates.\n* epsilon: A small value used to avoid division by zero.\n\nThe Adamax optimizer also has the following additional hyperparameters:\n\n* decay: The decay rate for the learning rate.\n* name: The name of the optimizer.\n\nThe Adamax optimizer can be used in the following way:\n\n```\noptimizer = Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n```\n\nThis optimizer can be used in the same way as the Adam optimizer. It is recommended to use the Adamax optimizer when the gradients are large and the learning rate is small.\n\nNote: The Adamax optimizer is not recommended for use with the Adam optimizer. The Adam optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the AdamW optimizer. The AdamW optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the Adagrad optimizer. The Adagrad optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the Adadelta optimizer. The Adadelta optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the Adam optimizer. The Adam optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the RMSprop optimizer. The RMSprop optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the SGD optimizer. The SGD optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the Momentum optimizer. The Momentum optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the Nadam optimizer. The Nadam optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the AdagradDA optimizer. The AdagradDA optimizer uses a different update rule for the moment estimates, which can cause the gradients to explode.\n\nNote: The Adamax optimizer is not recommended for use with the AdagradDecay optimizer. The AdagradDecay optimizer uses a different update rule for the moment estimates, which can cause the grad", "  ```\n\nDocstring for AgglomerationTransform class:\n\nAgglomerationTransform is a transformer class that takes in a dataset X and applies a clustering algorithm to group similar data points together. The class uses the `TransformerMixin` class from scikit-learn to provide a consistent API for transforming data.\n\nThe class has two methods: `transform` and `inverse_transform`. The `transform` method takes in a dataset X and applies the clustering algorithm to group similar data points together. The `inverse_transform` method takes in a dataset X and returns the original data points.\n\nThe class has two attributes: `pooling_func` and `labels_`. The `pooling_func` attribute is a function that is used to pool the data points together. The `labels_` attribute is a numpy array that contains the labels for each data point.\n\nThe class also has a `__metadata_request__inverse_transform` attribute that is used to specify the metadata for the `inverse_transform` method.\n\nThe class is designed to be used with the `TransformerMixin` class from scikit-learn, and it provides a consistent API for transforming data. The class is also designed to be used with the `Agglomeration` class, which is a clustering algorithm that is used to group similar data points together.\n\nThe class is well-documented, with clear and concise documentation for each method and attribute. The class is also well-tested, with a test suite that covers a wide range of scenarios.\n\nOverall, the AgglomerationTransform class is a well-designed and well-documented class that provides a consistent API for transforming data using a clustering algorithm.", "  \"\"\"\nAveragePooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)\n\nAverage pooling layer for 1D inputs.\n\n# Arguments\n- pool_size: An integer or tuple/list of a single integer,\n    specifying the size of the pooling window.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n- strides: An integer or tuple/list of a single integer,\n    specifying the strides of the pooling operation.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n- padding: A string, either 'valid' or 'same',\n    specifying the padding algorithm.\n    See `Conv1D` for more information.\n- data_format: A string, one of `channels_last` (default) or `channels_first`.\n    The ordering of the dimensions in the inputs.\n    `channels_last` corresponds to inputs with shape\n    `(batch, steps, features)` while `channels_first`\n    corresponds to inputs with shape `(batch, features, steps)`.\n- **kwargs: Additional keyword arguments passed to the `Pooling1D` constructor.\n\n# Input shape\n- If `data_format='channels_last'`:\n    `batch_shape + (steps, features)`\n- If `data_format='channels_first'`:\n    `batch_shape + (features, steps)`\n\n# Output shape\n- If `data_format='channels_last'`:\n    `batch_shape + (steps, features)`\n- If `data_format='channels_first'`:\n    `batch_shape + (features, steps)`\n\n# References\n- [Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)\n    (CS231n lecture notes)\n- [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n    (ArXiv e-print)\n\"\"\"", "  \"\"\"\nAveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nAverage pooling layer for 2D inputs.\n\nThis layer performs average pooling across the input data. The output is a 2D tensor with the same number of rows and columns as the input.\n\n# Arguments\n\n- **pool_size**: The size of the pooling window.\n- **strides**: The stride of the pooling window.\n- **padding**: The padding to be applied to the input.\n- **data_format**: The data format of the input.\n- **kwargs**: Additional keyword arguments passed to the layer.\n\n# Input shape\n\n4D tensor with shape:\n\n- If `data_format` is `\"channels_last\"`:\n\n(samples, rows, cols, channels)\n\n- If `data_format` is `\"channels_first\"`:\n\n(samples, channels, rows, cols)\n\n# Output shape\n\n4D tensor with shape:\n\n- If `data_format` is `\"channels_last\"`:\n\n(samples, pooled_rows, pooled_cols, channels)\n\n- If `data_format` is `\"channels_first\"`:\n\n(samples, channels, pooled_rows, pooled_cols)\n\n# References\n\n- [A. Krizhevsky et al., ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\"\"\"", "  \"\"\"\nAveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nA 3D average pooling layer.\n\n# Arguments\n\n- **pool_size**: The size of the pooling window.\n- **strides**: The stride of the pooling window.\n- **padding**: The padding algorithm.\n- **data_format**: The data format of the input data.\n- **kwargs**: Additional keyword arguments passed to the parent class, Pooling3D.\n\n# Returns\n\nA 3D average pooling layer.\n\"\"\"", "  \"\"\"\n    Bayesian Gaussian Mixture Model\n\n    This class implements a Bayesian Gaussian Mixture Model. It is a probabilistic model that represents a mixture of Gaussian distributions with unknown parameters. The model is defined by the following generative process:\n\n    - The data is generated by a mixture of K Gaussian distributions, where K is the number of components.\n    - Each Gaussian distribution has a mean vector and a covariance matrix.\n    - The mean vector and covariance matrix are unknown parameters of the model.\n    - The model is defined by a prior distribution over the parameters, which is a Dirichlet process.\n    - The prior distribution is defined by a concentration parameter, which controls the spread of the distribution.\n    - The model is also defined by a likelihood function, which is the probability of observing the data given the model.\n    - The likelihood function is defined by a Dirichlet distribution over the weights of the components, which controls the relative importance of each component.\n    - The model is also defined by a prior distribution over the weights of the components, which controls the relative importance of each component.\n\n    The class provides methods for estimating the parameters of the model, such as the mean vector and covariance matrix, as well as the weights of the components. It also provides methods for computing the likelihood of the data given the model, as well as the posterior distribution over the parameters.\n\n    Parameters\n    ----------\n    n_components : int, default=1\n        The number of components (i.e., the number of Gaussian distributions) in the mixture model.\n    covariance_type : str, default='full'\n        The type of covariance matrix to use. The options are:\n\n        - 'full': Each component has a full covariance matrix.\n        - 'tied': All components share the same covariance matrix.\n        - 'diag': Each component has a diagonal covariance matrix.\n        - 'spherical': Each component has a spherical covariance matrix (equal variance across all features).\n    tol : float, default=0.001\n        The convergence tolerance for the optimization algorithm.\n    reg_covar : float, default=1e-06\n        The regularization parameter for the covariance matrix.\n    max_iter : int, default=100\n        The maximum number of iterations for the optimization algorithm.\n    n_init : int, default=1\n        The number of initializations to perform. The best results are kept.\n    init_params : str, default='kmeans'\n        The method to use for initialization. The options are:\n\n        - 'kmeans': Use k-means clustering to initialize the parameters.\n        - 'random': Use random initialization.\n    weight_concentration_prior_type : str, default='dirichlet_process'\n        The type of prior distribution to use for the weights of the components. The options are:\n\n        - 'dirichlet_process': A Dirichlet process prior.\n        - 'dirichlet_distribution': A Dirichlet distribution prior.\n    weight_concentration_prior : float or None, default=None\n        The concentration parameter of the prior distribution over the weights of the components.\n    mean_precision_prior : float or None, default=None\n        The precision parameter of the prior distribution over the mean vectors of the components.\n    mean_prior : array-like or None, default=None\n        The prior distribution over the mean vectors of the components.\n    degrees_of_freedom_prior : float or None, default=None\n        The prior distribution over the degrees of freedom of the components.\n    covariance_prior : array-like or None, default=None\n        The prior distribution over the covariance matrices of the components.\n    random_state : int or None, default=None\n        The random seed to use for the initialization.\n    warm_start : bool, default=False\n        Whether to use a warm start for the optimization algorithm.\n    verbose : int, default=0\n        The verbosity level.\n    verbose_interval : int, default=10\n        The interval at which to print the progress of the optimization algorithm.\n\n    Attributes\n    ----------\n    weights_ : array-like\n        The weights of the components.\n    means_ : array-like\n        The mean vectors of the components.\n    precisions_ :", "  \"\"\"\nThis is a docstring for the `Conv` class in Keras.\n\nThe `Conv` class is a convolutional layer that applies a convolution operation to the input data. It takes in a set of parameters, such as the number of filters, the kernel size, and the stride, and applies them to the input data to produce an output.\n\nThe `Conv` class inherits from the `Layer` class in Keras, and it has a number of attributes and methods that are used to define its behavior. These include the `rank` attribute, which specifies the number of dimensions in the input data (e.g. 2 for images, 3 for volumes); the `filters` attribute, which specifies the number of filters to use in the convolution operation; the `kernel_size` attribute, which specifies the size of the convolution kernel; the `strides` attribute, which specifies the stride of the convolution operation; the `padding` attribute, which specifies the padding method to use (e.g. 'valid' or 'same'); and the `activation` attribute, which specifies the activation function to use (e.g. 'relu' or 'sigmoid').\n\nThe `Conv` class also has a number of methods that are used to define its behavior, such as the `build` method, which is called during the construction of the layer to define the layer's weights and biases; the `call` method, which is called during the forward pass of the layer to apply the convolution operation to the input data; and the `compute_output_shape` method, which is called during the construction of the layer to compute the output shape of the layer.\n\nOverall, the `Conv` class is a powerful tool for applying convolutional operations to input data in Keras. It is a key component of many deep learning architectures, and it is widely used in computer vision and other applications.", "  \"\"\"Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\nCreates a 1D convolution layer (e.g. spatial convolution over images).\n\nInput shape\n    4D tensor with shape:\n    (batch, steps, channels) if data_format='channels_first'\n    (batch, steps, channels) if data_format='channels_last'\n\nOutput shape\n    4D tensor with shape:\n    (batch, steps, filters) if data_format='channels_first'\n    (batch, steps, filters) if data_format='channels_last'\n\nArguments:\n    filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n    padding: Currently, only 'valid' padding is supported.\n    data_format: A string, one of 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs.\n        'channels_last' corresponds to inputs with shape (batch, steps, channels)\n        while 'channels_first' corresponds to inputs with shape (batch, channels, steps).\n    dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution.\n        Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1.\n    groups: Integer, the number of groups to split the input into (contextual convolutions).\n    activation: Activation function to use. If you don't specify anything, no activation is applied (ie. 'linear' activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the kernel weights matrix (see `keras.initializers`).\n    bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n    kernel_regularizer: Regularizer function applied to the kernel weights matrix (see `keras.regularizers`).\n    bias_regularizer: Regularizer function applied to the bias vector (see `keras.regularizers`).\n    activity_regularizer: Regularizer function applied to the output of the layer (its 'activation') (see `keras.regularizers`).\n    kernel_constraint: Constraint function applied to the kernel weights matrix (see `keras.constraints`).\n    bias_constraint: Constraint function applied to the bias vector (see `keras.constraints`).\n    **kwargs: Additional keyword arguments passed to the `Layer` superclass.\n\nReferences:\n    - [Convolutional Neural Networks](https://www.cs.toronto.edu/~hinton/absps/cvpr1993.pdf)\n    - [Convolutional Sequences: Filters, Features, and Frames](https://arxiv.org/abs/1408.5882)\n    - [Deep Learning with Caffe](http://caffe.berkeleyvision.org/)\n    - [Convolutional Neural Networks for Visual Recognition](https://arxiv.org/abs/1409.1556)\n    - [Convolutional Neural Networks for Visual Recognition](https://arxiv.org/abs/1409.1556)\n    - [Convolutional Neural Networks for Visual Recognition](https://arxiv.org/abs/1409.1556)\n    - [Convolutional Neural Networks for Visual Recognition](https://arxiv.org/abs/140", "  This is a Python class for a Conv1DTranspose layer. The class inherits from the Conv1D class and overrides some of its methods to implement the transposed convolution operation.\n\nThe class has the following attributes:\n\n* filters: The number of filters to use in the convolution.\n* kernel_size: The size of the convolution kernel.\n* strides: The stride of the convolution.\n* padding: The padding method to use (either 'valid' or 'same').\n* output_padding: The output padding to use.\n* data_format: The data format to use (either 'channels_first' or 'channels_last').\n* dilation_rate: The dilation rate to use.\n* activation: The activation function to use.\n* use_bias: Whether to use a bias term.\n* kernel_initializer: The initializer to use for the kernel.\n* bias_initializer: The initializer to use for the bias.\n* kernel_regularizer: The regularizer to use for the kernel.\n* bias_regularizer: The regularizer to use for the bias.\n* activity_regularizer: The regularizer to use for the output.\n* kernel_constraint: The constraint to use for the kernel.\n* bias_constraint: The constraint to use for the bias.\n\nThe class also has the following methods:\n\n* build: Builds the layer.\n* call: Invokes the layer on inputs.\n* compute_output_shape: Computes the output shape of the layer.\n* get_config: Gets the configuration of the layer.\n\nThe docstring for the class is:\n\nConv1DTranspose(filters, kernel_size, strides=1, padding='valid', output_padding=None, data_format=None, dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\nThis class is a Conv1D layer with the transposed convolution operation. It takes the input tensor and applies the transposed convolution operation to it, using the specified parameters. The output of the layer is the result of the transposed convolution operation.", "  \"\"\"\n    A 2D convolution layer.\n\n    This layer creates a convolution kernel that is convolved with the input to produce an output.\n    If `use_bias` is True, a bias vector is created and added to the output.\n    If `activation` is not `None`, it is applied to the output.\n    If `return_sequences` is True, the output will contain the full sequence of the input,\n    otherwise it will contain only the last timestep.\n\n    Arguments:\n        filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n        kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution.\n            Specifying any stride value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n        padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs.\n            `channels_last` corresponds to inputs with shape `(batch, height, width, channels)`\n            while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be \"channels_last\".\n        dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate\n            to use for dilated convolution.\n            Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying\n            any `strides` value != 1.\n        groups: A positive integer specifying the number of groups in which the input is split\n            along the channel axis, in conjunction with a depthwise convolution.\n            In the context of this layer, the groups must be a divisor of the input's number of filters.\n            This argument is only supported for `tf.keras` backend.\n        activation: Activation function to use. If you don't specify anything, no activation is applied.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the kernel weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n        kernel_constraint: Constraint function applied to the kernel weights matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n        return_sequences: Boolean, whether to return the last output in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False). If True, process the input sequence backwards and return the reversed sequence.\n        **kwargs: Additional keyword arguments passed to the `Layer` superclass.\n    \"\"\"", "  \"\"\"\nConv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\nConv2DTranspose layer for 2D convolution with transposed filters.\n\n__init__\n\n    Initializes the layer with the given shape and parameters.\n\n    Arguments:\n        filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n        padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` means padding with zeroes such that the output has the same height and width as the input. Note that `\"same\"` is slightly different from the same padding mode in TensorFlow, which is called `\"SAME\"`.\n        output_padding: An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output shape. Can be a single integer to specify the same value for all spatial dimensions.\n        data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be `'channels_last'`.\n        dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any stride value != 1.\n        activation: Activation function to use. If you don't specify anything, no activation is applied (see `keras.activations`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix (see `keras.initializers`).\n        bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix (see `keras.regularizers`).\n        bias_regularizer: Regularizer function applied to the bias vector (see `keras.regularizers`).\n        activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\") (see `keras.regularizers`).\n        kernel_constraint: Constraint function applied to the `kernel` weights matrix (see `keras.constraints`).\n        bias_constraint: Constraint function applied to the bias vector (see `keras.constraints`).\n        **kwargs: Additional keyword arguments to pass to the `Layer` superclass.\n\ncall\n\n    Invokes the layer on inputs.\n\n    Arguments:\n        inputs: Input tensor (of shape `(batch_size, channels, height, width)` if `data_format='channels_last'` or `(batch_size, height, width, channels)` if `data_format='channels_first'`).\n\n    Returns:\n        Output tensor of shape `(batch_size, filters, new_height, new_width)` if `data_format='channels_last'` or `(batch_size, new_height, new_width, filters)` if `data_", "  \"\"\"\nConv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\nA 3D convolution layer.\n\n# Arguments\n\n- filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n- kernel_size: An integer or tuple/list of 3 integers, specifying the size of the 3D convolution window.\n- strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along the 3 axes.\n- padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n- data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, length, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, length, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be `'channels_last'`.\n- dilation_rate: An integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any `dilation_rate` value != 1 is equivalent to specifying `dilation_rate` as (1, 1, 1).\n- groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with a weight matrix, and the outputs are concatenated to form the final output.\n- activation: Activation function to use. If you don't specify anything, no activation is applied (see `keras.activations`).\n- use_bias: Boolean, whether the layer uses a bias vector.\n- kernel_initializer: Initializer for the kernel weights matrix (see `keras.initializers`).\n- bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n- kernel_regularizer: Regularizer function applied to the kernel weights matrix (see `keras.regularizers`).\n- bias_regularizer: Regularizer function applied to the bias vector (see `keras.regularizers`).\n- activity_regularizer: Regularizer function applied to the output of the layer (its 'activation') (see `keras.regularizers`).\n- kernel_constraint: Constraint function applied to the kernel weights matrix (see `keras.constraints`).\n- bias_constraint: Constraint function applied to the bias vector (see `keras.constraints`).\n- **kwargs: Additional keyword arguments passed to the `Layer` superclass.\n\n# Input shape\n\n- If `data_format='channels_last'`:\n    `B` (batch size), `L` (length), `W` (width), `C` (number of channels)\n- If `data_format='channels_first'`:\n    `B` (batch size), `C` (number of channels), `L` (length), `W` (width)\n\n# Output shape\n\n- If `data_format='channels_last'`:\n    `B` (batch size), `L` (length), `W` (width), `C` (number of filters)\n- If `data_format='channels_first'`:\n    `B` (batch size), `C` (number of filters), `L` (length), `W` (width)\n\n# References\n\n- [Convolutional Neural Networks](https://www.cs.toronto.edu/~hinton/absps/cvpr1993.pdf)\n    (Hinton et al., 1993)\n- [Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1409.1556)\n    (Krizhevsky et al.,", "  This is a docstring for the `Conv3DTranspose` class in Keras. It provides a brief description of the class and its purpose, as well as a list of its attributes and methods.\n\nHere is the docstring for the `Conv3DTranspose` class:\n```\nConv3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n```\nThis docstring describes the constructor for the `Conv3DTranspose` class, which takes the following arguments:\n\n* `filters`: The number of filters to use in the convolution.\n* `kernel_size`: The size of the convolution kernel.\n* `strides`: The stride of the convolution.\n* `padding`: The padding method to use for the convolution.\n* `output_padding`: The output padding method to use for the convolution.\n* `data_format`: The data format to use for the convolution.\n* `activation`: The activation function to use for the convolution.\n* `use_bias`: Whether to use a bias term in the convolution.\n* `kernel_initializer`: The initializer to use for the convolution kernel.\n* `bias_initializer`: The initializer to use for the bias term.\n* `kernel_regularizer`: The regularizer to use for the convolution kernel.\n* `bias_regularizer`: The regularizer to use for the bias term.\n* `activity_regularizer`: The regularizer to use for the output of the convolution.\n* `kernel_constraint`: The constraint to use for the convolution kernel.\n* `bias_constraint`: The constraint to use for the bias term.\n* `**kwargs`: Additional keyword arguments to pass to the constructor.\n\nThe docstring also includes a list of the class's attributes and methods, including:\n\n* `output_padding`: The output padding method to use for the convolution.\n* `build`: The method to use to build the convolution.\n* `call`: The method to use to call the convolution.\n* `compute_output_shape`: The method to use to compute the output shape of the convolution.\n* `get_config`: The method to use to get the configuration of the convolution.\n\nOverall, this docstring provides a brief description of the `Conv3DTranspose` class and its purpose, as well as a list of its attributes and methods. It also includes information about the constructor and its arguments, as well as the methods that can be used to build and call the convolution.", "  ```\nCropping1D(\n    cropping=(1, 1),\n    **kwargs\n)\n```\n\nThis class implements a 1D cropping layer. It takes a tuple of two integers as input, representing the number of rows to crop from the beginning and end of the input tensor, respectively. The output tensor will have the same shape as the input tensor, but with the specified number of rows cropped from the beginning and end.\n\nThe `compute_output_shape` method computes the output shape of the layer, given the input shape. It returns a tensor shape object, representing the shape of the output tensor.\n\nThe `call` method applies the cropping operation to the input tensor, and returns the cropped tensor.\n\nThe `get_config` method returns the configuration of the layer, which can be used to recreate the layer from the configuration.", "  ```\nCropping2D(\n    cropping=((0, 0), (0, 0)),\n    data_format=None,\n    **kwargs\n)\n```\n\nThis docstring describes the `Cropping2D` class, which is a layer in the Keras library for deep learning. It takes in a tuple of two integers or two tuples of two integers as its `cropping` argument, which specifies the amount of cropping to be applied to the input data in the height and width dimensions. The `data_format` argument specifies the format of the input data, and can be either `'channels_first'` or `'channels_last'`.\n\nThe `compute_output_shape` method computes the output shape of the layer, given the input shape. It takes in the input shape as a tensor shape, and returns the output shape as a tensor shape.\n\nThe `call` method applies the cropping operation to the input data, and returns the cropped data. It takes in the input data as a tensor, and returns the cropped data as a tensor.\n\nThe `get_config` method returns the configuration of the layer, which can be used to recreate the layer from the configuration.", "  ```\nCropping3D(\n    cropping=((1, 1), (1, 1), (1, 1)),\n    data_format='channels_first',\n    **kwargs\n)\n```\n\nThis docstring describes the `Cropping3D` class, which is a layer in the Keras library for deep learning. It takes a 5D input tensor and applies cropping to the 3D spatial dimensions. The `cropping` argument specifies the amount of cropping to be applied to each dimension, and the `data_format` argument specifies the format of the input data. The `**kwargs` argument is used to pass any additional arguments to the parent class.", "  \"\"\"\nDBSCAN(ClusterMixin, BaseEstimator)\n\nThis class implements the DBSCAN clustering algorithm.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for them to be considered as in the same neighborhood.\nmin_samples : int, default=5\n    The minimum number of samples required in a neighborhood for a sample to be considered as a core sample.\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating the distance between samples. If 'precomputed' is specified, the distance is assumed to be already computed and passed as input.\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\nalgorithm : str, default='auto'\n    The algorithm to use for the neighborhood search.\n    Valid options are 'auto', 'ball_tree', 'kd_tree', and 'brute'.\n    'auto' will attempt to choose the algorithm automatically.\n    'ball_tree' will use a ball tree algorithm.\n    'kd_tree' will use a k-d tree algorithm.\n    'brute' will use a brute-force algorithm.\nleaf_size : int, default=30\n    The size of the leaf nodes used in the tree.\n    Only used if algorithm is 'ball_tree' or 'kd_tree'.\np : float, default=None\n    The power of the Minkowski metric to be used.\n    If None, it uses the standard Euclidean metric.\n    Only used if metric is 'minkowski'.\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    If None, it will be set to the number of CPU cores.\n\nAttributes\n----------\ncore_sample_indices_ : ndarray of shape (n_core_samples,)\n    The indices of the core samples.\nlabels_ : ndarray of shape (n_samples,)\n    The labels of each sample.\ncomponents_ : ndarray of shape (n_core_samples, n_features)\n    The coordinates of the core samples.\n\nMethods\n-------\nfit(X, y=None, sample_weight=None)\n    Fit the model using X as training data.\nfit_predict(X, y=None, sample_weight=None)\n    Fit the model and predict the labels for X.\n\"\"\"", "  \"\"\"\nDepthwiseConv2D(Conv2D)\n\nThis class extends the Conv2D class and adds a depthwise convolution operation to it.\n\nParameters:\n\n* kernel_size: The size of the convolution kernel.\n* strides: The strides of the convolution kernel.\n* padding: The padding method to use.\n* depth_multiplier: The number of depthwise convolution output channels for each input channel.\n* data_format: The data format of the input tensor.\n* dilation_rate: The dilation rate of the convolution kernel.\n* activation: The activation function to use.\n* use_bias: Whether to add a bias term to the output.\n* depthwise_initializer: The initializer to use for the depthwise kernel.\n* bias_initializer: The initializer to use for the bias.\n* depthwise_regularizer: The regularizer to use for the depthwise kernel.\n* bias_regularizer: The regularizer to use for the bias.\n* activity_regularizer: The regularizer to use for the output.\n* depthwise_constraint: The constraint to use for the depthwise kernel.\n* bias_constraint: The constraint to use for the bias.\n\nMethods:\n\n* __init__: Initializes the DepthwiseConv2D layer.\n* build: Builds the layer.\n* call: Invokes the layer on inputs.\n* compute_output_shape: Computes the output shape of the layer.\n* get_config: Gets the configuration of the layer.\n\"\"\"", "  \"\"\"\n\nEmbedding(Layer)\n----------------\n\n### Class Embedding\n\nThe Embedding layer is a type of neural network layer that is used to transform input data with a fixed dictionary of vectors. It is a combination of a Dense layer and an Embedding layer.\n\n### Arguments\n\n- **input_dim**: The dimensionality of the input data (integer). This argument is required.\n- **output_dim**: The dimensionality of the output data (integer). This argument is required.\n- **embeddings_initializer**: Initializer for the `embeddings` matrix (see `keras.initializers`). Defaults to 'uniform'.\n- **embeddings_regularizer**: Regularizer function applied to the `embeddings` matrix (see `keras.regularizers`). Defaults to None.\n- **activity_regularizer**: Regularizer function applied to the output of the layer (see `keras.regularizers`). Defaults to None.\n- **embeddings_constraint**: Constraint function applied to the `embeddings` matrix (see `keras.constraints`). Defaults to None.\n- **mask_zero**: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using `Embedding` layers as the first layer in a model. Defaults to False.\n- **input_length**: Length of input sequences, when it is constant. This argument is required if you are using `Embedding` layer as the first layer in a model. Defaults to None.\n\n### Input shape\n\n- nD tensor with shape: `(batch_size, ..., input_dim)`. The most common situation would be a 2D input with shape `(batch_size, input_dim)`.\n\n### Output shape\n\n- nD tensor with shape: `(batch_size, ..., output_dim)`. For instance, for a 2D input with shape `(batch_size, input_dim)`, the output would have shape `(batch_size, output_dim)`.\n\n### Masking\n\nIf `mask_zero` is set to `True`, the input value 0 is treated as a special \"padding\" value that is not embedded.\n\n### Examples\n\n```python\n# Embedding layer with a fixed dictionary\nembedding_layer = Embedding(input_dim=10, output_dim=64, input_length=10)\n\n# Embedding layer with a learned dictionary\nembedding_layer = Embedding(input_dim=10, output_dim=64, embeddings_initializer='uniform')\n```\n\n\"\"\"", "  \"\"\"\nThis is a docstring for a Python class. It provides a brief description of the class and its purpose.\n\nThe class is a Flask application, which is a web application framework for Python. It provides a way to build web applications using Python.\n\nThe class has several attributes and methods that are used to configure and run the application. These include:\n\n* `default_config`: a dictionary of default configuration options for the application.\n* `request_class`: the class to use for creating request objects.\n* `response_class`: the class to use for creating response objects.\n* `session_interface`: the interface to use for storing and retrieving session data.\n* `__init__`: the constructor for the class, which takes a number of arguments for configuring the application.\n* `get_send_file_max_age`: a method for determining the maximum age of a file to be sent.\n* `send_static_file`: a method for sending a static file.\n* `open_resource`: a method for opening a resource file.\n* `open_instance_resource`: a method for opening an instance resource file.\n* `create_jinja_environment`: a method for creating a Jinja environment.\n* `create_url_adapter`: a method for creating a URL adapter.\n* `raise_routing_exception`: a method for raising a routing exception.\n* `update_template_context`: a method for updating the template context.\n* `make_shell_context`: a method for creating a shell context.\n* `run`: a method for running the application.\n* `test_client`: a method for creating a test client.\n* `test_cli_runner`: a method for creating a test CLI runner.\n* `handle_http_exception`: a method for handling an HTTP exception.\n* `handle_user_exception`: a method for handling a user exception.\n* `handle_exception`: a method for handling an exception.\n* `dispatch_request`: a method for dispatching a request.\n* `full_dispatch_request`: a method for full dispatching a request.\n* `finalize_request`: a method for finalizing a request.\n* `make_default_options_response`: a method for making a default options response.\n* `ensure_sync`: a method for ensuring that a function is synchronous.\n* `async_to_sync`: a method for converting an asynchronous function to a synchronous function.\n* `url_for`: a method for generating a URL for a given endpoint.\n* `make_response`: a method for creating a response.\n* `preprocess_request`: a method for preprocessing a request.\n* `process_response`: a method for processing a response.\n* `do_teardown_request`: a method for doing teardown for a request.\n* `do_teardown_appcontext`: a method for doing teardown for an application context.\n* `app_context`: a method for creating an application context.\n* `request_context`: a method for creating a request context.\n* `test_request_context`: a method for creating a test request context.\n* `wsgi_app`: a method for creating a WSGI application.\n* `__call__`: a method for calling the application.\n\nThis docstring provides a brief overview of the class and its purpose, as well as a list of its attributes and methods. It does not provide any detailed information about how to use the class or its methods.", "  Docstring for the FunctionTransformer class:\n\n---\n\nFunctionTransformer(TransformerMixin, BaseEstimator)\n\n---\n\nThis class provides a way to apply a function to the data before fitting a machine learning model. The function can be any callable that takes a 1D array-like object and returns a 1D array-like object. The function is applied element-wise to each column of the data.\n\nThe class has the following parameters:\n\n* func: The function to apply to the data. This can be any callable that takes a 1D array-like object and returns a 1D array-like object. If this parameter is not specified, the identity function is used.\n* inverse_func: The inverse function to apply to the data. This can be any callable that takes a 1D array-like object and returns a 1D array-like object. If this parameter is not specified, the identity function is used.\n* validate: Whether to validate the data before fitting the model. If this parameter is set to True, the data is validated using the `check_array` function from scikit-learn.\n* accept_sparse: Whether to accept sparse matrices as input. If this parameter is set to True, the data is converted to a dense matrix before applying the function.\n* check_inverse: Whether to check that the inverse function is the inverse of the function. If this parameter is set to True, the `check_inverse_transform` method is called on the data before fitting the model.\n* feature_names_out: The names of the output features. This can be any callable that takes a 1D array-like object and returns a 1D array-like object. If this parameter is not specified, the names of the input features are used.\n* kw_args: The keyword arguments to pass to the function. If this parameter is not specified, no keyword arguments are passed.\n* inv_kw_args: The keyword arguments to pass to the inverse function. If this parameter is not specified, no keyword arguments are passed.\n\nThe class has the following methods:\n\n* fit: Fits the model to the data.\n* transform: Applies the function to the data.\n* inverse_transform: Applies the inverse function to the data.\n* get_feature_names_out: Returns the names of the output features.\n* set_output: Sets the output configuration for the model.\n\nThe class has the following properties:\n\n* _parameter_constraints: The constraints for the parameters of the class.\n* _sklearn_output_config: The output configuration for the model.\n\nThe class has the following methods:\n\n* _check_input: Checks the input data.\n* _check_inverse_transform: Checks that the inverse function is the inverse of the function.\n* _transform: Applies the function to the data.\n* _more_tags: Returns the tags for the class.\n\nThe class has the following properties:\n\n* __sklearn_is_fitted__: Whether the class is fitted.\n\nThe class has the following methods:\n\n* __init__: Initializes the class.\n* __sklearn_is_fitted__: Returns whether the class is fitted.\n* __sklearn_is_fitted__: Sets whether the class is fitted.\n* __sklearn_is_fitted__: Returns the tags for the class.\n\nThe class has the following properties:\n\n* __sklearn_is_fitted__: Whether the class is fitted.\n\nThe class has the following methods:\n\n* __init__: Initializes the class.\n* __sklearn_is_fitted__: Returns whether the class is fitted.\n* __sklearn_is_fitted__: Sets whether the class is fitted.\n* __sklearn_is_fitted__: Returns the tags for the class.\n\nThe class has the following properties:\n\n* __sklearn_is_fitted__: Whether the class is fitted.\n\nThe class has the following methods:\n\n* __init__: Initializes the class.\n* __sklearn_is_fitted__: Returns whether the class is fitted.\n* __sklearn_is_fitted__: Sets whether the class is fitted.\n* __sklearn_is_fitted__: Returns the tags for the class.\n\nThe class has the following properties:\n\n* __sklearn_is_fitted__: Whether the", "  \"\"\"\nGaussianMixture(n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)\n\nThis class implements a Gaussian mixture model for clustering data. It is a subclass of the BaseMixture class and inherits its methods.\n\nParameters:\n\n* n_components: The number of components (clusters) to form.\n* covariance_type: The type of covariance to use. Can be 'full', 'tied', 'diag', or 'spherical'.\n* tol: The convergence tolerance.\n* reg_covar: The regularization parameter for covariance.\n* max_iter: The maximum number of iterations.\n* n_init: The number of initializations.\n* init_params: The initialization parameters.\n* weights_init: The initial weights.\n* means_init: The initial means.\n* precisions_init: The initial precisions.\n* random_state: The random state.\n* warm_start: Whether to warm start the optimization.\n* verbose: The verbosity level.\n* verbose_interval: The verbosity interval.\n\nMethods:\n\n* _check_parameters: Checks the parameters.\n* _initialize_parameters: Initializes the parameters.\n* _initialize: Initializes the parameters.\n* _m_step: Performs the M-step.\n* _estimate_log_prob: Estimates the log probability.\n* _estimate_log_weights: Estimates the log weights.\n* _compute_lower_bound: Computes the lower bound.\n* _get_parameters: Gets the parameters.\n* _set_parameters: Sets the parameters.\n* _n_parameters: Gets the number of parameters.\n* bic: Gets the BIC.\n* aic: Gets the AIC.\n\"\"\"", "  \"\"\"\n    Global Average Pooling 1D\n    =======================\n\n    Global Average Pooling 1D is a layer that computes the average of the input data across the 1D spatial dimensions.\n\n    The output of this layer has the same number of dimensions as the input, but with the specified number of spatial\n    dimensions of size 1.\n\n    For example, if the input has shape `(batch_size, timesteps, channels)`, then the output would have shape\n    `(batch_size, 1, channels)`.\n\n    This layer uses the `call` method of its parent class, `GlobalPooling1D`, as its implementation.\n\n    Parameters\n    ----------\n    data_format : str, optional\n        The ordering of the dimensions in the input.\n        Can be either `'channels_last'` (default) or `'channels_first'`.\n    **kwargs\n        Additional keyword arguments passed to the `GlobalPooling1D` constructor.\n\n    Attributes\n    ----------\n    supports_masking : bool\n        Whether this layer supports masking for variable-length input.\n\n    Methods\n    -------\n    call(inputs, mask=None)\n        Computes the average of the input data across the 1D spatial dimensions.\n    compute_mask(inputs, mask=None)\n        Computes the output mask.\n    \"\"\"", "  \"\"\"Global Average Pooling 2D layer.\n\nThis layer computes the global average of the input, along the spatial dimensions.\n\nInput shape:\n\n* Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n\nOutput shape:\n\n* A tensor of shape `(batch_size, channels)`\n\nArguments:\n\n- **data_format**: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch_size, ..., channels)` while `channels_first` corresponds to inputs with shape `(batch_size, channels, ...)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be `channels_last`.\n\n- **keepdims**: A boolean, whether to keep the dimensions or not. If `keepdims` is `False`, the rank of the tensor is reduced by 1.\n\n- **name**: A string, the name of the layer.\n\n- **dtype**: A string, the dtype of the layer's weights.\n\n- **input_shape**: A tuple of integers, the shape of the input.\n\n- **input_dtype**: A string, the dtype of the input.\n\n- **input_spec**: A `InputSpec` object, the spec of the input.\n\n- **trainable**: A boolean, whether the layer's weights will be updated during training or not.\n\n- **weights**: A list of tensors, the weights of the layer.\n\n- **losses**: A list of tensors, the losses added by the layer.\n\n- **updates**: A list of tensors, the updates of the layer.\n\n- **trainable_weights**: A list of tensors, the trainable weights of the layer.\n\n- **non_trainable_weights**: A list of tensors, the non-trainable weights of the layer.\n\n- **built**: A boolean, whether the layer has been built or not.\n\n- **input**: A tensor, the input of the layer.\n\n- **output**: A tensor, the output of the layer.\n\n- **loss**: A tensor, the loss of the layer.\n\n- **eager_output**: A tensor, the output of the layer in eager execution.\n\n- **inbound_nodes**: A list of lists, the inbound nodes of the layer.\n\n- **outbound_nodes**: A list of lists, the outbound nodes of the layer.\n\n- **state_updates**: A list of tensors, the state updates of the layer.\n\n- **non_trainable_state_updates**: A list of tensors, the non-trainable state updates of the layer.\n\n- **input_mask**: A tensor, the input mask of the layer.\n\n- **output_mask**: A tensor, the output mask of the layer.\n\n- **input_shape**: A tuple of integers, the shape of the input.\n\n- **input_dtype**: A string, the dtype of the input.\n\n- **input_spec**: A `InputSpec` object, the spec of the input.\n\n- **trainable**: A boolean, whether the layer's weights will be updated during training or not.\n\n- **weights**: A list of tensors, the weights of the layer.\n\n- **losses**: A list of tensors, the losses added by the layer.\n\n- **updates**: A list of tensors, the updates of the layer.\n\n- **trainable_weights**: A list of tensors, the trainable weights of the layer.\n\n- **non_trainable_weights**: A list of tensors, the non-trainable weights of the layer.\n\n- **built**: A boolean, whether the layer has been built or not.\n\n- **input**: A tensor, the input of the layer.\n\n- **output**: A tensor, the output of the layer.\n\n- **loss**: A tensor, the loss of the layer.\n", "  \"\"\"\nGlobalAveragePooling3D(GlobalPooling3D)\n\nPerforms a global average pooling operation on a 3D input tensor.\n\nInput shape\n-----------\n\n- If `data_format='channels_last'`:\n    - 5D tensor with shape: `(batch_size, axis1, axis2, axis3, channels)`\n- If `data_format='channels_first'`:\n    - 5D tensor with shape: `(batch_size, channels, axis1, axis2, axis3)`\n\nOutput shape\n------------\n\n- If `data_format='channels_last'`:\n    - 5D tensor with shape: `(batch_size, 1, 1, 1, channels)`\n- If `data_format='channels_first'`:\n    - 5D tensor with shape: `(batch_size, channels, 1, 1, 1)`\n\nReturns\n-------\n\n- A tensor with the global average pooling operation applied to the input tensor.\n\nArguments\n----------\n\n- **data_format**: A string, one of `channels_last` or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch_size, axis1, axis2, axis3, channels)` while `channels_first` corresponds to inputs with shape `(batch_size, channels, axis1, axis2, axis3)`.\n\n- **keepdims**: A boolean, whether to keep the dimensions or not. If `keepdims` is `False`, the rank of the tensor is reduced by 1 for each axis where pooling is performed.\n\n- **name**: A string, the name of the layer.\n\n- ****kwargs**: Additional keyword arguments to pass to the `GlobalPooling3D` layer.\n\nReferences\n----------\n\n- [Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/)\n\"\"\"", "  \"\"\"\n    GlobalMaxPooling1D(GlobalPooling1D)\n\n    Performs global max pooling on an input tensor.\n\n    Input shape:\n        3D tensor with shape:\n        `(batch_size, steps, features)`\n\n    Output shape:\n        2D tensor with shape:\n        `(batch_size, features)`\n\n    Arguments:\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs.\n            `channels_last` corresponds to inputs with shape\n            `(batch_size, steps, features)` while `channels_first` corresponds to\n            inputs with shape `(batch_size, features, steps)`.\n        keepdims: A boolean, whether to keep the temporal dimension or not.\n            If `keepdims` is `False`, the temporal dimension will be removed from\n            the output shape.\n    \"\"\"\n    def call(self, inputs):\n        steps_axis = 1 if self.data_format == 'channels_last' else 2\n        return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)", "  \"\"\"\nGlobalMaxPooling2D(GlobalPooling2D)\n\nThis class implements the global max pooling operation for 2D inputs.\n\nIt extends the `GlobalPooling2D` class and overrides the `call` method to perform the global max pooling operation.\n\nThe `call` method takes an input tensor and applies the global max pooling operation along the spatial dimensions (i.e., the first two dimensions).\n\nThe `data_format` attribute determines the order of the dimensions in the input tensor. If `data_format='channels_last'`, the spatial dimensions are the last two dimensions, and the channel dimension is the first dimension. If `data_format='channels_first'`, the spatial dimensions are the first two dimensions, and the channel dimension is the third dimension.\n\nThe `keepdims` attribute determines whether to keep the spatial dimensions or not. If `keepdims=True`, the spatial dimensions are kept, and the output tensor has the same rank as the input tensor. If `keepdims=False`, the spatial dimensions are removed, and the output tensor has rank 1.\n\nThe `GlobalMaxPooling2D` class can be used as a custom layer in a Keras model. It can be applied to an input tensor with any number of dimensions, but the first dimension must be the batch dimension.\n\nExample usage:\n```\n# Create a GlobalMaxPooling2D layer\npooling = GlobalMaxPooling2D(data_format='channels_last', keepdims=True)\n\n# Apply the layer to an input tensor\ninput_tensor = tf.random.normal([1, 2, 3, 4])\noutput_tensor = pooling(input_tensor)\n\n# Print the output tensor\nprint(output_tensor)\n```\nThis will output a tensor with shape `(1, 1, 1, 4)`, where the first dimension is the batch dimension, and the last dimension is the output of the global max pooling operation.", "  \"\"\"\nGlobalMaxPooling3D(GlobalPooling3D)\n\nPerforms a global max pooling operation on a 3D input tensor.\n\nInput shape\n-----------\n\n- If `data_format='channels_last'`:\n    - 5D tensor with shape: `(batch_size, axis1, axis2, axis3, channels)`\n- If `data_format='channels_first'`:\n    - 5D tensor with shape: `(batch_size, channels, axis1, axis2, axis3)`\n\nOutput shape\n------------\n\n- If `data_format='channels_last'`:\n    - 4D tensor with shape: `(batch_size, 1, 1, channels)`\n- If `data_format='channels_first'`:\n    - 4D tensor with shape: `(batch_size, channels, 1, 1)`\n\nArguments\n---------\n\n- **data_format**: A string, one of `channels_last` or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch_size, axis1, axis2, axis3, channels)` while `channels_first` corresponds to inputs with shape `(batch_size, channels, axis1, axis2, axis3)`.\n\n- **keepdims**: A boolean, whether to keep the dimensions or not. If `keepdims` is `False`, the rank of the tensor is reduced by 1 for each axis where max pooling is performed.\n\nRaises\n------\n\n- **ValueError**: If `data_format` is neither `'channels_last'` nor `'channels_first'`.\n\n- **ValueError**: If `keepdims` is not a boolean.\n\nExamples\n--------\n\n```\n>>> import keras\n>>> input_tensor = keras.Input(shape=(10, 10, 3))\n>>> output_tensor = keras.layers.GlobalMaxPooling3D()(input_tensor)\n>>> print(output_tensor.shape)\n(None, 1, 1, 3)\n```\n\"\"\"", "  \"\"\"\nGlobalPooling1D(data_format='channels_last', keepdims=False, **kwargs)\n\nThis layer applies global pooling to 1D input.\n\n__init__\n\n* data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, steps, features)` while `channels_first` corresponds to inputs with shape `(batch, features, steps)`.\n* keepdims: A boolean, whether to keep the temporal dimension or not. If `True`, the output shape will be `(batch, 1, features)` for `channels_last` data format and `(batch, features, 1)` for `channels_first` data format.\n* **kwargs: Additional keyword arguments to pass to the Layer constructor.\n\ncompute_output_shape\n\n* input_shape: A tensor shape.\n* Returns: A tensor shape, the output shape of the layer.\n\ncall\n\n* inputs: A tensor.\n* Returns: A tensor, the output of the layer.\n\nget_config\n\n* Returns: A dictionary containing the configuration of the layer.\n\"\"\"", "  \"\"\"\nGlobalPooling2D(Layer)\n\nThis layer applies global pooling operation to the input data.\n\n__init__\n\n* data_format: A string, one of \"channels_last\" (default) or \"channels_first\". The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width). It defaults to the value of `image_data_format` found in your Keras config file at `~/.keras/keras.json`.\n* keepdims: A boolean, whether to keep the dimensions or not. If `keepdims` is `False`, the rank of the tensor is reduced by 1 for each axis where global pooling is performed.\n* **kwargs: Additional keyword arguments passed to the parent class.\n\ncompute_output_shape\n\n* input_shape: A tensor shape.\n* Returns: A tensor shape.\n\ncall\n\n* inputs: A tensor.\n* Returns: A tensor.\n\nget_config\n\n* Returns: A dictionary containing the configuration of the layer.\n\"\"\"", "  \"\"\"\nGlobalPooling3D(data_format=None, keepdims=False, **kwargs)\n\nThis layer applies global pooling operation to 3D data.\n\n__init__\n\n* data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, ..., channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, ...)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be `channels_last`.\n* keepdims: A boolean, whether to keep the dimensions or not. If `keepdims` is `False`, the output shape will be `(batch, channels)` or `(batch, channels, 1, 1, 1)` depending on the `data_format`. If `keepdims` is `True`, the output shape will be `(batch, 1, 1, 1, channels)` or `(batch, 1, channels)` depending on the `data_format`.\n* **kwargs: Additional keyword arguments to pass to the Layer superclass.\n\ncompute_output_shape\n\n* input_shape: A tensor shape.\n* Returns: A tensor shape.\n\ncall\n\n* inputs: A tensor.\n* Returns: A tensor.\n\nget_config\n\n* Returns: A dictionary containing the configuration of the layer.\n\"\"\"", "  \"\"\"\nGroupTimeSeriesSplit(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling')\n\nSplit a dataset into training and test sets for time series forecasting.\n\nParameters:\n\n* test_size: The size of the test set.\n* train_size: The size of the training set. If None, the training set will be the same size as the test set.\n* n_splits: The number of splits. If None, the number of splits will be equal to the number of groups in the dataset minus the gap size minus the test size.\n* gap_size: The size of the gap between the training and test sets.\n* shift_size: The size of the shift between the training and test sets.\n* window_type: The type of window to use for the split. Can be either 'rolling' or 'expanding'.\n\nReturns:\n\n* A tuple containing the training and test sets.\n\nRaises:\n\n* ValueError: If either train_size or n_splits is not defined.\n* ValueError: If the window type is not 'rolling' or 'expanding'.\n* ValueError: If train_size is not None and the window type is 'expanding'.\n\nExample:\n\n>>> from sklearn.model_selection import GroupTimeSeriesSplit\n>>> from sklearn.linear_model import LinearRegression\n>>> import numpy as np\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n>>> y = np.array([1, 3, 5, 7, 9])\n>>> groups = np.array([1, 1, 2, 2, 2])\n>>> gts = GroupTimeSeriesSplit(test_size=2, train_size=2, gap_size=1, shift_size=1, window_type='rolling')\n>>> for train_idx, test_idx in gts.split(X, y, groups):\n...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n...\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [1 2] TEST: [3 4]\nTRAIN: [2 3] TEST: [4 5]\nTRAIN: [3 4] TEST: [5 6]\nTRAIN: [4 5] TEST: [6 7]\n\"\"\"", "  \"\"\"\nKmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0)\n\nInitialize a Kmeans model with the given parameters.\n\nParameters:\n\n* k: The number of clusters to form.\n* max_iter: The maximum number of iterations to perform.\n* convergence_tolerance: The tolerance for convergence.\n* random_seed: The random seed to use for initialization.\n* print_progress: Whether to print progress information.\n\nReturns:\n\n* A Kmeans model.\n\nMethods:\n\n* fit(X, init_params=True): Fit the model to the given data.\n* predict(X): Predict the cluster indices for the given data.\n* _fit(X, init_params=True): The internal fit method.\n* _get_cluster_idx(X, centroids): The internal method for getting the cluster indices.\n* _predict(X): The internal predict method.\n\"\"\"", "  \"\"\"\nLabelBinarizer(TransformerMixin, BaseEstimator)\n\nThis class is used to convert a multi-class label into a binary label.\n\nParameters\n----------\nneg_label : int, optional (default=0)\n    The negative label.\npos_label : int, optional (default=1)\n    The positive label.\nsparse_output : bool, optional (default=False)\n    Whether to output a sparse matrix.\n\nAttributes\n----------\nneg_label : int\n    The negative label.\npos_label : int\n    The positive label.\nsparse_output : bool\n    Whether to output a sparse matrix.\n\nMethods\n-------\nfit(y)\n    Fit the label binarizer to the given data.\nfit_transform(y)\n    Fit the label binarizer to the given data and transform it.\ntransform(y)\n    Transform the given data.\ninverse_transform(Y, threshold=None)\n    Inverse transform the given data.\n\nSee Also\n--------\nsklearn.preprocessing.label_binarize\n\nExamples\n--------\n>>> from sklearn.preprocessing import LabelBinarizer\n>>> lb = LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2, 3])\nLabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n>>> lb.transform([1, 2, 6, 4, 2, 3])\narray([[1, 0],\n       [0, 1],\n       [0, 0],\n       [0, 1],\n       [1, 0],\n       [0, 1]])\n>>> lb.inverse_transform([[1, 0], [0, 1]])\narray([1, 2])\n\"\"\"", "  \"\"\"\nLabelEncoder(TransformerMixin, BaseEstimator)\n\nThis class is used to encode categorical data into numerical data. It is a scikit-learn compatible transformer that can be used to preprocess data before training a machine learning model.\n\nMethods\n-------\n\nfit(y)\n    Fit the encoder to the data.\n\n    Parameters\n    ----------\n    y : array-like, shape (n_samples,)\n        The categorical data to be encoded.\n\n    Returns\n    -------\n    self\n\nfit_transform(y)\n    Fit the encoder to the data and return the encoded data.\n\n    Parameters\n    ----------\n    y : array-like, shape (n_samples,)\n        The categorical data to be encoded.\n\n    Returns\n    -------\n    y : array-like, shape (n_samples,)\n        The encoded data.\n\ntransform(y)\n    Transform the data using the fitted encoder.\n\n    Parameters\n    ----------\n    y : array-like, shape (n_samples,)\n        The categorical data to be transformed.\n\n    Returns\n    -------\n    y : array-like, shape (n_samples,)\n        The transformed data.\n\ninverse_transform(y)\n    Transform the data back to its original categorical form using the fitted encoder.\n\n    Parameters\n    ----------\n    y : array-like, shape (n_samples,)\n        The numerical data to be transformed back to its original categorical form.\n\n    Returns\n    -------\n    y : array-like, shape (n_samples,)\n        The original categorical data.\n\n_more_tags()\n    Return the tags for this estimator.\n\n    Returns\n    -------\n    tags : dict\n        The tags for this estimator.\n\"\"\"", "  \"\"\"\nLinearRegression(method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0)\n\nThis class implements a linear regression model using the specified method.\n\nParameters:\n\n* method: The method to use for training the model. Supported methods are 'direct', 'sgd', 'svd', and 'qr'.\n* eta: The learning rate for the SGD method.\n* epochs: The number of epochs to train the model for.\n* minibatches: The number of minibatches to use for training the model.\n* random_seed: The random seed to use for reproducibility.\n* print_progress: Whether to print progress information during training.\n\nMethods:\n\n* fit(X, y): Fits the model to the training data.\n* predict(X): Predicts the output for the given input.\n* _fit(X, y, init_params=True): Fits the model to the training data.\n* _normal_equation(X, y): Computes the parameters for the model using the normal equation.\n* _net_input(X): Computes the net input for the given input.\n* _sum_squared_error_cost(y, y_val): Computes the sum of squared errors between the predicted and actual values.\n\"\"\"", "  \"\"\"\nLogistic Regression Classifier\n\nThis class implements a logistic regression classifier. It is a supervised learning algorithm that predicts the class of a given input based on the input's features.\n\nParameters:\n\n* eta (float): The learning rate of the algorithm.\n* epochs (int): The number of epochs to train the model.\n* l2_lambda (float): The L2 regularization parameter.\n* minibatches (int): The number of minibatches to use for training.\n* random_seed (int): The random seed to use for reproducibility.\n* print_progress (int): Whether to print progress information during training.\n\nMethods:\n\n* fit(X, y): Fits the model to the training data.\n* predict(X): Predicts the class of the input.\n* predict_proba(X): Predicts the probability of each class.\n\nAttributes:\n\n* w_ (numpy.ndarray): The weights of the model.\n* b_ (numpy.ndarray): The bias of the model.\n* cost_ (list): The cost of the model at each epoch.\n* init_time_ (float): The time it took to initialize the model.\n\nNotes:\n\n* The model uses the logistic function to transform the input into a probability space.\n* The model uses L2 regularization to prevent overfitting.\n* The model uses stochastic gradient descent with minibatches to optimize the parameters.\n\"\"\"", "  ```\nDocstring for Loss class:\n\nLoss(name=None, reduction='sum_over_batch_size', dtype=None)\n\n    Initializes a Loss instance.\n\n    Arguments:\n        name: Optional name for the instance.\n        reduction: One of \"none\", \"sum\", or \"sum_over_batch_size\".\n            - \"none\": No reduction will be applied to the output.\n            - \"sum\": The output will be summed.\n            - \"sum_over_batch_size\": The output will be summed and then divided by the batch size.\n        dtype: Data type of the output.\n\n    Attributes:\n        name: Name of the instance.\n        reduction: Reduction method to use.\n        dtype: Data type of the output.\n\n    Methods:\n        __call__(y_true, y_pred, sample_weight=None):\n            Computes the loss.\n        call(y_true, y_pred):\n            Computes the loss.\n        get_config():\n            Returns the configuration of the instance.\n        from_config(config):\n            Creates a Loss instance from the configuration.\n        _obj_type():\n            Returns the type of the instance.\n```", "  \"\"\"MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)\n\nApplies a max pooling operation for spatial data.\n\n# Arguments\n\n- pool_size: An integer or tuple/list of a single integer,\n    specifying the size of the pooling window.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n- strides: An integer or tuple/list of a single integer,\n    specifying the strides of the pooling operation.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n- padding: A string, either 'valid' or 'same',\n    specifying the padding algorithm.\n    See `Conv1D` for more information.\n- data_format: A string, one of `channels_last` (default) or `channels_first`.\n    The ordering of the dimensions in the inputs.\n    `channels_last` corresponds to inputs with shape\n    `(batch, steps, features)` while `channels_first`\n    corresponds to inputs with shape `(batch, features, steps)`.\n- **kwargs: Additional keyword arguments passed to the `Conv1D` constructor.\n\n# Returns\n\nA tensor with the max pooled values.\n\"\"\"", "  \"\"\"\nMaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nA 2D max pooling layer.\n\n# Arguments\n\n- pool_size: The size of the pooling window.\n- strides: The stride of the pooling window.\n- padding: The padding algorithm to use.\n- data_format: The data format of the input.\n- kwargs: Additional keyword arguments to pass to the parent class.\n\n# Returns\n\nA 2D max pooling layer.\n\"\"\"", "  \"\"\"\nMaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nApplies a 3D max pooling over an input signal composed of several input planes.\n\n# Arguments\n\n* **pool_size**: The size of the pooling window.\n* **strides**: The strides of the pooling window.\n* **padding**: The padding to be applied to the input.\n* **data_format**: The data format of the input.\n* **kwargs**: Additional keyword arguments passed to the constructor of the parent class.\n\n# Returns\n\n* **output**: The output of the max pooling operation.\n\"\"\"", "  ```\n\nThis is a Python class docstring for the `Metric` class in the Keras library. The docstring provides a brief overview of the class and its purpose, as well as a list of its attributes and methods.\n\nHere is a breakdown of the docstring:\n\n* The first line explains that the `Metric` class is a subclass of `KerasSaveable` and is exported by the `keras_export` decorator.\n* The second line provides a brief description of the class and its purpose.\n* The third line lists the class attributes: `name`, `dtype`, `variables`, and `metrics`.\n* The fourth line lists the class methods: `__init__`, `reset_state`, `update_state`, `stateless_update_state`, `result`, `stateless_result`, `stateless_reset_state`, `dtype`, `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The fifth line provides a brief explanation of each attribute and method.\n* The sixth line lists the class properties: `name`, `dtype`, and `variables`.\n* The seventh line lists the class methods that are not listed in the previous line: `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The eighth line provides a brief explanation of each property and method.\n* The ninth line lists the class variables: `name`, `dtype`, and `variables`.\n* The tenth line lists the class methods that are not listed in the previous line: `__call__`, `get_config`, and `from_config`.\n* The eleventh line provides a brief explanation of each variable and method.\n* The twelfth line lists the class properties: `name`, `dtype`, and `variables`.\n* The thirteenth line lists the class methods that are not listed in the previous line: `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The fourteenth line provides a brief explanation of each property and method.\n* The fifteenth line lists the class variables: `name`, `dtype`, and `variables`.\n* The sixteenth line lists the class methods that are not listed in the previous line: `__call__`, `get_config`, and `from_config`.\n* The seventeenth line provides a brief explanation of each variable and method.\n* The eighteenth line lists the class properties: `name`, `dtype`, and `variables`.\n* The nineteenth line lists the class methods that are not listed in the previous line: `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The twentieth line provides a brief explanation of each property and method.\n* The twenty-first line lists the class variables: `name`, `dtype`, and `variables`.\n* The twenty-second line lists the class methods that are not listed in the previous line: `__call__`, `get_config`, and `from_config`.\n* The twenty-third line provides a brief explanation of each variable and method.\n* The twenty-fourth line lists the class properties: `name`, `dtype`, and `variables`.\n* The twenty-fifth line lists the class methods that are not listed in the previous line: `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The twenty-sixth line provides a brief explanation of each property and method.\n* The twenty-seventh line lists the class variables: `name`, `dtype`, and `variables`.\n* The twenty-eighth line lists the class methods that are not listed in the previous line: `__call__`, `get_config`, and `from_config`.\n* The twenty-ninth line provides a brief explanation of each variable and method.\n* The thirtieth line lists the class properties: `name`, `dtype`, and `variables`.\n* The thirty-first line lists the class methods that are not listed in the previous line: `add_variable`, `add_weight`, `variables`, `__call__`, `get_config`, and `from_config`.\n* The thirty-second line provides a brief explanation of each property and method.\n* The thirty-third line lists the class variables: `name`, `dtype`, and `variables`.\n", "  \"\"\"\nMultiLabelBinarizer(classes=None, sparse_output=False)\n\nThis class is used to convert a sequence of multi-label indicators into a binary matrix.\n\nParameters:\n\n* classes: The set of classes to use. If None, the classes are inferred from the data.\n* sparse_output: Whether to output a sparse matrix.\n\nMethods:\n\n* fit(y): Fits the classifier to the data.\n* fit_transform(y): Fits the classifier to the data and transforms the data.\n* transform(y): Transforms the data.\n* inverse_transform(yt): Inverse transforms the data.\n\nAttributes:\n\n* classes_: The set of classes.\n* sparse_output: Whether to output a sparse matrix.\n\nRaises:\n\n* ValueError: If the classes argument contains duplicate classes.\n* ValueError: If the classes argument is not a valid set of classes.\n* ValueError: If the data contains unknown classes.\n* ValueError: If the data contains invalid labels.\n\nNote: This class is a scikit-learn compatible transformer.\n\"\"\"", "  \"\"\"\nThis is a docstring for the OneHotEncoder class in scikit-learn.\n\nParameters:\n\n* categories: Either 'auto' or a list of arrays. If 'auto', the categories are determined from the training data.\n* drop: Either None, 'first', 'if_binary', or an array-like of shape (n_features,). If None, no categories are removed. If 'first', the first category in each feature is removed. If 'if_binary', the first category in each binary feature is removed. If an array-like, the specified categories are removed.\n* dtype: The dtype of the output.\n* handle_unknown: Either 'error', 'ignore', or 'infrequent_if_exist'. If 'error', unknown categories will raise an error. If 'ignore', unknown categories are ignored. If 'infrequent_if_exist', unknown categories are set to the infrequent category.\n* min_frequency: The minimum number of samples required to keep a category. If None, all categories are kept.\n* max_categories: The maximum number of categories to keep. If None, all categories are kept.\n* feature_name_combiner: Either 'concat' or a callable. If 'concat', the feature name and category are concatenated. If a callable, it should take a feature name and category as input and return a Python string.\n\nMethods:\n\n* fit: Fits the OneHotEncoder to the data.\n* transform: Transforms the data to a one-hot encoding.\n* inverse_transform: Transforms the one-hot encoding back to the original data.\n* get_feature_names_out: Returns the feature names of the transformed data.\n\"\"\"", "  \"\"\"\n\nThis docstring describes the `OPTICS` class, which is a clustering algorithm that uses the OPTICS (Ordering Points To Identify the Clustering Structure) algorithm to identify clusters in a dataset.\n\nThe class has the following parameters:\n\n* `min_samples`: The minimum number of samples required to form a dense region.\n* `max_eps`: The maximum epsilon value for the algorithm.\n* `metric`: The distance metric to use for the algorithm.\n* `p`: The power to use for the distance metric.\n* `metric_params`: Additional parameters for the distance metric.\n* `cluster_method`: The method to use for clustering.\n* `eps`: The epsilon value for the algorithm.\n* `xi`: The xi value for the algorithm.\n* `predecessor_correction`: Whether to use predecessor correction.\n* `min_cluster_size`: The minimum cluster size.\n* `algorithm`: The algorithm to use for the clustering.\n* `leaf_size`: The leaf size for the algorithm.\n* `memory`: The memory to use for the algorithm.\n* `n_jobs`: The number of jobs to use for the algorithm.\n\nThe class also has the following methods:\n\n* `fit`: Fits the algorithm to the data.\n* `predict`: Predicts the clusters for the data.\n* `fit_predict`: Fits the algorithm to the data and predicts the clusters.\n\nThe docstring also includes a brief description of the algorithm and its parameters, as well as a warning about the data type of the `metric` parameter.", "  ```\n\nThis docstring provides a detailed description of the `OrdinalEncoder` class, including its purpose, parameters, and methods.\n\nPurpose:\nThe `OrdinalEncoder` class is a scikit-learn transformer that encodes categorical features as ordinal integers. It is designed to handle missing values and infrequent categories.\n\nParameters:\n\n* `categories`: The categories to encode. If 'auto', the categories are determined from the training data.\n* `dtype`: The dtype of the output.\n* `handle_unknown`: How to handle unknown categories. Can be 'error', 'use_encoded_value', or 'ignore'.\n* `unknown_value`: The value to use for unknown categories when `handle_unknown` is 'use_encoded_value'.\n* `encoded_missing_value`: The value to use for missing values when `handle_unknown` is 'use_encoded_value'.\n* `min_frequency`: The minimum frequency of a category to be considered infrequent.\n* `max_categories`: The maximum number of categories to consider infrequent.\n\nMethods:\n\n* `__init__`: Initializes the `OrdinalEncoder` object.\n* `fit`: Fits the `OrdinalEncoder` object to the training data.\n* `transform`: Transforms the training data using the fitted `OrdinalEncoder` object.\n* `inverse_transform`: Transforms the encoded data back to its original categorical form.\n\nNote: The docstring also includes a brief description of the parameters and methods, as well as any additional details that may be useful for users.", "  ```\nPooling1D(\n    pool_function: Callable,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n    data_format: str,\n    name: str,\n    **kwargs\n)\n```\n\nThis class implements a 1D pooling layer. It takes a callable `pool_function` that performs the pooling operation, a tuple `pool_size` representing the size of the pooling window, a tuple `strides` representing the strides of the pooling operation, a string `padding` representing the padding mode, a string `data_format` representing the data format, and a string `name` representing the name of the layer.\n\nThe `call` method of this class takes an input tensor and applies the pooling operation to it. The `compute_output_shape` method computes the output shape of the layer. The `get_config` method returns the configuration of the layer.", "  ```\nPooling2D(\n    pool_function: Callable,\n    pool_size: Tuple[int, int],\n    strides: Tuple[int, int],\n    padding: str,\n    data_format: str,\n    name: str,\n    **kwargs\n)\n```\n\nThis docstring describes the `Pooling2D` class, which is a layer that applies a pooling function to 2D inputs. The `Pooling2D` class takes the following arguments:\n\n* `pool_function`: A callable function that applies the pooling operation to the input.\n* `pool_size`: A tuple of two integers, the height and width of the pooling window.\n* `strides`: A tuple of two integers, the height and width of the strides.\n* `padding`: A string, either 'valid' or 'same', indicating the type of padding to use.\n* `data_format`: A string, either 'channels_first' or 'channels_last', indicating the format of the input data.\n* `name`: A string, the name of the layer.\n* `**kwargs`: Additional keyword arguments to pass to the parent class.\n\nThe `Pooling2D` class has the following methods:\n\n* `call`: Applies the pooling function to the input.\n* `compute_output_shape`: Computes the output shape of the layer.\n* `get_config`: Returns the configuration of the layer.\n\nThe `Pooling2D` class is a subclass of the `Layer` class, and inherits its methods.", "  ```\nPooling3D(\n    pool_function: Callable,\n    pool_size: Tuple[int, int, int],\n    strides: Tuple[int, int, int],\n    padding: str = 'valid',\n    data_format: str = 'channels_last',\n    name: str = None,\n    **kwargs\n)\n```\n\nThis docstring describes the `Pooling3D` class, which is a layer that applies a 3D pooling operation to the input data.\n\nThe `Pooling3D` class takes the following arguments:\n\n* `pool_function`: A callable function that applies the pooling operation.\n* `pool_size`: A tuple of three integers, indicating the size of the pooling window in the three dimensions.\n* `strides`: A tuple of three integers, indicating the stride of the pooling window in the three dimensions.\n* `padding`: A string indicating the padding mode. The default value is `'valid'`.\n* `data_format`: A string indicating the data format of the input data. The default value is `'channels_last'`.\n* `name`: A string indicating the name of the layer.\n* `**kwargs`: Additional keyword arguments to be passed to the parent class.\n\nThe `Pooling3D` class has the following methods:\n\n* `call`: Applies the pooling operation to the input data.\n* `compute_output_shape`: Computes the output shape of the layer.\n* `get_config`: Returns the configuration of the layer.\n\nThe `Pooling3D` class is a subclass of the `Layer` class, and it inherits all the methods and properties of the `Layer` class.", "  \"\"\"\nThis is a docstring for the `PrincipalComponentAnalysis` class.\n\nThe class is a Python implementation of Principal Component Analysis (PCA) for dimensionality reduction. It takes in a dataset `X` and performs the following steps:\n\n1. Computes the covariance matrix of the data `X`.\n2. Computes the eigenvectors and eigenvalues of the covariance matrix using the `eigen` or `svd` solver.\n3. Selects the top `n_components` eigenvectors corresponding to the largest eigenvalues.\n4. Computes the projection matrix `w` that projects the data onto the selected eigenvectors.\n5. If `whitening` is set to `True`, the eigenvectors are normalized by the square root of the eigenvalues.\n\nThe `fit` method takes in a dataset `X` and fits the PCA model to the data. The `transform` method takes in a dataset `X` and transforms the data onto the principal components.\n\nThe class has the following attributes:\n\n* `solver`: The solver used to compute the eigenvectors and eigenvalues. Can be either `eigen` or `svd`.\n* `n_components`: The number of principal components to retain. If `None`, all principal components are retained.\n* `whitening`: Whether to whiten the eigenvectors.\n* `e_vals_`: The eigenvalues of the covariance matrix.\n* `e_vecs_`: The eigenvectors of the covariance matrix.\n* `w_`: The projection matrix that projects the data onto the principal components.\n* `e_vals_normalized_`: The normalized eigenvalues.\n* `loadings_`: The loadings of the principal components.\n\nThe class has the following methods:\n\n* `fit`: Fits the PCA model to the data.\n* `transform`: Transforms the data onto the principal components.\n* `_covariance_matrix`: Computes the covariance matrix of the data.\n* `_decomposition`: Computes the eigenvectors and eigenvalues of the covariance matrix.\n* `_loadings`: Computes the loadings of the principal components.\n* `_projection_matrix`: Computes the projection matrix that projects the data onto the principal components.\n\nThe class is designed to be used as a scikit-learn transformer, and can be used as follows:\n```\nfrom sklearn.decomposition import PCA\n\n# Create a PCA object with 2 principal components\npca = PCA(n_components=2)\n\n# Fit the PCA model to the data\npca.fit(X)\n\n# Transform the data onto the principal components\nX_pca = pca.transform(X)\n```", "  \"\"\"\nRMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n\nOptimizer that implements the RMSProp algorithm.\n\nArgs:\n    learning_rate: A `Tensor` or a floating point value. The learning rate to use.\n    rho: A `Tensor` or a floating point value. The decay rate for the second moment estimates.\n    momentum: A `Tensor` or a floating point value. The momentum for the moving average.\n    epsilon: A `Tensor` or a floating point value. A small value used for numerical stability.\n    centered: If `True`, gradients are normalized by the estimated variance of the gradient; if `False`, by the uncentered second moment. Setting this to `True` may help with training, but is slightly more expensive in terms of computation and memory. Defaults to `False`.\n    name: Optional name for the operations created when applying gradients. Defaults to \"RMSprop\".\n    **kwargs: Keyword arguments. Allowed keys are `clipnorm` and `clipvalue`.\n\nProperties:\n    _HAS_AGGREGATE_GRAD: Indicates whether gradients should be aggregated.\n\nMethods:\n    _create_slots: Creates slots for the first and second moment estimates.\n    _prepare_local: Prepares the local step.\n    _resource_apply_dense: Applies gradients to variables, using a dense representation of the gradients.\n    _resource_apply_sparse: Applies gradients to variables, using a sparse representation of the gradients.\n    set_weights: Sets the optimizer's weights.\n    get_config: Returns the config of the optimizer.\n\"\"\"", "  \"\"\"\nThis is a docstring for the SelfTrainingClassifier class.\n\nParameters:\n\n* estimator: The estimator to use for training.\n* base_estimator: The base estimator to use for training.\n* threshold: The threshold for the threshold criterion.\n* criterion: The criterion to use for selecting the most informative samples.\n* k_best: The number of most informative samples to select.\n* max_iter: The maximum number of iterations to perform.\n* verbose: Whether to print information about the training process.\n\nMethods:\n\n* fit: Fits the estimator to the training data.\n* predict: Predicts the labels for the given data.\n* predict_proba: Predicts the probabilities for the given data.\n* decision_function: Predicts the decision function for the given data.\n* predict_log_proba: Predicts the log probabilities for the given data.\n* score: Scores the given data.\n\nMetadata:\n\n* estimator: The estimator used for training.\n* base_estimator: The base estimator used for training.\n* threshold: The threshold used for the threshold criterion.\n* criterion: The criterion used for selecting the most informative samples.\n* k_best: The number of most informative samples selected.\n* max_iter: The maximum number of iterations performed.\n* verbose: Whether to print information about the training process.\n* transduction_: The transduction matrix.\n* labeled_iter_: The iteration in which each sample was labeled.\n* n_iter_: The number of iterations performed.\n* termination_condition_: The condition that caused the training to terminate.\n\nNote: This docstring is generated using a python docstring generator. The actual docstring may contain additional details.", "  ```\n\nThis is a docstring for the `SeparableConv` class in Keras.\n\nThe class is a convolutional layer that uses a separable filter, which is a depthwise convolution followed by a pointwise convolution. This allows the layer to reduce the number of parameters and computations required in the convolution operation.\n\nThe class takes the following arguments in its constructor:\n\n* `rank`: The rank of the convolution operation.\n* `filters`: The number of filters to use in the convolution operation.\n* `kernel_size`: The size of the convolution kernel.\n* `strides`: The stride of the convolution operation.\n* `padding`: The padding to use in the convolution operation.\n* `data_format`: The data format to use in the convolution operation.\n* `depth_multiplier`: The number of depthwise convolutions to use.\n* `dilation_rate`: The dilation rate to use in the convolution operation.\n* `activation`: The activation function to use in the convolution operation.\n* `use_bias`: Whether to use a bias term in the convolution operation.\n* `depthwise_initializer`: The initializer to use for the depthwise convolution kernel.\n* `pointwise_initializer`: The initializer to use for the pointwise convolution kernel.\n* `bias_initializer`: The initializer to use for the bias term.\n* `depthwise_regularizer`: The regularizer to use for the depthwise convolution kernel.\n* `pointwise_regularizer`: The regularizer to use for the pointwise convolution kernel.\n* `bias_regularizer`: The regularizer to use for the bias term.\n* `activity_regularizer`: The regularizer to use for the output of the convolution operation.\n* `depthwise_constraint`: The constraint to use for the depthwise convolution kernel.\n* `pointwise_constraint`: The constraint to use for the pointwise convolution kernel.\n* `bias_constraint`: The constraint to use for the bias term.\n\nThe class defines the following methods:\n\n* `__init__`: The constructor for the class. It initializes the layer with the given arguments.\n* `build`: The method that builds the layer. It creates the weights for the layer and sets the `built` attribute to `True`.\n* `call`: The method that performs the forward pass of the layer. It applies the convolution operation to the input tensor.\n* `get_config`: The method that returns the configuration of the layer. It returns a dictionary with the layer's arguments.\n\nThe class inherits from the `Conv` class in Keras, which is a convolutional layer that uses a standard convolution kernel. The `SeparableConv` class is a specialization of the `Conv` class that uses a separable filter.", "  \"\"\"\n    SeparableConv1D(filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs)\n\n    A SeparableConv1D layer.\n\n    Separable convolutions consist of first performing a depthwise convolution\n    followed by a pointwise convolution. The depthwise convolution allows\n    filtering of the input channels independently, while the pointwise\n    convolution allows filtering of the resulting features. This layer\n    performs a separable depthwise convolution with a pointwise convolution.\n\n    Args:\n        filters: Integer, the dimensionality of the output space (i.e. the number\n            of output filters in the convolution).\n        kernel_size: An integer or tuple/list of a single integer, specifying the\n            length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer, specifying the\n            stride length of the convolution. Specifying any stride value != 1 is\n            incompatible with specifying any `dilation_rate` value != 1.\n        padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs.\n            `channels_last` corresponds to inputs with shape\n            `(batch, length, channels)` while `channels_first` corresponds to\n            inputs with shape `(batch, channels, length)`.\n        dilation_rate: An integer or tuple/list of a single integer, specifying\n            the dilation rate to use for dilated convolution. Currently, specifying\n            any `dilation_rate` value != 1 is incompatible with specifying any\n            `strides` value != 1.\n        depth_multiplier: The number of depthwise convolution output channels for\n            each input channel. The total number of depthwise convolution output\n            channels will be equal to `filters_in * depth_multiplier`.\n        activation: Activation function to use. If you don't specify anything, no\n            activation is applied (see `keras.activations`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        depthwise_initializer: Initializer for the depthwise kernel matrix.\n        pointwise_initializer: Initializer for the pointwise kernel matrix.\n        bias_initializer: Initializer for the bias vector.\n        depthwise_regularizer: Regularizer function applied to the depthwise kernel\n            matrix.\n        pointwise_regularizer: Regularizer function applied to the pointwise kernel\n            matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to the output of the\n            layer (its \"activation\").\n        depthwise_constraint: Constraint function applied to the depthwise kernel\n            matrix.\n        pointwise_constraint: Constraint function applied to the pointwise kernel\n            matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n        **kwargs: Additional keyword arguments to pass to the `Layer` superclass.\n    \"\"\"", "  \"\"\"\nSeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None, **kwargs)\n\nA 2D convolution layer with separable filters.\n\nThis layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If `use_bias` is True, a bias vector is added to the output.\n\nInput shape:\n    4D tensor with shape:\n    `(samples, channels, rows, cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(samples, rows, cols, channels)` if data_format='channels_last'.\n\nOutput shape:\n    4D tensor with shape:\n    `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n    `rows` and `cols` values might have changed due to padding.\n\nArguments:\n    filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n        specifying the strides of the convolution along the height and width.\n        Can be a single integer to specify the same value for all spatial dimensions.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding.\n        `\"same\"` means padding with zeroes.\n        Note that `\"same\"` is slightly inconsistent across backends with\n        different `image_data_format` values.\n        For instance, with `'channels_last'`,\n        `'same'` corresponds to padding the input with 0s to the left and right\n        of the input, whereas with `'channels_first'`,\n        `'same'` corresponds to padding the input with 0s up to the right edge\n        of the input.\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first` corresponds to\n        inputs with shape `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be `'channels_last'`.\n    dilation_rate: An integer or tuple/list of 2 integers, specifying\n        the dilation rate to use for dilated convolution.\n        Can be a single integer to specify the same value for\n        all spatial dimensions.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any stride value != 1.\n    depth_multiplier: The number of depthwise convolution output channels\n        for each input channel.\n        The total number of depthwise convolution output channels\n        will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer", "  \"\"\"\n\n    This docstring provides a brief overview of the `SequentialFeatureSelector` class. It explains that the class is a feature selection algorithm that selects the most relevant features for a given task. It also mentions that the class is a subclass of `_BaseXComposition` and `MetaEstimatorMixin`, which are both important components of the class.\n\n    The docstring also provides a list of the class's parameters, including `estimator`, `k_features`, `forward`, `floating`, `verbose`, `scoring`, `cv`, `n_jobs`, `pre_dispatch`, `clone_estimator`, `fixed_features`, and `feature_groups`. It also explains the purpose of each parameter and provides a brief description of what each parameter does.\n\n    The docstring also includes a section on the class's methods, which includes the `__init__`, `fit`, `transform`, `fit_transform`, `get_metric_dict`, and `_check_fitted` methods. The `__init__` method is used to initialize the class, while the `fit` method is used to fit the class to the data. The `transform` method is used to transform the data, and the `fit_transform` method is used to fit the class to the data and then transform it. The `get_metric_dict` method is used to get a dictionary of the class's metrics, and the `_check_fitted` method is used to check if the class has been fitted.\n\n    Overall, this docstring provides a clear and concise overview of the `SequentialFeatureSelector` class and its purpose, as well as a brief explanation of its parameters and methods.", "  \"\"\"\nSGD(Stochastic Gradient Descent) optimizer for TensorFlow.\n\nThis optimizer uses a learning rate to update the model parameters in a way that minimizes the loss.\n\nThe SGD optimizer has the following arguments:\n\n* learning_rate: The learning rate to use for each iteration.\n* momentum: The momentum to use for each iteration.\n* nesterov: Whether to use Nesterov momentum.\n* name: The name of the optimizer.\n\nThe SGD optimizer has the following methods:\n\n* get_config: Returns the configuration of the optimizer.\n* _create_slots: Creates the slots for the optimizer.\n* _prepare_local: Prepares the local variables for the optimizer.\n* _resource_apply_dense: Applies the optimizer to a dense resource variable.\n* _resource_apply_sparse_duplicate_indices: Applies the optimizer to a sparse resource variable with duplicate indices.\n* _resource_apply_sparse: Applies the optimizer to a sparse resource variable.\n\nThe SGD optimizer is a popular optimizer in TensorFlow and is widely used in deep learning. It is a simple optimizer that uses a learning rate to update the model parameters in a way that minimizes the loss. It is a good choice for many applications, but it may not be the best choice for all applications.\n\nHere is an example of how to use the SGD optimizer in TensorFlow:\n\n```\nimport tensorflow as tf\n\n# Create a model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model with the SGD optimizer\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\n```\n\nThis code will create a model, compile the model with the SGD optimizer, and train the model on the training data for 10 epochs with a batch size of 32.\n\nThe SGD optimizer is a simple optimizer that uses a learning rate to update the model parameters in a way that minimizes the loss. It is a good choice for many applications, but it may not be the best choice for all applications. It is important to carefully consider the choice of optimizer and the hyperparameters of the optimizer when training a model.", "  \"\"\"\nThis is a docstring for the SoftmaxRegression class in scikit-learn.\n\nParameters:\n\n* eta: The learning rate for the optimization algorithm.\n* epochs: The number of epochs to run the optimization algorithm for.\n* l2: The L2 regularization parameter.\n* minibatches: The number of minibatches to use for the optimization algorithm.\n* n_classes: The number of classes in the target variable.\n* random_seed: The random seed to use for the optimization algorithm.\n* print_progress: Whether to print progress information during training.\n\nMethods:\n\n* _net_input: Computes the input to the softmax activation function.\n* _softmax_activation: Computes the softmax activation function.\n* _cross_entropy: Computes the cross-entropy loss.\n* _cost: Computes the cost function.\n* _to_classlabels: Converts the predicted probabilities to class labels.\n* _forward: Performs a forward pass through the network.\n* _backward: Performs a backward pass through the network.\n* _fit: Trains the model on the given data.\n* predict_proba: Predicts the probabilities of each class for the given data.\n* _predict: Predicts the class labels for the given data.\n\"\"\"", "  ```\nDocstring for TargetEncoder class:\n\nTargetEncoder is a class for encoding categorical features using the target encoding method.\n\nParameters:\n\n* categories: The categories to encode. If 'auto', the categories will be inferred from the data.\n* target_type: The type of target to encode. If 'auto', the target type will be inferred from the data.\n* smooth: The amount of smoothing to apply to the target encoding. If 'auto', the amount of smoothing will be inferred from the data.\n* cv: The number of cross-validation folds to use when fitting the model.\n* shuffle: Whether to shuffle the data before fitting the model.\n* random_state: The random state to use when fitting the model.\n\nMethods:\n\n* fit: Fits the model to the data.\n* fit_transform: Fits the model to the data and transforms the data.\n* transform: Transforms the data using the model.\n* get_feature_names_out: Returns the names of the output features.\n\nTags:\n\n* requires_y: True\n```", "  \"\"\"\n\nTransactionEncoder\n==================\n\nA TransactionEncoder is a class that encodes transactions into a sparse matrix.\n\nParameters\n----------\n\nNone\n\nMethods\n-------\n\nfit(X)\n    Fit the TransactionEncoder to the given transactions.\n\ntransform(X, sparse=False)\n    Transform the given transactions into a sparse matrix.\n\ninverse_transform(array)\n    Inverse transform the given sparse matrix into a list of transactions.\n\nfit_transform(X, sparse=False)\n    Fit the TransactionEncoder to the given transactions and transform them into a sparse matrix.\n\nget_feature_names_out()\n    Get the feature names of the output matrix.\n\n\"\"\"", "  \"\"\"\nUpSampling1D layer for 1D inputs.\n\n# Arguments\n    size: Integer or tuple of integers, size of the upsampling factor for dim1.\n        If an integer, the same value is used for the depth and height.\n        If a tuple, the first value is used for the depth,\n        the second for the height and the third for the width.\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, length, channels)` while `channels_first` corresponds to\n        inputs with shape `(batch, channels, length)`.\n\n# Input shape\n    Arbitrary. Use the keyword argument `input_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Output shape\n    Arbitrary. Use the keyword argument `output_shape`\n    (tuple of integers, does not include the samples axis)\n    when using this layer as the first layer in a model.\n\n# Masking\n    This layer does not use masking.\n\n# Examples\n    ```python\n        # add a UpSampling1D layer to a sequential model\n        model = Sequential()\n        model.add(UpSampling1D(size=2, input_shape=(10, 1)))\n        # now the model will take as input arrays of shape (*, 10, 1)\n        # and output arrays of shape (*, 20, 1)\n        # after the UpSampling1D layer,\n        # the model will have one fewer parameters\n        # than the model that was just created.\n    ```\n    \"\"\"", "  ```\nUpSampling2D(\n    size=(2, 2),\n    data_format=None,\n    interpolation='nearest',\n    **kwargs\n)\n```\n\nThis docstring describes the `UpSampling2D` class, which is a layer in the Keras library for deep learning. The class takes in the following arguments:\n\n* `size`: A tuple of two integers, representing the height and width of the output.\n* `data_format`: A string, either `'channels_first'` or `'channels_last'`, specifying the format of the input data.\n* `interpolation`: A string, either `'nearest'` or `'bilinear'`, specifying the interpolation method to use.\n* `**kwargs`: Additional keyword arguments to pass to the parent class.\n\nThe class has the following methods:\n\n* `__init__`: The constructor method, which initializes the layer with the given arguments.\n* `compute_output_shape`: A method that computes the output shape of the layer.\n* `call`: A method that performs the upsampling operation on the input data.\n* `get_config`: A method that returns the configuration of the layer.\n\nThe class also inherits from the `Layer` class in Keras, which provides additional methods for the layer, such as `build`, `call`, and `compute_mask`.", "  ```\nUpSampling3D(size=(2, 2, 2), data_format=None, **kwargs)\n```\n\nThis is a class for upsampling a 3D input tensor. It takes in a size tuple, which specifies the upsampling factor for each dimension, and an optional data format string. The data format string can be either \"channels_first\" or \"channels_last\".\n\nThe class has the following methods:\n\n* `__init__`: Initializes the class with the given size and data format.\n* `compute_output_shape`: Computes the output shape of the upsampling operation.\n* `call`: Performs the upsampling operation on the input tensor.\n* `get_config`: Returns the configuration of the class.\n\nThe class also inherits from the `Layer` class in Keras, which provides additional methods for building and configuring neural network layers.", "  \"\"\"\nZeroPadding1D layer for Keras.\n\n# Arguments\npadding: Int, or tuple of two ints, or tuple of two tuples of ints.\n    - If int:\n        - Specifies the total padding along the two dimensions.\n    - If tuple of two ints:\n        - Specifies the padding along the two dimensions separately as\n          (symmetric_pad_left, symmetric_pad_right).\n    - If tuple of two tuples of ints:\n        - Specifies the padding along the two dimensions separately as\n          ((left_pad_1, right_pad_1), (left_pad_2, right_pad_2)).\n\n# Input shape\n3D tensor with shape:\n(samples, timesteps, channels)\n\n# Output shape\n3D tensor with shape:\n(samples, new_timesteps, channels)\n\n# Masking\nThis layer does not use masking.\n\n# Example\n```\n# Add padding to the input\nmodel = Sequential()\nmodel.add(ZeroPadding1D(padding=2, input_shape=(10, 1)))\n```\n\"\"\"", "  \"\"\"\nZeroPadding2D(padding=(1, 1), data_format=None, **kwargs)\n\nThis layer adds zero-padding to the 2D input tensor.\n\n# Arguments\n\n- **padding**: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints. The amount of padding along the height and width axes. If a single int is provided, the same padding will be used for both the height and width axes. If a tuple of 2 ints is provided, the first int will be used as the padding for the height axis and the second int will be used as the padding for the width axis. If a tuple of 2 tuples of 2 ints is provided, the first tuple will be used as the padding for the top and bottom axes and the second tuple will be used as the padding for the left and right axes.\n- **data_format**: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be `channels_last`.\n\n# Input shape\n\n4D tensor with shape:\n\n- If `data_format` is `\"channels_last\"`:\n    `(batch_size, rows, cols, channels)`\n- If `data_format` is `\"channels_first\"`:\n    `(batch_size, channels, rows, cols)`\n\n# Output shape\n\n4D tensor with shape:\n\n- If `data_format` is `\"channels_last\"`:\n    `(batch_size, padded_rows, padded_cols, channels)`\n- If `data_format` is `\"channels_first\"`:\n    `(batch_size, channels, padded_rows, padded_cols)`\n\n# Masking\n\nThis layer does not use masking.\n\n# Examples\n\n```python\n# Add zero-padding to a 2D input tensor of shape (2, 2, 3)\nmodel = Sequential()\nmodel.add(ZeroPadding2D((1, 1), input_shape=(2, 2, 3)))\n```\n\"\"\"", "  ```\nZeroPadding3D(\n    padding=(1, 1, 1),\n    data_format=None,\n    **kwargs\n)\n```\n\nThis docstring describes the `ZeroPadding3D` class, which is a layer in the Keras library for deep learning. The class is used to add zero padding to a 3D input tensor.\n\nThe `__init__` method of the class takes the following arguments:\n\n* `padding`: The amount of padding to add to the input tensor. This can be an integer or a tuple of three integers. If it is an integer, then the same amount of padding is added to all three dimensions. If it is a tuple of three integers, then the first integer is the amount of padding to add to the first dimension, the second integer is the amount of padding to add to the second dimension, and the third integer is the amount of padding to add to the third dimension.\n* `data_format`: The data format of the input tensor. This can be either `'channels_first'` or `'channels_last'`. If it is `'channels_first'`, then the first dimension of the input tensor is the number of channels, and the second dimension is the height, the third dimension is the width, and the fourth dimension is the depth. If it is `'channels_last'`, then the first dimension is the height, the second dimension is the width, the third dimension is the depth, and the fourth dimension is the number of channels.\n* `**kwargs`: Additional keyword arguments to pass to the parent class.\n\nThe `compute_output_shape` method of the class takes the shape of the input tensor as an argument and returns the shape of the output tensor. This method is used to determine the output shape of the layer.\n\nThe `call` method of the class takes the input tensor as an argument and returns the output tensor after applying the zero padding. This method is used to apply the zero padding to the input tensor.\n\nThe `get_config` method of the class returns the configuration of the layer. This method is used to save the configuration of the layer when it is serialized.\n\nOverall, the `ZeroPadding3D` class is a simple layer that can be used to add zero padding to a 3D input tensor. It is a useful layer to use when working with convolutional neural networks (CNNs) that require zero padding.", "  \"\"\"\n    A Python docstring for the `_BaseEncoder` class.\n\n    This class is a base class for all encoders in scikit-learn. It provides a set of methods and properties that are common to all encoders.\n\n    Parameters\n    ----------\n    categories : 'auto' or array-like, default='auto'\n        Categories (unique values) in the data. If 'auto', the categories are\n        inferred from the data.\n\n    Attributes\n    ----------\n    categories_ : array-like, shape (n_categories,)\n        Categories (unique values) in the data.\n\n    n_features_in_ : int\n        Number of features in the data.\n\n    Methods\n    -------\n    _check_X(X, force_all_finite=True)\n        Checks the input data and ensures it is in a format that can be used by\n        the encoder.\n\n    _fit(X, handle_unknown='error', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False)\n        Fits the encoder to the input data.\n\n    _transform(X, handle_unknown='error', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None)\n        Transforms the input data using the fitted encoder.\n\n    infrequent_categories_ : list of arrays\n        List of arrays containing the infrequent categories for each feature.\n\n    _check_infrequent_enabled()\n        Checks if the encoder is enabled for infrequent categories.\n\n    _identify_infrequent(category_count, n_samples, col_idx)\n        Identifies the infrequent categories for a given feature.\n\n    _fit_infrequent_category_mapping(n_samples, category_counts, missing_indices)\n        Fits the mapping for infrequent categories.\n\n    _map_infrequent_categories(X_int, X_mask, ignore_category_indices)\n        Maps the infrequent categories in the input data.\n\n    _more_tags()\n        Returns a dictionary of additional tags for the encoder.\n\n    \"\"\""]