["Adamax Optimizer.\n\nThe Adamax optimizer is a variant of the Adam optimizer that uses the infinity norm to compute the adaptive learning rate.\n\nThis optimizer is known for its ability to handle large learning rates and is often used in deep learning applications.\n\nThe Adamax optimizer has the following hyperparameters:\n\n* learning_rate: The learning rate for the optimizer.\n* beta_1: The exponential decay rate for the first moment estimates.\n* beta_2: The exponential decay rate for the second moment estimates.\n* epsilon: A small value added to the denominator for numerical stability.\n\nThe Adamax optimizer is implemented as a subclass of the `OptimizerV2` class and inherits its methods. The `_create_slots` method is used to create the slots for the first and second moment estimates, and the `_prepare_local` method is used to prepare the local variables for the optimizer. The `_resource_apply_dense` and `_resource_apply_sparse` methods are used to apply the optimizer to the variables, and the `get_config` method is used to get the configuration of the optimizer.", "This class is used for transforming and inverse transforming data. It is a subclass of TransformerMixin. \n\nThe transform method takes in a dataset X and applies a pooling function to it based on the labels. The pooling function is either the mean or a user-defined function. The result is a new dataset where each sample is represented by a single value.\n\nThe inverse_transform method takes in a dataset X and returns the original dataset. It does this by using the unique labels and their corresponding indices to reconstruct the original dataset.", "Initializes a 1D average pooling layer.", "Initializes a 2D average pooling layer.\n\nThis layer sets the output dimensions to the size of the input minus the pool size, with the strides and padding specified. \n\nThe pooling operation reduces spatial dimensions of the input data. \n\nThe default pool size is 2x2, the default strides is None (which means the pool size), and the default padding is 'valid'. \n\nThe data format is 'channels_last' by default.", "Initializes a 3D average pooling layer. \n\nThis layer sets the output to be the average of the input values, \nover a window of size specified by pool_size. The stride of the \nwindow is specified by strides. Padding can be 'valid' or'same'. \n\nThe data_format can be 'channels_last' or 'channels_first'. \n\nParameters: \npool_size (tuple): The size of the pooling window. \nstrides (tuple): The stride of the pooling window. \npadding (str): The padding mode. \ndata_format (str): The format of the input data. \n**kwargs: Additional keyword arguments.", ".. \n\nThis class implements a Bayesian Gaussian Mixture model. It is a subclass of the `BaseMixture` class and inherits its methods. \n\nThe Bayesian Gaussian Mixture model is a probabilistic model that represents a mixture of Gaussian distributions. It is a flexible model that can be used to model complex data distributions. \n\nThe model has several parameters, including the number of components, the covariance type, the weight concentration prior type, the weight concentration prior, the mean precision prior, the mean prior, the degrees of freedom prior, and the covariance prior. \n\nThe model can be initialized with a set of parameters, and it can be used to estimate the parameters of the model using the Expectation-Maximization algorithm. \n\nThe model can also be used to compute the log probability of a set of data given the model, and it can be used to compute the lower bound of the log probability of the data given the model. \n\nThe model is implemented using a combination of NumPy and SciPy functions.", ".. \n\nThis is a Convolutional Layer class, a fundamental component of Convolutional Neural Networks (CNNs). It is used to extract features from input data by applying a convolution operation. \n\nThe class has several attributes and methods. The attributes include the rank of the convolution, the number of filters, the kernel size, the strides, the padding, the data format, the dilation rate, the number of groups, the activation function, and the use of bias. \n\nThe methods include the initialization method, the build method, the call method, the compute output shape method, the get config method, and the recreate convolution operation method. \n\nThe initialization method sets the attributes of the class and validates the input parameters. The build method creates the kernel and bias weights of the convolution operation. The call method applies the convolution operation to the input data. The compute output shape method calculates the shape of the output data. The get config method returns the configuration of the class. The recreate convolution operation method is used to recreate the convolution operation.", "A Conv1D layer for 1D convolutional neural networks.", "\"\"\"\nConv1DTranspose is a 1D transposed convolutional layer (also known as a fractional max pooling layer).\n\nIt is often used in upsampling layers of a U-Net or similar architectures.\n\nThis layer does the inverse operation of a Conv1D layer and is used for upsampling the spatial dimensions.\n\nThe layer uses a kernel size, strides, and padding to determine the output shape.\n\nThe output padding is used to control the output shape when the stride is greater than 1.\n\nThe layer supports activation functions and the use of bias.\n\nThe layer also supports input shape and output shape computation.\n\nThe layer is typically used in combination with a Conv1D layer to form a U-Net or similar architecture.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a part of the Keras API and is used for building deep learning models.\n\nThe layer is a part of the TensorFlow API and is used for building deep learning models.\n\nThe layer is a", "A Conv2D layer (also known as a convolutional layer) is a type of neural network layer that is commonly used in image and signal processing. It applies a convolution operation to the input data, which involves sliding a small filter over the input data and computing the dot product of the filter and the input data at each position. The output of the convolution operation is a feature map, which is a 2D array that represents the output of the convolution operation at each position in the input data.", ".. \n\nThis is a Conv2DTranspose layer class. It is a type of neural network layer that performs a 2D convolutional transpose operation. This is often used in upsampling operations, such as in image segmentation tasks. \n\nThe layer takes in a 4D input tensor and applies a 2D convolutional transpose operation to it. The output of the layer is a 4D tensor with the same number of channels as the input, but with a larger spatial size. \n\nThe layer has several parameters that can be set, including the number of filters, the kernel size, the strides, the padding, the output padding, the data format, the dilation rate, the activation function, and the use of bias. \n\nThe layer also has several methods, including an initializer method, a build method, a call method, a compute output shape method, and a get config method.", "A 3D convolutional layer (e.g. spatial convolution over depths, i.e., images in the input being 3D).", ".. \n\nThis is a 3D convolutional transpose layer (often called a deconvolutional layer). It is used for upsampling the spatial dimensions of the input data. \n\nThe layer uses a 3D convolutional transpose operation to generate the output. The output shape is determined by the input shape, the kernel size, the strides, the padding, and the output padding. \n\nThe layer supports both 'channels_first' and 'channels_last' data formats. \n\nThe layer also supports activation functions, kernel regularizers, bias regularizers, and activity regularizers.", "\"\"\"\nCropping1D(Layer)\n\nThis class is used to crop the input data along the second axis.\n\n    Parameters:\n    cropping (tuple): A tuple of two integers representing the number of elements to crop from the start and end of the input data.\n\n    Methods:\n    __init__(self, cropping=(1, 1), **kwargs): Initializes the Cropping1D layer with the given cropping parameters.\n\n    compute_output_shape(self, input_shape): Computes the output shape of the layer given the input shape.\n\n    call(self, inputs): Applies the cropping operation to the input data.\n\n    get_config(self): Returns the configuration of the layer.", "\"\"\"\nCropping2D layer for 2D data.\n\nThis layer crops the input data by a specified amount.\n\n    Args:\n        cropping: A tuple of two tuples of two integers, specifying the amount of cropping.\n            The first tuple is for the height and the second tuple is for the width.\n            If a single integer is provided, it will be used for both height and width.\n            If a tuple of two integers is provided, it will be used for both height and width.\n            If a tuple of two tuples of two integers is provided, it will be used for both height and width.\n        data_format: A string, one of \"channels_first\" or \"channels_last\".\n            The default is \"channels_last\".\n\n    Input shape:\n        A 4D tensor.\n\n    Output shape:\n        A 4D tensor with the same number of channels as the input, but with the specified amount of cropping.\n\"\"\"", ".. \n\nThis is a 3D Cropping layer which crops the input 3D data along the spatial dimensions. \n\nIt can be used to remove a specified number of pixels from the beginning and/or end of the input data along each spatial dimension. \n\nThe cropping can be specified in two ways: either by providing a single integer value for each dimension, in which case the same number of pixels will be cropped from the beginning and end of each dimension, or by providing a tuple of two integers for each dimension, in which case the first integer specifies the number of pixels to crop from the beginning of the dimension and the second integer specifies the number of pixels to crop from the end of the dimension. \n\nThe layer supports both the 'channels_first' and 'channels_last' data formats.", "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups data points into clusters based on their density and proximity. It is a popular algorithm for clustering data that has varying densities and noise.\n\nThe algorithm works by first defining a neighborhood radius (eps) and a minimum number of samples (min_samples) required to form a dense region. It then iterates through the data points, identifying core samples (points that have at least min_samples neighboring points within eps distance) and noise points (points that do not have enough neighboring points within eps distance). The algorithm then assigns labels to the data points based on their density and proximity to the core samples.\n\nThe DBSCAN algorithm has several parameters that can be tuned to control its behavior. These include the neighborhood radius (eps), the minimum number of samples (min_samples), the distance metric (metric), and the algorithm used to compute the nearest neighbors (algorithm).\n\nThe DBSCAN algorithm is often used in applications where the data has varying densities and noise, such as in image segmentation, text clustering, and bioinformatics.", ".. \n\nThis is a Depthwise Convolutional 2D layer class. It is used for spatial 2D convolution with separable filters. The layer is designed to work with 4D tensors, where the last dimension is the number of channels. The depthwise convolution operation is applied to each channel separately, and then the outputs are concatenated. This is useful for reducing the number of parameters and the number of computations in the model.", "\"\"\"\nInitializes a Embedding layer.\n\nArgs:\n    input_dim: The number of features in the input.\n    output_dim: The number of features in the output.\n    embeddings_initializer: Initializer for the embeddings matrix.\n    embeddings_regularizer: Regularizer for the embeddings matrix.\n    activity_regularizer: Regularizer for the output of the layer.\n    embeddings_constraint: Constraint for the embeddings matrix.\n    mask_zero: Whether to treat the zero index as padding.\n    input_length: The length of the input sequence.\n    **kwargs: Additional keyword arguments.\n\nRaises:\n    ValueError: If both `input_dim` and `output_dim` are less than or equal to 0.\n\nMethods:\n    build: Builds the layer.\n    compute_mask: Computes the mask for the layer.\n    compute_output_shape: Computes the output shape of the layer.\n    call: Calls the layer.\n    get_config: Gets the configuration of the layer.\n\"\"\"", ".. class:: Flask(import_name: str, static_url_path: str | None=None, static_folder: str | os.PathLike[str] | None='static', static_host: str | None=None, host_matching: bool=False, subdomain_matching: bool=False, template_folder: str | os.PathLike[str] | None='templates', instance_path: str | None=None, instance_relative_config: bool=False, root_path: str | None=None)\n\n    The Flask class is a web application framework that provides a flexible and modular way to build web applications. It is designed to be easy to use and extend, and provides a lot of built-in functionality for handling HTTP requests and responses, routing, and templating.\n\n    :param import_name: The name of the module that contains the application.\n    :param static_url_path: The URL path that static files are served from.\n    :param static_folder: The folder that contains the static files.\n    :param static_host: The host that static files are served from.\n    :param host_matching: Whether to match the host of the request.\n    :param subdomain_matching: Whether to match the subdomain of the request.\n    :param template_folder: The folder that contains the templates.\n    :param instance_path: The path to the instance folder.\n    :param instance_relative_config: Whether the configuration file is relative to the instance folder.\n    :param root_path: The root path of the application.\n\n    :return: None\n\n    :raises: None", "\"\"\"\nFunctionTransformer is a class that can be used to transform data using a given function. It can also be used to inverse transform data using an inverse function. The transformer can be used to validate the input data and check if the provided functions are inverse of each other.\n\nThe class has several methods:\n- `__init__`: Initializes the transformer with a function, inverse function, and other parameters.\n- `_check_input`: Checks the input data and validates it if necessary.\n- `_check_inverse_transform`: Checks if the provided functions are inverse of each other.\n- `fit`: Fits the transformer to the input data.\n- `transform`: Transforms the input data using the provided function.\n- `inverse_transform`: Inverse transforms the input data using the provided inverse function.\n- `get_feature_names_out`: Returns the feature names for the output data.\n- `set_output`: Sets the output format for the transformer.\n\nThe transformer can be used to transform data in a stateless manner, meaning that it does not store any information about the input data. It can also be used to validate the input data and check if the provided functions are inverse of each other.\n\nThe transformer can be used with various types of data, including dense and sparse arrays. It can also be used with various types of functions, including callable functions and None.\n\nThe transformer has several parameters that can be set, including the function, inverse function, validate, accept_sparse, check_inverse, feature_names_out, kw_args, and inv_kw_args. The parameters can be set using the `__init__` method or the `set_output` method.\n\nThe transformer has several methods that can be used to check the input data and the provided functions. The methods include `_check_input`, `_check_inverse_transform`, and `get_feature_names_out`. The methods can be used to validate the input data and check if the provided functions are inverse of each other.\n\nThe transformer has several methods that can be used to transform the input data. The methods include `transform` and `inverse_transform`. The methods can be used to transform the input data using the provided function and inverse function.\n\nThe transformer has several methods that can be used to get the feature names for the output data. The methods include `get_feature_names_out`. The method can be used to get the feature names for the output data.\n\nThe transformer has several methods that can be used to set the output format for the transformer. The methods include `set_output`. The method can be used to set the output format for the transformer.\n\nThe transformer has several methods that can be used to check if the provided functions are inverse of each other. The methods include `_check_inverse_transform`. The method can be used to check if the provided functions are inverse of each other.\n\nThe transformer has several methods that can be used to validate the input data. The methods include `_check_input`. The method can be used to validate the input data.\n\nThe transformer has several methods that can be used to get the feature names for the output data. The methods include `get_feature_names_out`. The method can be used to get the feature names for the output data.\n\nThe transformer has several methods that can be used to set the output format for the transformer. The methods include `set_output`. The method can be used to set the output format for the transformer.\n\nThe transformer has several methods that can be used to check if the provided functions are inverse of each other. The methods include `_check_inverse_transform`. The method can be used to check if the provided functions are inverse of each other.\n\nThe transformer has several methods that can be used to validate the input data. The methods include `_check_input`. The method can be used to validate the input data.\n\nThe transformer has several methods that can be used to get the feature names for the output data. The methods include `get_feature_names_out`. The method can be used to get the feature names for the output data.\n\nThe transformer has several methods that can be used to set the output format for the transformer. The methods include `set_output`. The method can be used to set the output format for the transformer.\n\nThe transformer has several methods that can be used to check if the provided functions are inverse of each other. The methods include `_check_inverse_transform`. The method can be used to check if the provided functions are inverse of each other.\n\nThe transformer has several methods that can be used to validate the input data. The methods include `_check_input`. The method can be used to validate the input data.\n\nThe transformer has several methods that can be used to get the feature names for the output data. The methods include `get_feature_names_out`. The method can be used to get the feature names for the output data.\n\nThe transformer has several methods that can be used to set the output format for the transformer. The methods include `set_output`. The method can be used to set the output format for the transformer.\n\nThe transformer has several methods that", "This class implements a Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm. It is a subclass of the BaseMixture class and inherits its methods. The GMM is a probabilistic model that assumes that the data is a mixture of K Gaussian distributions. The model is trained using the EM algorithm, which iteratively updates the parameters of the model to maximize the likelihood of the data.\n\nThe class has several methods for initializing and updating the parameters of the model, as well as for computing the log probability of the data given the model and the log weights of the components. It also has methods for computing the BIC and AIC scores of the model.\n\nThe class has several parameters that can be set when initializing the model, including the number of components, the type of covariance matrix, the initial weights, means, and precisions, and the random state. The class also has several methods for checking and setting the parameters of the model.\n\nThe class is designed to be used with a dataset, and it assumes that the dataset is a numpy array of shape (n_samples, n_features). The class can be used to fit the model to the data using the fit method, and it can be used to compute the log probability of the data given the model using the score method.", "\"\"\"\nGlobalAveragePooling1D is a layer that performs global average pooling over the time axis of 1D inputs.\n\n    Args:\n        data_format: A string, one of \"channels_last\" or \"channels_first\". The ordering of the dimensions in the inputs. \"channels_last\" is default, i.e. shape `(batch, steps, features)`. \"channels_first\" can be specified if the inputs are in the shape `(batch, features, steps)`.\n        **kwargs: Additional keyword arguments passed to the parent class.\n\n    Returns:\n        A tensor with the same shape as the input, except for the time axis which is reduced to 1.\n\n    Raises:\n        ValueError: If the input is not a 3D tensor.\n\"\"\"", "Performs global average pooling on the input.\n\nThis method applies global average pooling to the input data. If the data format is 'channels_last', it averages along the spatial dimensions (height and width). If the data format is 'channels_first', it averages along the spatial dimensions (width and height).", "Call the global average pooling operation on 3D input data. \n\nThis method applies the global average pooling operation to the input data. The operation is applied along the spatial dimensions (height, width, depth) of the input data. The axis along which the mean is calculated depends on the data format. If the data format is 'channels_last', the mean is calculated along the height, width, and depth axes. If the data format is 'channels_first', the mean is calculated along the depth, height, and width axes. The keepdims parameter determines whether the output should have the same number of dimensions as the input.", "Call the global max pooling operation on the input. This is a layer class in Keras.", "Call the GlobalMaxPooling2D layer on a input tensor.\n\nThis method applies the global max pooling operation to the input tensor.\n\nParameters:\n    inputs: Input tensor.\n\nReturns:\n    A tensor with the same number of channels as the input tensor, but with the spatial dimensions reduced to 1.", "Call the global max pooling operation on the input tensor. \n\nThis method applies the global max pooling operation along the specified axes of the input tensor. The data_format parameter determines the axes along which the pooling operation is applied. If data_format is 'channels_last', the pooling operation is applied along the spatial axes (height, width) and the channel axis. If data_format is 'channels_first', the pooling operation is applied along the spatial axes (height, width) and the batch axis.", "This is a docstring for the GlobalPooling1D class:\n\nA 1D global pooling layer. This layer performs a global pooling operation over the time dimension of the input data.", "A Keras layer for global pooling in 2D data.\n\nThis layer performs a global pooling operation over the spatial dimensions of the input data. It can be used to reduce the spatial dimensions of the input data, which can be useful for reducing the number of parameters in a model or for improving the performance of a model on certain types of data.\n\nThe layer supports two types of pooling: average pooling and max pooling. The type of pooling used can be specified using the `pooling` argument.\n\nThe layer also supports the `data_format` argument, which specifies the format of the input data. The default value is 'channels_last', which is the format used by most deep learning frameworks.\n\nThe layer can be used as a standalone layer or as part of a larger model.", "This is a 3D global pooling layer. It reduces the spatial dimensions of the input data by taking the maximum or average value across the spatial dimensions. The layer supports both 'channels_last' and 'channels_first' data formats.", "\"\"\"\nGroupTimeSeriesSplit is a class for splitting a time series dataset into training and testing sets. It is designed to work with grouped data, where each group is a separate time series. The class allows for rolling or expanding window splits, and can handle varying group lengths.\n\nThe class has the following parameters:\n- test_size: The size of the test set.\n- train_size: The size of the training set. If not specified, it will be calculated based on the number of groups, test size, gap size, and shift size.\n- n_splits: The number of splits. If not specified, it will be calculated based on the number of groups, train size, gap size, and shift size.\n- gap_size: The size of the gap between the end of the training set and the start of the test set.\n- shift_size: The size of the shift between the start of each split.\n- window_type: The type of window to use for the split. Can be either \"rolling\" or \"expanding\".\n\nThe class has two methods:\n- split: Yields training and testing sets for each split.\n- get_n_splits: Returns the number of splits.\n\nThe class can be used as follows:\n- Create an instance of the class with the desired parameters.\n- Call the split method to get the training and testing sets for each split.\n- Call the get_n_splits method to get the number of splits.", "\"\"\"\nKmeans class is a clustering algorithm that partitions the data into k clusters based on the similarity of the data points. It is an iterative algorithm that starts with an initial set of centroids and iteratively updates the centroids until convergence.\n\nParameters:\n    k (int): The number of clusters to form.\n    max_iter (int): The maximum number of iterations to perform.\n    convergence_tolerance (float): The tolerance for convergence.\n    random_seed (int): The random seed for the initialization of centroids.\n    print_progress (int): The level of progress to print.\n\nAttributes:\n    k (int): The number of clusters.\n    max_iter (int): The maximum number of iterations.\n    convergence_tolerance (float): The tolerance for convergence.\n    random_seed (int): The random seed for the initialization of centroids.\n    print_progress (int): The level of progress to print.\n    centroids_ (numpy array): The centroids of the clusters.\n    clusters_ (dict): The clusters of the data points.\n    iterations_ (int): The number of iterations performed.\n    _is_fitted (bool): Whether the model is fitted.\n\nMethods:\n    _fit(X, init_params=True): Fits the model to the data.\n    _get_cluster_idx(X, centroids): Returns the cluster index for each data point.\n    _predict(X): Predicts the cluster index for each data point.", "This class is a Label Binarizer, a transformer that converts a multiclass or multilabel target array into a binary target array. It is used to transform the target variable into a binary format, where each class is represented by a binary value (0 or 1). This is useful for models that require binary targets, such as logistic regression or decision trees.", "This class is a LabelEncoder, a transformer that converts categorical labels into numerical labels. It is a subclass of `TransformerMixin` and `BaseEstimator`. \n\nThe `fit` method is used to fit the LabelEncoder to the data. It calculates the unique classes in the data and stores them in the `classes_` attribute. \n\nThe `fit_transform` method is used to fit the LabelEncoder to the data and then transform the data. It calculates the unique classes in the data, transforms the data, and returns the transformed data.\n\nThe `transform` method is used to transform the data. It checks if the LabelEncoder has been fitted, converts the data to a 1D array, and then transforms the data.\n\nThe `inverse_transform` method is used to transform the data back to its original form. It checks if the LabelEncoder has been fitted, converts the data to a 1D array, and then transforms the data back.\n\nThe `_more_tags` method is used to specify the types of data that the LabelEncoder can handle. In this case, it can handle 1D categorical labels.", "\"\"\"\nLinear Regression\n\nThis class implements a linear regression model. It can be used for both direct and iterative methods to fit the model. The model can be trained using different methods such as stochastic gradient descent (SGD), normal equation, QR decomposition, and singular value decomposition (SVD).\n\nParameters\n----------\n\nmethod : str, optional (default='direct')\n    The method to use for training the model. Can be 'direct','sgd', 'qr', or'svd'.\n\neta : float, optional (default=0.01)\n    The learning rate for the SGD method.\n\nepochs : int, optional (default=50)\n    The number of epochs for the SGD method.\n\nminibatches : int, optional\n    The number of minibatches for the SGD method.\n\nrandom_seed : int, optional\n    The random seed for the SGD method.\n\nprint_progress : int, optional (default=0)\n    The frequency of printing the progress during training.\n\nAttributes\n----------\n\nb_ : float\n    The bias term of the model.\n\nw_ : array_like\n    The weights of the model.\n\ncost_ : list\n    The cost at each epoch during training.\n\nMethods\n----------\n\n_fit(X, y, init_params=True)\n    Fits the model to the given data.\n\n_predict(X)\n    Makes predictions using the trained model.\n\n_sum_squared_error_cost(y, y_val)\n    Calculates the sum of squared errors between the given and predicted values.\n\"\"\"", "\"\"\"\nLogistic Regression model.\n\nThis class implements a logistic regression model, which is a type of linear model that is used for binary classification problems. It is a simple and effective model that is widely used in many applications.\n\nThe model has several parameters that can be adjusted, including the learning rate (eta), the number of epochs, the L2 regularization strength (l2_lambda), the number of minibatches, and the random seed. The model also has a method for fitting the data, which involves iteratively updating the model's parameters to minimize the loss function. The model also has a method for making predictions, which involves applying the model's parameters to the input data.\n\nThe model uses the sigmoid activation function to compute the output probabilities. The loss function used is the log loss, which is a common choice for binary classification problems.\n\nThe model can be used for both training and testing. During training, the model will adjust its parameters to minimize the loss function. During testing, the model will make predictions based on the input data.\n\nAttributes:\n    eta: The learning rate.\n    epochs: The number of epochs.\n    l2_lambda: The L2 regularization strength.\n    minibatches: The number of minibatches.\n    random_seed: The random seed.\n    print_progress: Whether to print the progress.\n\nMethods:\n    _forward: Computes the output probabilities.\n    _backward: Computes the gradients of the loss function with respect to the model's parameters.\n    _fit: Fits the model to the data.\n    _predict: Makes predictions based on the input data.\n    _net_input: Computes the input to the sigmoid activation function.\n    _logit_cost: Computes the log loss.\n    _sigmoid_activation: Computes the sigmoid activation function.\n\"\"\"", "\"\"\"\nBase class for Keras losses.\n\nThis class is used to define custom loss functions for Keras models. It provides a basic structure for loss functions and allows for easy configuration and serialization.\n\n    Args:\n        name: Optional name for the loss function.\n        reduction: Optional reduction method for the loss function. Can be'sum_over_batch_size','sum','mean', or 'none'.\n        dtype: Optional data type for the loss function. Can be a numpy dtype or a string representing a numpy dtype.\n\n    Methods:\n        __call__(y_true, y_pred, sample_weight=None): Evaluates the loss function on the given inputs.\n        call(y_true, y_pred): Should be implemented by subclasses to compute the loss.\n        get_config(): Returns a dictionary of the loss function's configuration.\n        from_config(config): Creates a loss function from a configuration dictionary.\n\n    Properties:\n        dtype: The data type of the loss function.\n\"\"\"", "Initializes a MaxPooling1D layer.\n\nThis layer applies a 1D max pooling operation to the input. It reduces spatial dimensions using a sliding window approach. \n\nThe size of the sliding window is specified by pool_size. The stride of the sliding window is specified by strides. If strides is not provided, it defaults to pool_size. \n\nThe padding mode is specified by padding. If padding is 'valid', the output will be smaller than the input. If padding is'same', the output will be the same size as the input. \n\nThe data_format is specified by data_format. If data_format is 'channels_last', the input is expected to be in the format (batch_size, steps, channels). If data_format is 'channels_first', the input is expected to be in the format (batch_size, channels, steps).", "Initializes a MaxPooling2D layer.\n\nThis class is a subclass of Pooling2D and uses the max pooling algorithm to downsample the input data.\n\nParameters:\n    pool_size (tuple): The size of the pooling window (default is (2, 2)).\n    strides (tuple): The stride of the pooling window (default is None, which means the stride is equal to the pool size).\n    padding (str): The padding mode (default is 'valid').\n    data_format (str): The data format (default is None, which means the data format is 'channels_last').\n    **kwargs: Additional keyword arguments passed to the parent class.", "Initializes a MaxPooling3D layer.\n\nThis class is a subclass of Pooling3D and uses the max pooling algorithm to downsample the input data.\n\nParameters:\n    pool_size (tuple): The size of the pooling window.\n    strides (tuple): The stride of the pooling window.\n    padding (str): The type of padding to use. Can be 'valid' or'same'.\n    data_format (str): The format of the input data. Can be 'channels_last' or 'channels_first'.\n    **kwargs: Additional keyword arguments to pass to the parent class.", "\"\"\"\nBase class for Keras metrics.\n\nThis class provides a basic implementation of a Keras metric. It can be used as a base class for custom metrics.\n\nAttributes:\n    name: The name of the metric.\n    dtype: The data type of the metric.\n    variables: A list of variables used by the metric.\n    tracker: A tracker object that keeps track of the variables and metrics.\n\nMethods:\n    __init__: Initializes the metric with a name and a data type.\n    reset_state: Resets the state of the metric.\n    update_state: Updates the state of the metric.\n    stateless_update_state: Updates the state of the metric without tracking the variables.\n    result: Returns the result of the metric.\n    stateless_result: Returns the result of the metric without tracking the variables.\n    stateless_reset_state: Resets the state of the metric without tracking the variables.\n    add_variable: Adds a variable to the metric.\n    add_weight: Adds a weight to the metric.\n    variables: Returns a list of variables used by the metric.\n    __call__: Updates the state of the metric and returns the result.\n    get_config: Returns the configuration of the metric.\n    from_config: Creates a metric from a configuration.\n    __setattr__: Sets an attribute of the metric.\n    _check_super_called: Checks if the super class has been called.\n    __repr__: Returns a string representation of the metric.\n    __str__: Returns a string representation of the metric.\n\"\"\"", "\"\"\"\nBinarize labels in a multi-label classification problem.\n\nThis class is used to transform a multi-label classification problem into a binary classification problem. It is often used in conjunction with a classifier that can handle multi-class classification, such as a one-vs-all classifier.\n\nParameters\n----------\nclasses : array-like, default=None\n    The classes to use for binarization. If None, the unique labels in y are used.\n\nsparse_output : boolean, default=False\n    Whether to output a sparse matrix or not.\n\nAttributes\n----------\nclasses_ : array-like\n    The classes used for binarization.\n\nMethods\n-------\nfit(y) : fit the model to the data\nfit_transform(y) : fit the model to the data and transform the labels\ntransform(y) : transform the labels\ninverse_transform(yt) : transform the binary labels back to the original labels\n\nNotes\n-----\nThe classes argument contains duplicate classes. Remove these duplicates before passing them to MultiLabelBinarizer.\nThe classes argument contains unknown class(es). These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are not in the original labels. These will be ignored.\nThe classes argument contains class(es) that are", ".. OneHotEncoder\n\nThis class is a one-hot encoder that transforms categorical data into a binary matrix. It is a part of the scikit-learn library.\n\nThe class has several parameters that can be set during initialization. These include:\n\n- `categories`: A list of categories for each feature. If set to 'auto', the categories will be inferred from the training data.\n- `drop`: A list of categories to drop from each feature. If set to 'first', the first category will be dropped. If set to 'if_binary', the category with the least frequency will be dropped if the feature is binary.\n- `sparse_output`: A boolean indicating whether the output should be sparse or not.\n- `dtype`: The data type of the output.\n- `handle_unknown`: A string indicating how to handle unknown categories. Options include 'error', 'ignore', and 'infrequent_if_exist'.\n- `min_frequency`: An integer indicating the minimum frequency of a category to be considered frequent.\n- `max_categories`: An integer indicating the maximum number of categories to be considered.\n- `feature_name_combiner`: A function or string indicating how to combine the feature name with the category name.\n\nThe class has several methods, including:\n\n- `fit`: Fits the encoder to the training data.\n- `transform`: Transforms the input data into a binary matrix.\n- `inverse_transform`: Inverts the transformation and converts the binary matrix back into the original categorical data.\n- `get_feature_names_out`: Returns the names of the output features.\n\nThe class also has several internal methods that are used to implement the encoding and decoding processes. These include `_map_drop_idx_to_infrequent`, `_set_drop_idx`, `_compute_transformed_categories`, `_remove_dropped_categories`, `_compute_n_features_outs`, `_transform`, and `_inverse_transform`.", "\"\"\"\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that can be used for both clustering and outlier detection. It is an extension of the DBSCAN algorithm and can handle varying densities in the data.\n\nThe algorithm works by first computing a reachability graph, which is a graph where each point is connected to its k-nearest neighbors. Then, it identifies clusters by traversing the graph and grouping points that are densely connected.\n\nThe algorithm has several parameters that can be tuned to control its behavior. These include the minimum number of samples required to form a dense region (min_samples), the maximum distance between two samples for them to be considered as in the same neighborhood (max_eps), the metric used to compute the distance between samples (metric), and the power parameter for the Minkowski metric (p).\n\nThe algorithm also has two clustering methods: 'xi' and 'dbscan'. The 'xi' method is used for clustering and outlier detection, while the 'dbscan' method is used for clustering only.\n\nThe algorithm can be used for both clustering and outlier detection. For clustering, it returns the labels of the clusters. For outlier detection, it returns the labels of the outliers.\n\nParameters\n----------\n\nmin_samples : int, optional (default=5)\n    The minimum number of samples required to form a dense region.\n\nmax_eps : float, optional (default=np.inf)\n    The maximum distance between two samples for them to be considered as in the same neighborhood.\n\nmetric : str or callable, optional (default='minkowski')\n    The metric used to compute the distance between samples. Can be'minkowski', 'euclidean','manhattan', or a callable.\n\np : float, optional (default=2)\n    The power parameter for the Minkowski metric.\n\nmetric_params : dict or None, optional (default=None)\n    The parameters for the metric function.\n\ncluster_method : str, optional (default='xi')\n    The clustering method to use. Can be 'xi' or 'dbscan'.\n\neps : float, optional (default=None)\n    The maximum distance between two samples for them to be considered as in the same neighborhood. Only used if cluster_method is 'dbscan'.\n\nxi : float, optional (default=0.05)\n    The parameter for the 'xi' clustering method.\n\npredecessor_correction : bool, optional (default=True)\n    Whether to use the predecessor correction method.\n\nmin_cluster_size : int or None, optional (default=None)\n    The minimum size of the clusters.\n\nalgorithm : str, optional (default='auto')\n    The algorithm to use. Can be 'auto', 'brute', 'ball_tree', or 'kd_tree'.\n\nleaf_size : int, optional (default=30)\n    The leaf size of the ball tree or kd tree.\n\nmemory : str or None, optional (default=None)\n    The memory to use. Can be 'auto', 'cache', or None.\n\nn_jobs : int or None, optional (default=None)\n    The number of jobs to run in parallel.\n\nReturns\n-------\n\nself : OPTICS\n    The fitted OPTICS object.", ".. \n\nOrdinalEncoder\n\nOrdinalEncoder is a class that performs ordinal encoding on categorical data. It maps categorical values to consecutive integers. The encoder can handle missing values and unknown categories. It also supports encoding of infrequent categories.\n\nParameters\n----------\n\n*   `categories`: A list of categories for each feature. It can be 'auto' for automatic detection of categories. Default is 'auto'.\n*   `dtype`: The data type of the output. Default is `np.float64`.\n*   `handle_unknown`: The strategy to handle unknown categories. It can be 'error' or 'use_encoded_value'. Default is 'error'.\n*   `unknown_value`: The value to use for unknown categories when `handle_unknown` is 'use_encoded_value'. Default is `np.nan`.\n*   `encoded_missing_value`: The value to use for missing values. Default is `np.nan`.\n*   `min_frequency`: The minimum frequency of categories to be considered. Default is `None`.\n*   `max_categories`: The maximum number of categories to be considered. Default is `None`.\n\nMethods\n----------\n\n*   `fit(X, y=None)`: Fits the encoder to the data.\n*   `transform(X)`: Transforms the data using the fitted encoder.\n*   `inverse_transform(X)`: Inverse transforms the data using the fitted encoder.\n\nNotes\n-----\n\n*   When `handle_unknown` is 'use_encoded_value', the `unknown_value` should be an integer or `np.nan`.\n*   When `handle_unknown` is 'error', the `unknown_value` should be `None`.\n*   The `encoded_missing_value` should be a value that is not used for encoding any categories.\n*   The `min_frequency` and `max_categories` parameters are used to filter out infrequent categories.", "\"\"\"\nPooling1D class is a 1D pooling layer. It applies a pooling function to the input data. The pooling function can be either 'avg' for average pooling or'max' for max pooling. The pool size and strides can be specified. The padding can be either 'valid' or'same'. The data format can be either 'channels_last' or 'channels_first'. The input data should be a 3D tensor.\n\n__init__ method initializes the pooling layer with the specified parameters.\n\ncall method applies the pooling function to the input data.\n\ncompute_output_shape method calculates the output shape of the pooling layer.\n\nget_config method returns the configuration of the pooling layer.\n\"\"\"", "\"\"\"\nPooling2D layer.\n\nThis layer applies a 2D pooling operation to the input.\n\n    Args:\n        pool_function: The pooling function to use. Can be'max' or 'avg'.\n        pool_size: The size of the pooling window.\n        strides: The stride of the pooling window.\n        padding: The padding mode to use. Can be 'valid','same', or 'causal'.\n        data_format: The data format of the input. Can be 'channels_last' or 'channels_first'.\n        name: The name of the layer.\n        **kwargs: Additional layer arguments.\n\n    Returns:\n        A 2D pooling operation.\n\n    Methods:\n        call: Applies the pooling operation to the input.\n        compute_output_shape: Computes the output shape of the pooling operation.\n        get_config: Returns the configuration of the layer.\n\"\"\"", "\"\"\"\nPooling3D layer for 3D data.\n\nThis layer applies a 3D pooling operation to the input data.\n\n    Args:\n        pool_function: The pooling function to use. Can be'max' or 'avg'.\n        pool_size: The size of the pooling window.\n        strides: The stride of the pooling window.\n        padding: The padding mode to use. Can be 'valid','same', or 'causal'.\n        data_format: The format of the input data. Can be 'channels_last' or 'channels_first'.\n        name: The name of the layer.\n        **kwargs: Additional layer arguments.\n\n    Returns:\n        A 3D tensor representing the output of the pooling operation.\n\n    Compute Output Shape:\n        The output shape is computed based on the input shape, pool size, strides, and padding.\n\n    Methods:\n        call: Applies the pooling operation to the input data.\n        compute_output_shape: Computes the output shape of the layer.\n        get_config: Returns the configuration of the layer.\n\"\"\"", "\"\"\"\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a new set of features that are uncorrelated and have maximum variance. This class implements a Principal Component Analysis model.\n\nThe model can be initialized with the number of components to retain, the solver to use (either'svd' or 'eigen'), and whether to perform whitening. The fit method is used to fit the model to the data, and the transform method is used to transform new data into the new feature space.\n\nThe model can be used to reduce the dimensionality of the data, which can be useful for reducing the complexity of the data and improving the performance of machine learning algorithms.\n\nParameters:\n    n_components: The number of components to retain. If None, all components are retained.\n    solver: The solver to use. Can be either'svd' or 'eigen'.\n    whitening: Whether to perform whitening. If True, the transformed data is whitened.\n\nAttributes:\n    n_components: The number of components retained.\n    solver: The solver used.\n    whitening: Whether whitening was performed.\n    w_: The projection matrix.\n    e_vals_: The eigenvalues.\n    e_vecs_: The eigenvectors.\n    e_vals_normalized_: The normalized eigenvalues.\n    loadings_: The loadings.\n\nMethods:\n    fit(X, y=None): Fits the model to the data.\n    transform(X): Transforms new data into the new feature space.\n    _covariance_matrix(X): Computes the covariance matrix of the data.\n    _decomposition(mat, n_samples): Computes the decomposition of the matrix.\n    _loadings(): Computes the loadings.\n    _projection_matrix(eig_vals, eig_vecs, whitening, n_components): Computes the projection matrix.\n\"\"\"", ".. class:: RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs)\n\nThe RMSprop optimizer is a popular stochastic gradient descent optimizer that adapts the learning rate for each parameter individually based on the magnitude of the gradient.\n\nThe learning rate is calculated as the product of the initial learning rate, the decay rate, and the inverse square root of the running average of the squared gradient.\n\nThe optimizer also supports momentum, which can help escape local minima, and centered RMSprop, which can help with exploding gradients.\n\nThe optimizer's parameters are:\n\n* `learning_rate`: The initial learning rate.\n* `rho`: The decay rate.\n* `momentum`: The momentum.\n* `epsilon`: The epsilon value.\n* `centered`: Whether to use centered RMSprop.\n\nThe optimizer's hyperparameters are:\n\n* `decay`: The decay rate.\n* `rho`: The decay rate.\n* `momentum`: The momentum.\n* `epsilon`: The epsilon value.\n\nThe optimizer's weights are:\n\n* `weights`: The optimizer's weights.\n\nThe optimizer's config is:\n\n* `learning_rate`: The initial learning rate.\n* `decay`: The decay rate.\n* `rho`: The decay rate.\n* `momentum`: The momentum.\n* `epsilon`: The epsilon value.\n* `centered`: Whether to use centered RMSprop.", ".. |SelfTrainingClassifier| replace:: SelfTrainingClassifier\n\nSelfTrainingClassifier\n=====================\n\nA self-training classifier that iteratively labels new instances based on the predictions of a base estimator.\n\nParameters\n----------\n\n- `estimator`: The base estimator to use for prediction. If not provided, the `base_estimator` will be used.\n- `base_estimator`: The base estimator to use for prediction. This parameter is deprecated and will be removed in future versions. Use `estimator` instead.\n- `threshold`: The threshold for selecting new instances to label. Default is 0.75.\n- `criterion`: The criterion for selecting new instances to label. Can be either 'threshold' or 'k_best'. Default is 'threshold'.\n- `k_best`: The number of instances to select for labeling when using the 'k_best' criterion. Default is 10.\n- `max_iter`: The maximum number of iterations. Default is 10.\n- `verbose`: Whether to print information about the training process. Default is False.\n\nMethods\n----------\n\n- `fit(X, y, **params)`: Fit the classifier to the data.\n- `predict(X, **params)`: Predict the labels for the instances in X.\n- `predict_proba(X, **params)`: Predict the probabilities of the instances in X.\n- `decision_function(X, **params)`: Predict the decision function for the instances in X.\n- `predict_log_proba(X, **params)`: Predict the log probabilities of the instances in X.\n- `score(X, y, **params)`: Score the classifier on the data.\n\nNotes\n-----\n\n- This classifier is designed to work with instances that have a label and instances that do not have a label.\n- The `base_estimator` parameter is deprecated and will be removed in future versions. Use `estimator` instead.\n- The `threshold` and `k_best` parameters are used to select new instances to label. The `threshold` parameter is used when the 'threshold' criterion is used, and the `k_best` parameter is used when the 'k_best' criterion is used.\n- The `max_iter` parameter is used to limit the number of iterations. If the maximum number of iterations is reached, the classifier will stop training.\n- The `verbose` parameter is used to print information about the training process.", "\"\"\"\nInitializes a separable convolution layer (also known as Inception layer).\n    \n    Args:\n        rank: Integer, the rank of the convolution layer.\n        filters: Integer, the number of filters in the convolution layer.\n        kernel_size: Integer or tuple of 2 integers, the size of the depthwise and pointwise convolution kernels.\n        strides: Integer or tuple of 2 integers, the strides of the convolution layer.\n        padding: String, the padding mode to use.\n        data_format: String, the format of the input data.\n        dilation_rate: Integer or tuple of 2 integers, the dilation rate of the convolution layer.\n        depth_multiplier: Integer, the number of depthwise convolution filters.\n        activation: Activation function to use.\n        use_bias: Boolean, whether to add a bias term.\n        depthwise_initializer: Initializer for the depthwise kernel weights.\n        pointwise_initializer: Initializer for the pointwise kernel weights.\n        bias_initializer: Initializer for the bias weights.\n        depthwise_regularizer: Regularizer function applied to the depthwise kernel weights.\n        pointwise_regularizer: Regularizer function applied to the pointwise kernel weights.\n        bias_regularizer: Regularizer function applied to the bias weights.\n        activity_regularizer: Regularizer function applied to the output of the layer.\n        depthwise_constraint: Constraint function applied to the depthwise kernel weights.\n        pointwise_constraint: Constraint function applied to the pointwise kernel weights.\n        bias_constraint: Constraint function applied to the bias weights.\n        trainable: Boolean, whether the layer is trainable.\n        name: String, the name of the layer.\n\"\"\"", "\"\"\"\nInitializes a Separable Conv1D layer.\n\nArgs:\n    filters: The number of filters in the convolutional layer.\n    kernel_size: An integer or tuple/list of 1 integers, specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of 1 integers, specifying the stride length of the convolution.\n    padding: A string, either 'valid' or 'causal'. Default is 'valid'.\n    data_format: A string, either 'channels_last' or 'channels_first'. Default is 'channels_last'.\n    dilation_rate: An integer or tuple/list of 1 integers, specifying the dilation rate to use for dilated convolution.\n    depth_multiplier: An integer, specifying the number of depthwise convolution output channels for each input channel.\n    activation: A string, the activation function to use.\n    use_bias: A boolean, whether to add a bias term.\n    depthwise_initializer: A string, the initializer for the depthwise kernel.\n    pointwise_initializer: A string, the initializer for the pointwise kernel.\n    bias_initializer: A string, the initializer for the bias.\n    depthwise_regularizer: A string, the regularizer for the depthwise kernel.\n    pointwise_regularizer: A string, the regularizer for the pointwise kernel.\n    bias_regularizer: A string, the regularizer for the bias.\n    activity_regularizer: A string, the activity regularizer.\n    depthwise_constraint: A string, the constraint for the depthwise kernel.\n    pointwise_constraint: A string, the constraint for the pointwise kernel.\n    bias_constraint: A string, the constraint for the bias.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    The output of the convolutional layer.\n\"\"\"", "\"\"\"\nA separable convolutional 2D layer (also known as depthwise separable convolution).\n\nThis layer is a variant of the standard convolutional 2D layer, where the depthwise and pointwise convolutions are separated.\n\nArgs:\n    filters: The number of filters in the convolutional layer.\n    kernel_size: An integer or tuple of 2 integers, specifying the height and width of the 2D convolution window.\n    strides: An integer or tuple of 2 integers, specifying the strides of the convolution along the height and width.\n    padding: A string, specifying the padding scheme to use. Can be 'valid','same', or 'causal'.\n    data_format: A string, specifying the data format of the input. Can be 'channels_first' or 'channels_last'.\n    dilation_rate: An integer or tuple of 2 integers, specifying the dilation rate to use for the convolution.\n    depth_multiplier: An integer, specifying the number of depthwise convolutional filters to use.\n    activation: A string, specifying the activation function to use. Can be'relu','sigmoid', 'tanh', or'softmax'.\n    use_bias: A boolean, specifying whether to use a bias term in the convolutional layer.\n    depthwise_initializer: A string, specifying the initializer to use for the depthwise convolutional filters.\n    pointwise_initializer: A string, specifying the initializer to use for the pointwise convolutional filters.\n    bias_initializer: A string, specifying the initializer to use for the bias term.\n    depthwise_regularizer: A string, specifying the regularizer to use for the depthwise convolutional filters.\n    pointwise_regularizer: A string, specifying the regularizer to use for the pointwise convolutional filters.\n    bias_regularizer: A string, specifying the regularizer to use for the bias term.\n    activity_regularizer: A string, specifying the regularizer to use for the output of the convolutional layer.\n    depthwise_constraint: A string, specifying the constraint to use for the depthwise convolutional filters.\n    pointwise_constraint: A string, specifying the constraint to use for the pointwise convolutional filters.\n    bias_constraint: A string, specifying the constraint to use for the bias term.\n\nReturns:\n    A 3D tensor, representing the output of the convolutional layer.", ".. class:: SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)\n\nSequential Feature Selector.\n\nThis class is a wrapper around a given estimator that allows for feature selection based on a scoring function. It iteratively adds the most informative features to the model until a stopping criterion is reached.\n\nParameters:\nestimator : object\n    The estimator to be used for feature selection.\n\nk_features : int or tuple, optional (default=1)\n    The number of features to select. If an integer, it is the number of features to select. If a tuple, it is a range of features to select.\n\nforward : bool, optional (default=True)\n    Whether to select features in a forward or backward manner.\n\nfloating : bool, optional (default=False)\n    Whether to allow features to be added or removed during the selection process.\n\nverbose : int, optional (default=0)\n    The verbosity level.\n\nscoring : str or callable, optional (default=None)\n    The scoring function to use for feature selection.\n\ncv : int, optional (default=5)\n    The number of folds for cross-validation.\n\nn_jobs : int, optional (default=1)\n    The number of jobs to run in parallel.\n\npre_dispatch : str or int, optional (default='2*n_jobs')\n    The number of jobs to pre_dispatch.\n\nclone_estimator : bool, optional (default=True)\n    Whether to clone the estimator.\n\nfixed_features : list or tuple, optional (default=None)\n    The features to fix during the selection process.\n\nfeature_groups : list or tuple, optional (default=None)\n    The groups of features to consider during the selection process.\n\nAttributes:\nestimator_ : object\n    The estimator used for feature selection.\n\nnamed_estimators_ : list\n    The named estimators.\n\nsubsets_ : dict\n    The subsets of features selected during the feature selection process.\n\nk_feature_idx_ : tuple\n    The indices of the selected features.\n\nk_score_ : float\n    The score of the selected features.\n\nk_feature_names_ : list\n    The names of the selected features.\n\nfitted : bool\n    Whether the feature selector has been fitted.\n\ninterrupted_ : bool\n    Whether the feature selector was interrupted.\n\nTESTING_INTERRUPT_MODE : bool\n    Whether to test the interrupt mode.\n\nMethods:\n__init__(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)\n    Initialize the Sequential Feature Selector.\n\nnamed_estimators(self)\n    Return the named estimators.\n\nget_params(self, deep=True)\n    Get the parameters of the Sequential Feature Selector.\n\nset_params(self, **params)\n    Set the parameters of the Sequential Feature Selector.\n\nfit(self, X, y, groups=None, **fit_params)\n    Fit the Sequential Feature Selector.\n\ntransform(self, X)\n    Transform the input data using the selected features.\n\nfit_transform(self, X, y, groups=None, **fit_params)\n    Fit and transform the input data.\n\nget_metric_dict(self, confidence_interval=0.95)\n    Get the metric dictionary.\n\n_finalize_fit(self)\n    Finalize the fit.\n\n_check_fitted(self)\n    Check if the feature selector has been fitted.", ".. class:: SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs)\n\nStochastic Gradient Descent (SGD) optimizer.\n\nThis optimizer minimizes a loss function using the stochastic gradient descent algorithm.\n\n    Args:\n        learning_rate: The learning rate to use. Defaults to 0.01.\n        momentum: The momentum to use. Defaults to 0.0.\n        nesterov: Whether to use Nesterov momentum. Defaults to False.\n        name: The name of the optimizer. Defaults to 'SGD'.\n        **kwargs: Additional keyword arguments passed to the optimizer.\n\n    Methods:\n        _create_slots: Creates slots for the optimizer.\n        _prepare_local: Prepares local variables for the optimizer.\n        _resource_apply_dense: Applies the optimizer to a dense variable.\n        _resource_apply_sparse_duplicate_indices: Applies the optimizer to a sparse variable with duplicate indices.\n        _resource_apply_sparse: Applies the optimizer to a sparse variable.\n        get_config: Returns the configuration of the optimizer.", "\"\"\"\nSoftmax Regression\n\nThis class implements a Softmax Regression model, a type of multi-class classification model.\n\nAttributes:\n    eta (float): The learning rate.\n    epochs (int): The number of epochs to train the model.\n    l2 (float): The L2 regularization strength.\n    minibatches (int): The number of minibatches to use during training.\n    n_classes (int): The number of classes in the target variable.\n    random_seed (int): The random seed for reproducibility.\n    print_progress (int): Whether to print the training progress.\n\nMethods:\n    _net_input: Computes the net input to the softmax activation function.\n    _softmax_activation: Computes the softmax activation function.\n    _cross_entropy: Computes the cross-entropy loss.\n    _cost: Computes the cost function.\n    _to_classlabels: Converts the output of the softmax function to class labels.\n    _forward: Computes the output of the model.\n    _backward: Computes the gradients of the loss with respect to the weights and bias.\n    _fit: Trains the model.\n    predict_proba: Predicts the probabilities of the classes.\n    _predict: Predicts the class labels.\n\"\"\"", ".. _TargetEncoder:\n\nTargetEncoder is a class for encoding categorical variables using a target variable. It is a type of one-to-one feature mixer that can handle both categorical and numerical data. The target variable is used to determine the encoding strategy, which can be 'auto', 'continuous', 'binary', or'multiclass'.\n\n    Parameters\n    ----------\n    categories : {'auto', list}, default='auto'\n        The categories to use for encoding. If 'auto', the categories will be inferred from the data.\n    target_type : {'auto', 'continuous', 'binary','multiclass'}, default='auto'\n        The type of target variable. If 'auto', the type will be inferred from the data.\n    smooth : {'auto', interval}, default='auto'\n        The smoothing parameter for encoding. If 'auto', the smoothing will be determined automatically.\n    cv : int, default=5\n        The number of folds for cross-validation.\n    shuffle : bool, default=True\n        Whether to shuffle the data before encoding.\n    random_state : int, default=None\n        The random state for shuffling and encoding.\n\n    Methods\n    -------\n    fit(X, y)\n        Fits the encoder to the data.\n    fit_transform(X, y)\n        Fits the encoder to the data and returns the encoded data.\n    transform(X)\n        Transforms the data using the fitted encoder.\n\n    Attributes\n    ----------\n    categories_ : list\n        The categories used for encoding.\n    target_type_ : {'auto', 'continuous', 'binary','multiclass'}\n        The type of target variable.\n    smooth_ : {'auto', interval}\n        The smoothing parameter for encoding.\n    cv_ : int\n        The number of folds for cross-validation.\n    shuffle_ : bool\n        Whether to shuffle the data before encoding.\n    random_state_ : int\n        The random state for shuffling and encoding.\n    classes_ : list\n        The classes for multiclass encoding.\n    target_mean_ : array\n        The mean of the target variable.\n    encodings_ : array\n        The encodings for the data.\n\n    Notes\n    -----\n    TargetEncoder is a type of one-to-one feature mixer that can handle both categorical and numerical data. It is designed to work with scikit-learn's preprocessing module.", "\"\"\"\nTransforms a list of transactions into a binary matrix where each row represents a transaction and each column represents a unique item.\nThe `fit` method is used to determine the unique items in the transactions and create a mapping from items to column indices.\nThe `transform` method is used to create the binary matrix. If `sparse` is True, a sparse matrix is returned; otherwise, a dense matrix is returned.\nThe `inverse_transform` method is used to transform the binary matrix back into a list of transactions.\nThe `fit_transform` method is a convenience method that fits the model and then transforms the data.\nThe `get_feature_names_out` method returns the feature names for the output of the transformer.", "\"\"\"\nUpsampling1D layer class.\n\nThis class is used for upsampling 1D data. It repeats the input data along the specified axis.\n\nArgs:\n    size: The size of upsampling. Default is 2.\n\nReturns:\n    A tensor with the upsampled data.\n\nExample:\n    >>> upsampling = UpSampling1D(size=3)\n    >>> output = upsampling(input_data)\n\"\"\"", "\"\"\"\nUpsampling2D layer class.\n\nThis class is used to upsample 2D data. It can be used to increase the spatial dimensions of an image.\n\n    Args:\n        size: A tuple of two integers. The upsampling factor for the height and width.\n        data_format: A string. The format of the input data. It can be either 'channels_first' or 'channels_last'.\n        interpolation: A string. The interpolation method to use. It can be either 'nearest' or 'bilinear'.\n\n    Methods:\n        __init__: Initializes the UpSampling2D layer.\n        compute_output_shape: Computes the output shape of the layer.\n        call: Applies the upsample operation to the input data.\n        get_config: Returns the configuration of the layer.", "\"\"\"\nUpsampling3D Layer\n\nUpsamples 3D data.\n\nArgs:\n    size: Tuple of 3 integers. Factor by which to upsample.\n    data_format: String, 'channels_first' or 'channels_last'. Default: None.\n\nInput shape:\n    A 5D tensor.\n\nOutput shape:\n    A 5D tensor with the same batch size and number of channels as the input,\n    and the specified size in the spatial dimensions.\n\nExample:\n    >>> from keras.layers import UpSampling3D\n    >>> upsampling = UpSampling3D(size=(2, 2, 2))\n    >>> output = upsampling(input_tensor)\n\"\"\"", "\"\"\"\nZeroPadding1D(Layer)\n\nA layer that adds zeros to the input tensor.\n\n    Args:\n        padding (int or tuple): The number of zeros to add to the input tensor.\n            If a tuple, it should contain two values, one for the start and one for the end.\n            Default is 1.\n\n    Methods:\n        __init__: Initializes the layer with the given padding.\n        compute_output_shape: Computes the output shape of the layer.\n        call: Adds zeros to the input tensor.\n        get_config: Returns the configuration of the layer.\n\"\"\"", "\"\"\"\nZeroPadding2D\n\nZero padding layer for 2D data.\n\nThis layer can be used to add zero padding to the input data. The padding can be specified as a single integer, a tuple of two integers, or a tuple of two tuples of two integers.\n\nArgs:\n    padding: The padding to be added. It can be a single integer, a tuple of two integers, or a tuple of two tuples of two integers.\n    data_format: The format of the input data. It can be 'channels_first' or 'channels_last'.\n\nReturns:\n    A 4D tensor with the added padding.\n\nExample:\n    >>> from keras.layers import ZeroPadding2D\n    >>> from keras.layers import Input\n    >>> from keras.models import Model\n    >>> inputs = Input(shape=(28, 28, 1))\n    >>> x = ZeroPadding2D(padding=(2, 2))(inputs)\n    >>> model = Model(inputs=inputs, outputs=x)\n    >>> model.summary()\n    >>> model.predict(inputs)\n\"\"\"", "\"\"\"\nZeroPadding3D\n\nZero padding layer for 3D data.\n\nThis layer can add padding to the input data in the spatial dimensions (height, width, depth).\n\nArguments:\n    padding: A tuple of 3 integers or a tuple of 3 tuples of 2 integers. The padding to be added in the spatial dimensions.\n    data_format: A string, either 'channels_first' or 'channels_last'. The format of the input data.\n\nInput shape:\n    A 5D tensor.\n\nOutput shape:\n    A 5D tensor with the same batch size and number of channels as the input, but with the added padding in the spatial dimensions.\n\nMethods:\n    __init__: Initializes the layer with the given padding and data format.\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: Applies the padding to the input data.\n    get_config: Returns the configuration of the layer.\n\"\"\"", ".. _BaseEncoder:\n\nBase class for categorical encoding.\n\nThis class is a base class for categorical encoding. It provides a set of methods for encoding categorical data. The encoding methods include one-hot encoding, label encoding, and hash encoding.\n\nThe class has several methods:\n\n* `_check_X`: checks if the input data is a 2D array and if it contains categorical data.\n* `_fit`: fits the encoding model to the data.\n* `_transform`: transforms the data using the encoding model.\n* `_more_tags`: returns a dictionary of tags that describe the class.\n\nThe class also has several properties:\n\n* `n_features_in_`: the number of features in the input data.\n* `categories_`: the categories of the categorical data.\n* `infrequent_categories_`: the infrequent categories of the categorical data.\n\nThe class is designed to be used as a base class for other categorical encoding classes. It provides a set of methods that can be overridden by subclasses to implement different encoding algorithms."]