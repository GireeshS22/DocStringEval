["\"\"\"\nAdamax optimizer.\n\nAdamax is a variant of the Adam optimizer that uses the infinity norm instead of the variance for the adaptive learning rate.\nIt is similar to Adam but uses the exponentially weighted infinity norm for the second moment estimate.\n\nArgs:\n    learning_rate: A float hyperparameter, the learning rate.\n    beta_1: A float hyperparameter, the exponential decay rate for the first moment estimates (default: 0.9).\n    beta_2: A float hyperparameter, the exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon: A small constant for numerical stability (default: 1e-7).\n    name: Optional name for the operations created when applying gradients. Defaults to \"Adamax\" (default: 'Adamax').\n\nProperties:\n    _HAS_AGGREGATE_GRAD: A boolean indicating whether the gradient can be aggregated (default: True).\n\nMethods:\n    _create_slots: Creates slots for the variables.\n    _prepare_local: Prepares local variables for the optimizer.\n    _resource_apply_dense: Applies gradients to variables (dense case).\n    _resource_apply_sparse: Applies gradients to variables (sparse case).\n    get_config: Returns the configuration of the optimizer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nAgglomerationTransform is a transformer class that performs agglomerative transformation on input data.\n\nParameters\n----------\nNone\n\nAttributes\n----------\npooling_func : callable\n    Function used for pooling data.\nlabels_ : array-like\n    Labels assigned to each sample.\n\nMethods\n-------\ntransform(X)\n    Transform the input data X using the fitted agglomerative model.\n    - X : array-like of shape (n_samples, n_features)\n        Input data to be transformed.\n\ninverse_transform(X=None, *, Xt=None)\n    Inverse transform the input data X or Xt.\n    - X : array-like of shape (n_samples, n_features), default=None\n        Input data to be inverse transformed.\n    - Xt : array-like of shape (n_samples, n_features), default=None\n        Input data to be inverse transformed (deprecated, use X instead).\n    Returns\n    -------\n    inverse_transformed : array-like of shape (n_samples, n_features)\n        Inverse transformed data.\n\"\"\"", "\"\"\"\nAveragePooling1D(layer)\n\nOne-dimensional average pooling layer.\n\nParameters\n----------\npool_size : int, optional\n    Factor by which to downscale. The default is 2.\nstrides : int or None, optional\n    Strides of the pooling operation. If None, it will be equal to `pool_size`. The default is None.\npadding : {'valid', 'same'}, optional\n    Padding method. The default is 'valid'.\ndata_format : {'channels_last', 'channels_first'}, optional\n    Data format of the input. The default is 'channels_last'.\n\n\"\"\"", "\"\"\"\nAveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nApplies 2D average pooling over input signals.\n\nParameters:\n- pool_size: An integer or tuple/list of 2 integers, specifying the size of the pooling window. Default is (2, 2).\n- strides: An integer or tuple/list of 2 integers, specifying the strides of the pooling operation. Default is None, in which case it is set to the size of the pooling window.\n- padding: One of 'valid' or 'same' (case-insensitive). 'valid' means no padding. 'same' results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. Default is 'valid'.\n- data_format: A string, one of 'channels_last' (default) or 'channels_first'. It specifies the ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape (batch, height, width, channels) while 'channels_first' corresponds to inputs with shape (batch, channels, height, width). Default is None, which will be automatically inferred based on the input shape.\n- **kwargs: Additional keyword arguments to be passed to the base class.\n\"\"\"", "\"\"\"\nAveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nApplies 3D average pooling over input data.\n\nParameters:\n- pool_size (tuple): Size of the pooling window. Default is (2, 2, 2).\n- strides (tuple or None): Stride of the pooling window. If None, it will be equal to pool_size. Default is None.\n- padding (str): Type of padding. Can be 'valid' or 'same'. Default is 'valid'.\n- data_format (str or None): Data format of the input data. Can be 'channels_first' or 'channels_last'. If None, it will be inferred from the input. Default is None.\n- **kwargs: Additional keyword arguments to be passed to the base class.\n\"\"\"", "\"\"\"\nBayesian Gaussian Mixture Model\n\nThis class implements a Bayesian approach to Gaussian Mixture Models (GMMs). It allows for flexible modeling of complex data distributions by assuming that the data is generated from a mixture of Gaussian distributions, each with its own mean and covariance. The Bayesian framework is used to infer the posterior distribution over the model parameters, providing a principled way to handle uncertainty in the model.\n\nParameters\n----------\nn_components : int, default=1\n    The number of mixture components.\n\ncovariance_type : {'spherical', 'tied', 'diag', 'full'}, default='full'\n    The type of covariance parameters to use.\n\ntol : float, default=1e-3\n    The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.\n\nreg_covar : float, default=1e-6\n    Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.\n\nmax_iter : int, default=100\n    The maximum number of EM iterations to perform.\n\nn_init : int, default=1\n    The number of initializations to perform. The best results are kept.\n\ninit_params : {'kmeans', 'random'}, default='kmeans'\n    The method used to initialize the weights, means, and precisions.\n\nweight_concentration_prior_type : {'dirichlet_process', 'dirichlet_distribution'}, default='dirichlet_process'\n    The type of the weight concentration prior.\n\nweight_concentration_prior : float or None, default=None\n    The weight concentration parameter, which controls the sparsity of the weights. If None, it is inferred from the data.\n\nmean_precision_prior : float or None, default=None\n    The precision prior for the mean. If None, it is inferred from the data.\n\nmean_prior : array-like or None, default=None\n    The prior for the mean. If None, it is inferred from the data.\n\ndegrees_of_freedom_prior : float or None, default=None\n    The prior for the degrees of freedom. If None, it is inferred from the data.\n\ncovariance_prior : array-like or float or None, default=None\n    The prior for the covariance. If None, it is inferred from the data.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for weights initialization. Pass an int for reproducible results across multiple function calls.\n\nwarm_start : bool, default=False\n    If True, the previous solution will be used as initialization.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints the current initialization and iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.\n\nverbose_interval : int, default=10\n    Number of iteration done before the next print.\n\nAttributes\n----------\nweights_ : array, shape (n_components,)\n    The weights of each mixture components.\n\nmeans_ : array, shape (n_components, n_features)\n    The mean of each mixture component.\n\ncovariances_ : array, shape (n_components, n_features, n_features)\n    The covariance of each mixture component.\n\nprecisions_ : array, shape (n_components, n_features, n_features)\n    The precision matrices for each component.\n\nprecisions_cholesky_ : array, shape (n_components, n_features, n_features)\n    The cholesky decomposition of the precision matrices of each component.\n\nMethods\n-------\nfit(X)\n    Estimate model parameters with the EM algorithm.\n\npredict(X)\n    Predict the labels for the data samples in X.\n\npredict_log_proba(X)\n    Predict log probabilities of the data samples for each component in the model.\n\npredict_proba(X)\n    Predict posterior probability of each component for the data samples in X.\n\nscore(X)\n    Compute the log-likelihood of X under the model.\n\n\"\"\"", "\"\"\"\nA Convolutional Layer for performing convolution operations on input data.\n\nParameters:\n- rank: An integer, the rank of the convolution, e.g. 1 for 1D convolution.\n- filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n- kernel_size: An integer or tuple/list of n integers, specifying the size of the convolution window.\n- strides: An integer or tuple/list of n integers, specifying the strides of the convolution. Specifying any stride value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n- padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"causal\"` is supported for 1D convolutions only.\n- data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, ..., channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, ...)`.\n- dilation_rate: An integer or tuple/list of n integers, specifying the dilation rate to use for dilated convolution.\n- groups: A positive integer specifying the number of groups in a group convolution.\n- activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n- use_bias: Boolean, whether the layer uses a bias vector.\n- kernel_initializer: Initializer for the kernel weights matrix.\n- bias_initializer: Initializer for the bias vector.\n- kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n- bias_regularizer: Regularizer function applied to the bias vector.\n- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n- kernel_constraint: Constraint function applied to the kernel weights matrix.\n- bias_constraint: Constraint function applied to the bias vector.\n- trainable: Boolean, if `True` (default) the layer will be trainable.\n- name: String, name of the layer.\n- conv_op: String, the type of convolution operation to use (e.g., 'conv1d', 'conv2d', etc.).\n\nMethods:\n- build(input_shape): Builds the layer from input shape.\n- call(inputs): Invokes the layer on a input tensor.\n- compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n- get_config(): Returns the config of the layer.\n- _compute_causal_padding(inputs): Computes the padding required for causal convolution.\n- _get_channel_axis(): Returns the channel axis for the given data format.\n- _get_input_channel(input_shape): Returns the number of input channels.\n- _recreate_conv_op(inputs): Recreates the convolution operation.\n\"\"\"", "\"\"\"\nConv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\n1D convolution layer (e.g. temporal convolution).\n\nArgs:\n    filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, length, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, length)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any `dilation_rate` value != 1 is incompatible with specifying any `strides` value != 1.\n    groups: A positive integer specifying the number of groups in a group convolution. Groups can be used to partition the input into separate subspaces with shared weights. This applies to the first `filters` input dimensions. `filters` must be divisible by `groups`. When `groups == 1`, this is equivalent to a standard convolution. When `groups == filters`, this is equivalent to a separable convolution.\n    activation: Activation function to use. If you don't specify anything, no activation is applied (see `keras.activations`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix (see `keras.initializers`).\n    bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n    kernel_regularizer: Regularizer function applied to the `kernel` weights matrix (see `keras.regularizers`).\n    bias_regularizer: Regularizer function applied to the bias vector (see `keras.regularizers`).\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\") (see `keras.regularizers`).\n    kernel_constraint: Constraint function applied to the kernel matrix (see `keras.constraints`).\n    bias_constraint: Constraint function applied to the bias vector (see `keras.constraints`).\n\"\"\"", "\"\"\"\nA 1D transposed convolution layer (also called a deconvolution layer).\n\nThis layer can add missing entries to the output of a 1D convolutional layer\nby performing a transposed convolution operation. It is useful for upsampling\nin neural networks.\n\nParameters:\n- filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n- kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n- strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Default is 1.\n- padding: One of 'valid' or 'same' (case-insensitive). 'valid' means no padding. 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n- output_padding: An integer or tuple/list of a single integer, specifying the amount of padding along the output dimension. This is only used when padding='same'. Default is None.\n- data_format: A string, one of 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape (batch, length, channels) while 'channels_first' corresponds to inputs with shape (batch, channels, length).\n- dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Default is 1.\n- activation: Activation function to use. If you don't specify anything, no activation is applied (i.e., \"linear\" activation: `a(x) = x`).\n- use_bias: Boolean, whether the layer uses a bias vector. Default is True.\n- kernel_initializer: Initializer for the kernel weights matrix. Default is 'glorot_uniform'.\n- bias_initializer: Initializer for the bias vector. Default is 'zeros'.\n- kernel_regularizer: Regularizer function applied to the kernel weights matrix. Default is None.\n- bias_regularizer: Regularizer function applied to the bias vector. Default is None.\n- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). Default is None.\n- kernel_constraint: Constraint function applied to the kernel weights matrix. Default is None.\n- bias_constraint: Constraint function applied to the bias vector. Default is None.\n\nMethods:\n- build(input_shape): Builds the layer from input shape.\n- call(inputs): Invokes the layer on a input tensor.\n- compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n- get_config(): Returns the config dictionary for the layer.\n\"\"\"", "\"\"\"\nConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n\n2D convolution layer (e.g. spatial convolution over images).\n\nArgs:\n    filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Dilated convolution is also known as atrous convolution.\n    groups: A positive integer specifying the number of groups in a group convolution. If set to 1, this is just a standard 2D convolution, otherwise the input will be split into `groups` separate inputs.\n    activation: Activation function to use. If you don't specify anything, no activation is applied (see `keras.activations`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix (see `keras.initializers`).\n    bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n    kernel_regularizer: Regularizer function applied to the `kernel` weights matrix (see `keras.regularizers`).\n    bias_regularizer: Regularizer function applied to the bias vector (see `keras.regularizers`).\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\") (see `keras.regularizers`).\n    kernel_constraint: Constraint function applied to the kernel matrix (see `keras.constraints`).\n    bias_constraint: Constraint function applied to the bias vector (see `keras.constraints`).\n\"\"\"", "\"\"\"\nA 2D transposed convolution layer (a.k.a. deconvolution layer).\n\nThis layer can add several zero entries to the height and width of the input\naccording to the `output_padding` argument. This can be useful for increasing\nthe spatial dimensions of the output.\n\nArgs:\n    filters: Integer, the dimensionality of the output space (i.e. the number\n        of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n        and width of the 2D convolution window. Can be a single integer to\n        specify the same value for all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of\n        the convolution along the height and width. Can be a single integer to\n        specify the same value for all spatial dimensions. Specifying any\n        stride value != 1 is incompatible with specifying any `output_padding`\n        value != 0.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers, specifying\n        the amount of zero padding to apply to the output in the height and\n        width dimensions. Can be a single integer to specify the same value\n        for all spatial dimensions. This parameter can be used to ensure that\n        the output spatial dimensions are multiples of the strides.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs. `channels_last` corresponds\n        to inputs with shape `(batch, height, width, channels)` while\n        `channels_first` corresponds to inputs with shape `(batch, channels,\n        height, width)`. It defaults to the `image_data_format` value found in\n        your Keras config file at `~/.keras/keras.json`. If you never set it,\n        then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation\n        rate to use for dilated convolution. Can be a single integer to specify\n        the same value for all spatial dimensions.\n    activation: Activation function to use. If you don't specify anything, no\n        activation is applied (see `keras.activations`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix (see\n        `keras.initializers`). Defaults to 'glorot_uniform'.\n    bias_initializer: Initializer for the bias vector (see `keras.initializers`).\n        Defaults to 'zeros'.\n    kernel_regularizer: Regularizer function applied to the `kernel` weights\n        matrix (see `keras.regularizers`).\n    bias_regularizer: Regularizer function applied to the bias vector (see\n        `keras.regularizers`).\n    activity_regularizer: Regularizer function applied to the output of the\n        layer (its \"activation\") (see `keras.regularizers`).\n    kernel_constraint: Constraint function applied to the kernel matrix (see\n        `keras.constraints`).\n    bias_constraint: Constraint function applied to the bias vector (see\n        `keras.constraints`).\n\nInput shape:\n    4D tensor with shape: `(batch_size, channels, rows, cols)` if\n        `data_format` is `channels_first` or 4D tensor with shape:\n        `(batch_size, rows, cols, channels)` if `data_format` is\n        `channels_last`.\n\nOutput shape:\n    4D tensor with shape: `(batch_size, filters, new_rows, new_cols)` if\n        `data_format` is `channels_first` or 4D tensor with shape:\n        `(batch_size, new_rows, new_cols, filters)` if `data_format` is\n        `channels_last`. `rows` and `cols` values might have changed due to\n        padding.\n\"\"\"", "\"\"\"\nConv3D(layer)\n\nThree-dimensional convolution layer (e.g. spatial convolution over volumes).\n\nParameters\n----------\nfilters : int\n    The number of filters in the convolution.\nkernel_size : tuple or list of 3 ints\n    The spatial dimensions of the filters.\nstrides : tuple or list of 3 ints, optional\n    The strides of the convolution along the height, width, and depth.\npadding : {'valid', 'same'}, optional\n    The padding method for the convolution.\ndata_format : {'channels_last', 'channels_first'}, optional\n    The data format for the input data.\ndilation_rate : tuple or list of 3 ints, optional\n    The dilation rate to use for the convolution.\ngroups : int, optional\n    The number of groups for group convolution.\nactivation : str or None, optional\n    The activation function to use. If None, no activation is applied.\nuse_bias : bool, optional\n    Whether to use a bias vector for the convolution.\nkernel_initializer : str or Initializer, optional\n    The initializer for the kernel weights.\nbias_initializer : str or Initializer, optional\n    The initializer for the bias vector.\nkernel_regularizer : str or Regularizer, optional\n    The regularizer for the kernel weights.\nbias_regularizer : str or Regularizer, optional\n    The regularizer for the bias vector.\nactivity_regularizer : str or Regularizer, optional\n    The regularizer for the output activation.\nkernel_constraint : str or Constraint, optional\n    The constraint for the kernel weights.\nbias_constraint : str or Constraint, optional\n    The constraint for the bias vector.\n\n\"\"\"", "\"\"\"\nConv3DTranspose is a 3D transposed convolution layer (also known as a deconvolution layer) in Keras. It is used to upscale the input volume, effectively performing the reverse operation of Conv3D. This layer takes a 5D input tensor (batch_size, channels, depth, height, width) and produces a 5D output tensor with the specified output shape.\n\nParameters:\n- filters: Integer, the dimensionality of the output space (i.e., the number of output filters in the convolution).\n- kernel_size: An integer or tuple/list of 3 integers, specifying the depth, height, and width of the 3D convolution window.\n- strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height, and width. Specifying any stride value != 1 is incompatible with specifying any `output_padding` value != None.\n- padding: One of 'valid' or 'same' (case-insensitive). 'valid' means no padding. 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n- output_padding: An integer or tuple/list of 3 integers, specifying the amount of padding along the depth, height, and width. This is only used if `padding` is 'valid', and must be provided if strides > 1.\n- data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, depth, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, depth, height, width)`.\n- dilation_rate: An integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution.\n- activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n- use_bias: Boolean, whether the layer uses a bias vector.\n- kernel_initializer: Initializer for the kernel weights matrix.\n- bias_initializer: Initializer for the bias vector.\n- kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n- bias_regularizer: Regularizer function applied to the bias vector.\n- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")..\n- kernel_constraint: Constraint function applied to the kernel weights matrix.\n- bias_constraint: Constraint function applied to the bias vector.\n\nMethods:\n- build(input_shape): Builds the layer given the input shape.\n- call(inputs): Invokes the layer on a input tensor and returns the output tensor.\n- compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n- get_config(): Returns the config dictionary for the layer.\n\"\"\"", "\"\"\"\nCropping1D layer for 1D inputs.\n\nThis layer crops along the temporal dimension of a 3D input tensor (batch, length, channels).\nIt supports cropping from both the beginning and the end of the sequence.\n\nArgs:\n    cropping (tuple): A tuple of 2 integers, specifying the cropping lengths from the start and end of the sequence.\n\nInput shape:\n    3D tensor with shape: `(batch_size, steps, input_dim)`.\n\nOutput shape:\n    3D tensor with shape: `(batch_size, steps - cropping[0] - cropping[1], input_dim)`.\n\nMethods:\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: The logic for the forward pass of the layer.\n    get_config: Returns the configuration of the layer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nCropping2D layer for spatial data.\n\nThis layer crops the input along the spatial dimensions (height and width) only.\n\nArgs:\n    cropping: Int, tuple of 2 ints, tuple of 2 tuples of 2 ints.\n        - If int, the same symmetric cropping is applied to height and width.\n        - If tuple of 2 ints, interpreted as two different symmetric cropping values for height and width: `(symmetric_height_crop, symmetric_width_crop)`.\n        - If tuple of 2 tuples of 2 ints, interpreted as `((top_crop, bottom_crop), (left_crop, right_crop))`.\n\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n\nInput shape:\n    4D tensor with shape:\n    - If `data_format='channels_last':` `(batch_size, rows, cols, channels)`\n    - If `data_format='channels_first':` `(batch_size, channels, rows, cols)`\n\nOutput shape:\n    4D tensor with shape:\n    - If `data_format='channels_last':` `(batch_size, cropped_rows, cropped_cols, channels)`\n    - If `data_format='channels_first':` `(batch_size, channels, cropped_rows, cropped_cols)`\n\"\"\"", "\"\"\"\nCropping3D layer for 3D inputs.\n\nThis layer crops a 5D input tensor (e.g., a sequence of 3D volumes) by slicing it along the spatial dimensions. It supports different cropping modes for each dimension and can handle both 'channels_first' and 'channels_last' data formats.\n\nParameters:\n- cropping: An integer, a tuple of 3 integers (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), or a tuple of 3 tuples of 2 integers ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)). Specifies the cropping amounts for each dimension.\n- data_format: A string, either 'channels_first' or 'channels_last'. It specifies the data format of the input tensor.\n\nInput shape:\n- 5D tensor with shape: `(batch_size, channels, depth, height, width)` if data_format='channels_first' or `(batch_size, depth, height, width, channels)` if data_format='channels_last'.\n\nOutput shape:\n- 5D tensor with shape: `(batch_size, channels, dim1, dim2, dim3)` if data_format='channels_first' or `(batch_size, dim1, dim2, dim3, channels)` if data_format='channels_last', where `dim1`, `dim2`, and `dim3` are the cropped dimensions.\n\nMethods:\n- call: Applies the cropping operation to the input tensor.\n- compute_output_shape: Computes the output shape of the cropping operation.\n- get_config: Returns the configuration of the layer as a dictionary.\n\"\"\"", "\"\"\"\nDBSCAN clustering algorithm.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : str or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string, it must be one of the options\n    allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n    If X is the distance array itself, use \"precomputed\" as the metric.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method.\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or KDTree. This can affect the\n    speed of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends on the nature of the\n    problem.\n\np : float, default=None\n    The power of the Minkowski metric to be used to calculate distance.\n    If None, will use the L2 norm.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\ncore_sample_indices_ : array, shape = [n_core_samples]\n    Indices of core samples.\n\ncomponents_ : array, shape = [n_core_samples, n_features]\n    Copy of each core sample found by training.\n\nlabels_ : array, shape = [n_samples]\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\nSee Also\n--------\nsklearn.neighbors.NearestNeighbors : Nearest sample queries.\n\nNotes\n-----\nThe default values for the parameters ``eps`` and ``min_samples`` might be\nnot optimal for the application at hand. For many applications ``eps`` should\nnot be smaller than ``sqrt(1 / n_features)``.\n\nReferences\n----------\n.. [1] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, A\n       Density-Based Algorithm for Discovering Clusters in Large Spatial\n       Databases with Noise, In 2nd International Conference on Knowledge\n       Discovery and Data Mining (KDD-96), 1996\n\"\"\"", "\"\"\"\nDepthwise 2D convolution layer.\n\nThis layer performs a depthwise convolution that acts separately on channels,\npreserving the number of feature maps. It is commonly used in mobile vision\napplications to reduce the computational complexity of convolutional networks.\n\nArgs:\n    kernel_size: An integer or tuple/list of 2 integers, specifying the\n        height and width of the 2D convolution window.\n    strides: An integer or tuple/list of 2 integers, specifying the strides\n        of the convolution along the height and width. Can be a single integer\n        to specify the same value for all spatial dimensions.\n    padding: One of 'valid' or 'same' (case-insensitive).\n    depth_multiplier: The number of depthwise convolution output channels for\n        each input channel. The total number of depthwise convolution output\n        channels will be equal to `filters * depth_multiplier`.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs. `channels_last` corresponds\n        to inputs with shape `(batch, height, width, channels)` while\n        `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your Keras config file.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation\n        rate to use for dilated convolution. Can be a single integer to specify\n        the same value for all spatial dimensions.\n    activation: Activation function to use. If you don't specify anything, no\n        activation is applied (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to the depthwise kernel matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")..\n    depthwise_constraint: Constraint function applied to the depthwise kernel matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\nInput shape:\n    4D tensor with shape: `(batch_size, rows, cols, channels)` if data_format='channels_last'\n    or 4D tensor with shape: `(batch_size, channels, rows, cols)` if data_format='channels_first'.\n\nOutput shape:\n    4D tensor with shape: `(batch_size, new_rows, new_cols, channels * depth_multiplier)` if data_format='channels_last'\n    or 4D tensor with shape: `(batch_size, channels * depth_multiplier, new_rows, new_cols)` if data_format='channels_first'.\n\"\"\"", "\"\"\"\nEmbedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n\nCreates a trainable embedding matrix for word indices.\n\nArgs:\n    input_dim (int): Size of the vocabulary, i.e. maximum integer index + 1.\n    output_dim (int): Dimension of the dense embedding.\n    embeddings_initializer (str or Initializer): Initializer for the embeddings matrix.\n    embeddings_regularizer (str or Regularizer): Regularizer function applied to the embeddings matrix.\n    activity_regularizer (str or Regularizer): Regularizer function applied to the output of the layer (its \"activation\").\n    embeddings_constraint (str or Constraint): Constraint function applied to the embeddings matrix.\n    mask_zero (bool): Whether or not the input value 0 is a special \"padding\" value that should be masked out.\n    input_length (int): Length of input sequences, when it is fixed.\n\nReturns:\n    A Keras Layer that contains the embedding matrix and can be used to transform integer indices into dense vectors.\n\nMethods:\n    build(input_shape): Builds the layer.\n    compute_mask(inputs, mask=None): Computes the output mask.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): Invokes the layer on a input tensor.\n    get_config(): Returns the config of the layer.\n\"\"\"", "```python\n\"\"\"\nA WSGI application class for creating and running web applications. This class is the core of a Flask application and provides methods for setting up the application, handling requests, and serving responses. It includes configuration management, request handling, error management, and support for testing and debugging.\n\nAttributes:\n    default_config (ImmutableDict): Default configuration settings for the application.\n    request_class (type[Request]): The request class used by the application.\n    response_class (type[Response]): The response class used by the application.\n    session_interface (SessionInterface): The session interface used by the application.\n\nMethods:\n    __init__: Initializes the Flask application with the given parameters.\n    get_send_file_max_age: Returns the maximum age for a send file.\n    send_static_file: Sends a static file from the application's static folder.\n    open_resource: Opens a resource file from the application's root path.\n    open_instance_resource: Opens a resource file from the application's instance path.\n    create_jinja_environment: Creates the Jinja2 environment for the application.\n    create_url_adapter: Creates a URL adapter for the application.\n    raise_routing_exception: Raises a routing exception if the request is invalid.\n    update_template_context: Updates the template context with additional variables.\n    make_shell_context: Creates a shell context for the application.\n    run: Runs the application in a development server.\n    test_client: Returns a test client for the application.\n    test_cli_runner: Returns a test CLI runner for the application.\n    handle_http_exception: Handles HTTP exceptions.\n    handle_user_exception: Handles user exceptions.\n    handle_exception: Handles uncaught exceptions.\n    log_exception: Logs an exception.\n    dispatch_request: Dispatches the request to the appropriate view function.\n    full_dispatch_request: Dispatches the request and handles the response.\n    finalize_request: Finalizes the request and returns the response.\n    make_default_options_response: Creates a default OPTIONS response.\n    ensure_sync: Ensures a function is synchronous.\n    async_to_sync: Converts an asynchronous function to a synchronous one.\n    url_for: Generates a URL for a given endpoint.\n    make_response: Creates a response object from the given value.\n    preprocess_request: Preprocesses the request before dispatching.\n    process_response: Processes the response after dispatching.\n    do_teardown_request: Teardowns the request context.\n    do_teardown_appcontext: Teardowns the application context.\n    app_context: Returns an application context.\n    request_context: Returns a request context.\n    test_request_context: Returns a test request context.\n    wsgi_app: The WSGI application callable.\n    __call__: The WSGI application callable.\n\"\"\"\n```", "\"\"\"\nA transformer that uses user-supplied functions for transforming and inverse transforming data.\n\nParameters\n----------\nfunc : callable or None, default=None\n    The function to use for transforming the data. If None, the identity function is used.\ninverse_func : callable or None, default=None\n    The function to use for inverse transforming the data. If None, the identity function is used.\nvalidate : bool, default=False\n    If True, input data will be checked for NaNs and infinities.\naccept_sparse : bool, default=False\n    If True, accept sparse matrices from scikit-learn as inputs.\ncheck_inverse : bool, default=True\n    If True, check that application of inverse function to transformed data recovers the original data.\nfeature_names_out : callable, {'one-to-one'}, or None, default=None\n    Method for computing feature names after transform. If 'one-to-one', uses the input feature names. If None, the output will have feature names that are the column indices of the transformed data.\nkw_args : dict or None, default=None\n    Keyword arguments to pass to the `func` when transforming the data.\ninv_kw_args : dict or None, default=None\n    Keyword arguments to pass to the `inverse_func` when inverse transforming the data.\n\nMethods\n-------\nfit(X, y=None)\n    Fit the transformer to the data.\ntransform(X)\n    Transform the data using the provided function.\ninverse_transform(X)\n    Inverse transform the data using the provided inverse function.\nget_feature_names_out(input_features=None)\n    Get output feature names for transformation.\n\"\"\"", "\"\"\"\nGaussianMixture provides a generative mixture model for fitting data with a mixture of Gaussian distributions.\n\nParameters\n----------\nn_components : int, default=1\n    The number of mixture components.\ncovariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n    String describing the type of covariance parameters to use.\ntol : float, default=1e-3\n    The convergence threshold. EM iterations will stop when the increase in log-likelihood is below this value.\nreg_covar : float, default=1e-6\n    Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.\nmax_iter : int, default=100\n    The number of EM iterations to perform.\nn_init : int, default=1\n    The number of initializations to perform. The best results are kept.\ninit_params : {'kmeans', 'random'}, default='kmeans'\n    The method used to initialize the weights, means and precisions covariance parameters. String must be one of 'kmeans' or 'random'.\nweights_init : array-like of shape (n_components,), default=None\n    The user-provided initial weights, ignored if None.\nmeans_init : array-like of shape (n_components, n_features), default=None\n    The user-provided initial means, ignored if None.\nprecisions_init : array-like of shape (n_components, n_features, n_features), default=None\n    The user-provided initial precisions (inverse of covariance matrices), ignored if None.\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for weights and means initialization. Pass an int for reproducible results across multiple function calls.\nwarm_start : bool, default=False\n    If True, reuse the solution of the previous call to fit as initialization, otherwise, just use random initialization.\nverbose : int, default=0\n    Controls the verbosity of the EM algorithm.\nverbose_interval : int, default=10\n    Controls the frequency of verbose output if verbose is not 0. None means no output.\n\nMethods\n-------\nfit(X)\n    Estimate model parameters with the EM algorithm.\npredict(X)\n    Predict the labels for the data samples in X using trained model.\nscore(X)\n    Compute the log-likelihood of X under the current model.\nbic(X)\n    Bayesian information criterion for the current model fit on the data X.\naic(X)\n    Akaike information criterion for the current model fit on the data X.\n\"\"\"", "\"\"\"\nGlobalAveragePooling1D performs global average pooling operation for 1D inputs. It supports masking for ignoring specific timesteps during the pooling operation.\n\nParameters:\n- data_format: A string, either 'channels_last' (default) or 'channels_first'. It specifies the ordering of the dimensions of the input tensors.\n- **kwargs: Additional keyword arguments passed to the superclass.\n\nMethods:\n- call(inputs, mask=None): Applies the global average pooling operation to the inputs. If a mask is provided, it is used to ignore specific timesteps.\n- compute_mask(inputs, mask=None): Returns None, as the mask is not propagated through the pooling operation.\n\"\"\"", "\"\"\"\nApply 2D global average pooling over the input.\n\nArgs:\n    inputs: Input tensor.\n\nReturns:\n    Output tensor.\n\n\"\"\"", "\"\"\"\nApply 3D global average pooling over the input.\n\nArgs:\n    inputs: Input tensor.\n\nReturns:\n    Output tensor.\n\n\"\"\"", "\"\"\"\nApply global max pooling over the specified axis of a 3D input tensor.\n\nParameters:\n- inputs: Input tensor of shape (batch_size, steps, channels) if data_format='channels_last'\n          or (batch_size, channels, steps) if data_format='channels_first'.\n\nReturns:\n- Output tensor of shape (batch_size, 1, channels) if keepdims=True, otherwise (batch_size, channels).\n\"\"\"", "\"\"\"\nApply global max pooling over the specified spatial dimensions of a 4D input tensor.\n\nParameters:\n- inputs: Input tensor of shape (batch_size, channels, height, width) if data_format='channels_last' or (batch_size, height, width, channels) otherwise.\n\nReturns:\n- Output tensor of shape (batch_size, channels, 1, 1) if keepdims is True, otherwise (batch_size, channels).\n\nNotes:\n- The data_format parameter determines the spatial dimensions of the input tensor.\n- The keepdims parameter, if True, retains the pooled dimensions with size 1.\n\"\"\"", "\"\"\"\nApply global max pooling over the spatial (or temporal) dimensions of a 3D input tensor.\n\nParameters:\n- inputs: Input tensor of shape (batch_size, depth, height, width, channels) if data_format='channels_last'\n          or (batch_size, depth, channels, height, width) if data_format='channels_first'.\n\nReturns:\n- Output tensor of shape (batch_size, depth, 1, 1, channels) if data_format='channels_last'\n  or (batch_size, depth, 1, channels, 1) if data_format='channels_first', containing the max values\n  over the spatial dimensions.\n\nAttributes:\n- data_format: A string, either 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs.\n- keepdims: A boolean, whether to keep the spatial dimensions with size 1 in the output tensor.\n\"\"\"", "\"\"\"\nGlobalPooling1D(layer)\n\nGlobal 1D pooling layer.\n\nArgs:\n    data_format: A string, either \"channels_last\" (default) or \"channels_first\". \n        The ordering of the dimensions in the inputs. \n        \"channels_last\" corresponds to inputs with shape \n        `(batch, steps, features)` while \"channels_first\" corresponds to \n        inputs with shape `(batch, features, steps)`.\n    keepdims: A boolean, whether to keep the reduced dimensions with length 1. \n        If `data_format` is \"channels_last\", \n        the output shape is `(batch, 1, 1)` if `keepdims` is True, \n        and `(batch, 1)` if `keepdims` is False. \n        If `data_format` is \"channels_first\", \n        the output shape is `(batch, 1, 1)` if `keepdims` is True, \n        and `(batch, 1)` if `keepdims` is False.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer, \n        given the input shape.\n    call(inputs): The logic for the forward pass of the layer. \n        Currently, this method is not implemented and raises a NotImplementedError.\n    get_config(): Returns the config of the layer, which can be used to \n        recreate the layer.\n\"\"\"", "\"\"\"\nGlobalPooling2D(layer)\n\nGlobal 2D pooling layer - Max or Average pooling over the entire map.\n\nParameters:\n- data_format: A string, either \"channels_last\" (default) or \"channels_first\". \n  The ordering of the dimensions in the inputs. \n  \"channels_last\" corresponds to inputs with shape \n  `(batch, height, width, channels)` while \"channels_first\" corresponds to \n  inputs with shape `(batch, channels, height, width)`.\n- keepdims: A boolean, whether to keep the reduced dimensions with length 1, \n  for broadcasting purposes.\n\nInput shape:\n- 4D tensor with shape: \n  `(batch_size, channels, rows, cols)` if data_format='channels_last'\n  or \n  `(batch_size, channels, cols, rows)` if data_format='channels_first'.\n\nOutput shape:\n- 4D tensor of pool values with shape: \n  `(batch_size, channels, 1, 1)` if keepdims=True and data_format='channels_last'\n  or \n  `(batch_size, channels, 1, 1)` if keepdims=True and data_format='channels_first'\n  or \n  `(batch_size, channels)` if keepdims=False.\n\nMethods:\n- call: Raises a NotImplementedError, as this is an abstract base class.\n- get_config: Returns the configuration of the layer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nGlobalPooling3D(layer)\n\nA 3D global pooling layer for spatial or temporal data.\n\nParameters\n----------\ndata_format : {'channels_last', 'channels_first'}, optional\n    The data format of the input data. Defaults to 'channels_last'.\nkeepdims : bool, optional\n    Whether to keep the reduced dimensions with length 1. Defaults to False.\n\nInput shape\n-----------\n5D tensor with shape:\n- For 'channels_last': `(batch_size, channels, depth, height, width)`\n- For 'channels_first': `(batch_size, channels, height, width, depth)`\n\nOutput shape\n------------\n- If `keepdims` is False:\n  - For 'channels_last': `(batch_size, channels)`\n  - For 'channels_first': `(batch_size, height, width)`\n- If `keepdims` is True:\n  - For 'channels_last': `(batch_size, 1, 1, 1, channels)`\n  - For 'channels_first': `(batch_size, channels, 1, 1, 1)`\n\nMethods\n-------\ncall(inputs)\n    To be implemented by subclasses. Processes the input tensor.\nget_config()\n    Returns the configuration of the layer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nGroupTimeSeriesSplit\n--------------------\n\nSplits time series data into training and testing sets while respecting group boundaries.\n\nParameters\n----------\ntest_size : int\n    Size of the test set.\ntrain_size : int, optional\n    Size of the training set. Required if `n_splits` is not specified.\nn_splits : int, optional\n    Number of train/test splits. Required if `train_size` is not specified.\ngap_size : int, default=0\n    Number of groups between the end of the training set and the start of the test set.\nshift_size : int, default=1\n    Number of groups to shift the training and test sets for the next split.\nwindow_type : str, default='rolling'\n    Type of window to use for splitting. Can be either 'rolling' or 'expanding'.\n\nMethods\n-------\nsplit(X, y=None, groups=None)\n    Generate indices to split data into training and test sets.\nget_n_splits(X=None, y=None, groups=None)\n    Return the number of splitting iterations in the cross-validator.\n_calculate_split_params()\n    Calculate the parameters needed for splitting the data based on the specified sizes and shifts.\n\"\"\"", "\"\"\"\nKmeans clustering algorithm.\n\nThis class implements the K-means clustering algorithm, which is a method for\npartitioning a given dataset into a set of k clusters. The algorithm aims to\nminimize the within-cluster sum of squares.\n\nParameters\n----------\nk : int\n    The number of clusters to form.\nmax_iter : int, default=10\n    Maximum number of iterations of the k-means algorithm for a single run.\nconvergence_tolerance : float, default=1e-05\n    The convergence tolerance, i.e., the threshold for the change in the\n    centroids to declare convergence.\nrandom_seed : int, default=None\n    The seed for the random number generator used when initializing the centroids.\nprint_progress : int, default=0\n    The frequency of progress printing. If greater than 0, the progress will be\n    printed every `print_progress` iterations.\n\nAttributes\n----------\ncentroids_ : numpy.ndarray\n    The final centroids of the clusters.\nclusters_ : dict\n    Dictionary mapping cluster indices to the indices of the samples in each cluster.\niterations_ : int\n    The number of iterations the algorithm ran for.\n_is_fitted : bool\n    Whether the model has been fitted to data.\n\nMethods\n-------\n_fit(X)\n    Fit the K-means model to the data.\n_predict(X)\n    Predict the cluster index for each sample in X.\n\"\"\"", "\"\"\"\nTransform labels into binary labels and back.\n\nThis transformer is useful for transforming multi-class labels into binary\nlabels and multi-label indicator matrices. It can also invert the transformation.\n\nParameters\n----------\nneg_label : int, default=0\n    The value with which to label the negative class.\npos_label : int, default=1\n    The value with which to label the positive class.\nsparse_output : bool, default=False\n    Whether to return sparse matrix instead of 2D array.\n\nAttributes\n----------\nclasses_ : array\n    The classes encountered during fitting.\ny_type_ : str\n    The type of the target data encountered during fitting.\nsparse_input_ : bool\n    Whether the input data was sparse during fitting.\n\nMethods\n-------\nfit(y)\n    Fit label binarizer according to the given training data.\nfit_transform(y)\n    Fit label binarizer and transform the input data.\ntransform(y)\n    Transform multi-class labels into binary labels or multi-label indicator\n    matrices.\ninverse_transform(Y, threshold=None)\n    Invert the transformation, returning the original labels.\n\"\"\"", "\"\"\"\nA transformer for encoding labels to integers and vice versa.\n\nParameters\n----------\nauto_wrap_output_keys : bool or None, default=None\n    Whether to automatically wrap the output keys. If None, the behavior\n    is determined by the context.\n\nMethods\n-------\nfit(y)\n    Fit the label encoder to the provided labels.\n\nfit_transform(y)\n    Fit the label encoder to the provided labels and return the transformed labels.\n\ntransform(y)\n    Transform the provided labels to their encoded integer representation.\n\ninverse_transform(y)\n    Inverse transform the provided encoded labels to their original representation.\n\n_more_tags()\n    Additional tags for the transformer, indicating that it can handle 1d labels.\n\"\"\"", "\"\"\"\nLinearRegression is a class for linear regression models that supports different fitting methods including direct solution, stochastic gradient descent (SGD), QR decomposition, and singular value decomposition (SVD). It can be used for regression tasks.\n\nParameters:\n- method: str, default='direct'\n    - 'direct': uses the normal equation to fit the model.\n    - 'sgd': uses stochastic gradient descent for fitting.\n    - 'qr': uses QR decomposition for fitting.\n    - 'svd': uses singular value decomposition for fitting.\n- eta: float, default=0.01\n    - Learning rate for the SGD method.\n- epochs: int, default=50\n    - Number of epochs (iterations over the dataset) for SGD.\n- minibatches: int or None, default=None\n    - Number of minibatches for SGD. If None, the whole dataset is used.\n- random_seed: int or None, default=None\n    - Seed for random number generation used in SGD.\n- print_progress: int, default=0\n    - Level of progress printing. 0 for no printing, 1 for printing cost at each epoch.\n\nMethods:\n- _fit(X, y, init_params=True)\n    - Fits the model to the training data.\n- _normal_equation(X, y)\n    - Computes the parameters using the normal equation.\n- _net_input(X)\n    - Computes the net input to the model.\n- _predict(X)\n    - Predicts target values for the input data.\n- _sum_squared_error_cost(y, y_val)\n    - Computes the sum of squared error cost.\n\"\"\"", "\"\"\"\nLogisticRegression class for training and making predictions using logistic regression.\n\nParameters\n----------\neta : float, default=0.01\n    Learning rate (between 0.0 and 1.0).\nepochs : int, default=50\n    Number of passes over the training dataset.\nl2_lambda : float, default=0.0\n    L2 regularization parameter.\nminibatches : int, default=1\n    Number of minibatches to use when averaging the gradient, if 1 is given then full batch is used.\nrandom_seed : int, default=None\n    Seed for random number generator.\nprint_progress : int, default=0\n    Prints progress in fitting to this level: 0=None, 1=Epochs, 2=Iterations.\n\nAttributes\n----------\neta : float\n    Learning rate.\nepochs : int\n    Number of passes over the training dataset.\nl2_lambda : float\n    L2 regularization parameter.\nminibatches : int\n    Number of minibatches to use when averaging the gradient.\nrandom_seed : int\n    Seed for random number generator.\nprint_progress : int\n    Prints progress in fitting to this level.\n_is_fitted : bool\n    True if the model has been fitted.\n\nMethods\n-------\n_fit(X, y)\n    Fit the model according to the given training data.\n_predict(X)\n    Predict class labels for samples in X.\n_predict_proba(X)\n    Predict probabilities of class labels for samples in X.\n\"\"\"", "\"\"\"\nA base class for defining loss functions in Keras. This class provides the foundational structure for creating custom loss functions, including handling of reduction, data types, and mask propagation.\n\nParameters:\n- name (str, optional): The name of the loss function. If not provided, it will be auto-generated based on the class name.\n- reduction (str, optional): Type of reduction to apply to the loss. Options include 'sum', 'sum_over_batch_size', 'none'. Default is 'sum_over_batch_size'.\n- dtype (str or tf.DType, optional): The data type policy for the loss function. If not provided, it defaults to the backend's floatx.\n\nProperties:\n- dtype: The data type policy for the loss function.\n\nMethods:\n- __call__(self, y_true, y_pred, sample_weight=None): Computes the loss given true and predicted values, applying reduction and sample weighting.\n- call(self, y_true, y_pred): A method to be implemented by subclasses to define the specific loss computation.\n- get_config(self): Returns the configuration of the loss function, which can be used to recreate the loss instance.\n- from_config(cls, config): A class method to create a loss instance from its configuration.\n\"\"\"", "\"\"\"\nMaxPooling1D(layer)\n\nOne-dimensional max pooling layer.\n\nParameters\n----------\npool_size : int, optional\n    Factor by which to downscale. The default is 2.\nstrides : int or None, optional\n    Strides of the pooling operation. If None, it will be equal to `pool_size`. The default is None.\npadding : {'valid', 'same'}, optional\n    Padding method. 'valid' means no padding. 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. The default is 'valid'.\ndata_format : {'channels_last', 'channels_first'}, optional\n    A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, steps, features)` while `channels_first` corresponds to inputs with shape `(batch, features, steps)`. The default is 'channels_last'.\n\"\"\"", "\"\"\"\nMaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\nApplies 2D max pooling over input data.\n\nParameters\n----------\npool_size : tuple of int, default (2, 2)\n    The size of the max pooling window.\nstrides : tuple of int, default None\n    The stride of the pooling window. If None, it will be equal to `pool_size`.\npadding : str, default 'valid'\n    The padding method, either 'valid' (no padding) or 'same' (padding to match output size).\ndata_format : str, default None\n    The data format of the input data, either 'channels_last' (default) or 'channels_first'.\n**kwargs\n    Additional keyword arguments to be passed to the base class.\n\"\"\"", "\"\"\"\nMaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n\n3D max pooling layer.\n\nParameters\n----------\npool_size : tuple of int, default (2, 2, 2)\n    Factor by which to downscale the input in each spatial dimension.\nstrides : tuple of int or None, default None\n    Strides of the pooling operation. If None, it will be set to `pool_size`.\npadding : {'valid', 'same'}, default 'valid'\n    Padding mode for the input borders.\ndata_format : {'channels_last', 'channels_first'}, default None\n    Data format of the input.\n**kwargs\n    Additional keyword arguments to be passed to the parent class.\n\"\"\"", "\"\"\"\nA base class for Keras metrics. Metrics are used to monitor the training process and evaluate the performance of models.\n\nMethods:\n    __init__(self, dtype=None, name=None): Initialize the metric with a name and dtype.\n    reset_state(self): Reset the state of the metric variables.\n    update_state(self, *args, **kwargs): Update the metric state based on inputs.\n    stateless_update_state(self, metric_variables, *args, **kwargs): Update the metric state in a stateless context.\n    result(self): Compute the result of the metric.\n    stateless_result(self, metric_variables): Compute the result of the metric in a stateless context.\n    stateless_reset_state(self): Reset the state of the metric variables in a stateless context.\n    dtype(self): Get the dtype of the metric.\n    _obj_type(self): Get the type of the object.\n    add_variable(self, shape, initializer, dtype=None, aggregation='sum', name=None): Add a variable to the metric.\n    add_weight(self, shape=(), initializer=None, dtype=None, name=None): Add a weight to the metric.\n    variables(self): Get the list of variables associated with the metric.\n    __call__(self, *args, **kwargs): Update the state and return the result of the metric.\n    get_config(self): Get the configuration of the metric.\n    from_config(cls, config): Create a metric from its configuration.\n    __setattr__(self, name, value): Track and set attributes.\n    _check_super_called(self): Ensure that `super().__init__()` is called in the `__init__()` method.\n    __repr__(self): Return a string representation of the metric.\n    __str__(self): Return a string representation of the metric.\n\"\"\"", "\"\"\"\nTransform between multi-label sets and binary indicator format.\n\nThis transformer encodes labels to binary arrays and vice versa. It is useful\nwhen a multi-label classification problem is formulated as a binary problem.\n\nParameters\n----------\nclasses : array-like or None, default=None\n    Array consisting of the label classes in the order they should be\n    encoded. If not provided, labels will be inferred from the input data.\nsparse_output : bool, default=False\n    Whether the output should be in sparse CSR format. If set to False, the\n    output will be in dense format.\n\nAttributes\n----------\nclasses_ : array of shape (n_classes,)\n    Holds the label classes as sorted array.\n_classes : array of shape (n_classes,)\n    Holds the label classes as array with original order.\n\nMethods\n-------\nfit(y)\n    Fit the MultiLabelBinarizer to the provided label set.\n\nfit_transform(y)\n    Fit to data, then transform it.\n\ntransform(y)\n    Transform multi-label data to binary indicator format.\n\ninverse_transform(yt)\n    Transform binary indicator format to multi-label data.\n\n_more_tags()\n    Additional tags for the transformer.\n\"\"\"", "\"\"\"\nOneHotEncoder is a class for encoding categorical features as a one-hot numeric array. This class handles the encoding process with various options for handling unknown categories, dropping categories, and managing sparse output.\n\nParameters\n----------\ncategories : 'auto', list, or None, default='auto'\n    Categories (unique values) per feature. 'auto' determines categories automatically from the training data.\ndrop : {'first', 'if_binary'}, array-like, or None, default=None\n    Specifies a category which is dropped during the one-hot encoding. If 'if_binary' and the feature is binary (i.e. the feature has only two categories), the first category is dropped. If None, no category is dropped.\nsparse_output : bool, default=True\n    Whether the output transform is a sparse matrix or dense array.\ndtype : data type, default=np.float64\n    Desired dtype of output.\nhandle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, default='error'\n    Whether to raise an error or ignore if a unknown category is present in the data.\nmin_frequency : int or float, default=None\n    Minimum frequency threshold for categories to be included in the one-hot encoding. If a float, it should be between 0.0 and 1.0.\nmax_categories : int, default=None\n    Maximum number of categories to be one-hot encoded. If None, all categories are encoded.\nfeature_name_combiner : {'concat'}, callable, default='concat'\n    Function or method to combine feature name with category. If 'concat', feature names are concatenated with category values.\n\nMethods\n-------\nfit(X, y=None)\n    Fit the OneHotEncoder to the data.\ntransform(X)\n    Transform the data using one-hot encoding.\ninverse_transform(X)\n    Inverse transform the data back to the original categorical format.\nget_feature_names_out(input_features=None)\n    Get output feature names for transformation.\n\"\"\"", "\"\"\"\nFit the OPTICS algorithm to the data.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself : object\n    Returns the instance itself.\n\"\"\"", "\"\"\"\nEncodes categorical features as an integer array.\n\nThis class is a transformer that converts categorical data (which is typically\nnominal, i.e., cannot be ranked) to one-hot numeric data. It is designed to\nhandle both numerical and string categories. The encoding is a one-to-one\nmapping and missing values can be handled according to the specified\nparameters.\n\nParameters\n----------\ncategories : 'auto' or list, default='auto'\n    Categories (unique values) per feature. 'auto' determines categories\n    automatically from the training data.\n\ndtype : data type, default=np.float64\n    Desired dtype of output.\n\nhandle_unknown : {'error', 'use_encoded_value'}, default='error'\n    Whether to raise an error or encode unknown categories with a given\n    value.\n\nunknown_value : int or np.nan, default=None\n    Value to encode unknown categories (only valid when\n    handle_unknown='use_encoded_value').\n\nencoded_missing_value : int or np.nan, default=np.nan\n    Value to encode missing values.\n\nmin_frequency : int or float, default=None\n    Minimum frequency of categories to include. If an int, the absolute\n    minimum frequency. If a float, the relative minimum frequency\n    (between 0 and 1).\n\nmax_categories : int, default=None\n    Maximum number of categories to include. If None, all categories are\n    included.\n\nAttributes\n----------\ncategories_ : list of arrays\n    The categories of each feature determined during fitting.\n\ndtype : data type\n    The dtype of the output.\n\nhandle_unknown : {'error', 'use_encoded_value'}\n    The way to handle unknown categories.\n\nunknown_value : int or np.nan\n    The value to encode unknown categories.\n\nencoded_missing_value : int or np.nan\n    The value to encode missing values.\n\nmin_frequency : int or float\n    The minimum frequency of categories to include.\n\nmax_categories : int\n    The maximum number of categories to include.\n\nMethods\n-------\nfit(X, y=None)\n    Fit the OrdinalEncoder to X.\n\ntransform(X)\n    Transform X using one-hot encoding.\n\ninverse_transform(X)\n    Reverse transform the data.\n\nNotes\n-----\nThis class assumes that the input data is non-negative. If the input data\ncontains negative values, they will be treated as missing values and will be\nencoded as `encoded_missing_value`.\n\nExamples\n--------\n>>> from sklearn.preprocessing import OrdinalEncoder\n>>> enc = OrdinalEncoder()\n>>> enc.fit([[1, 2], [3, 4]])\nOrdinalEncoder()\n>>> enc.transform([[1, 2], [3, 4]])\narray([[0., 1.],\n       [1., 0.]])\n\"\"\"", "\"\"\"\nPooling1D(layer)\n\n1D Pooling layer - AveragePooling1D or MaxPooling1D.\n\nArgs:\n    pool_function: Function to use for pooling. Can be `tf.nn.max_pool` or `tf.nn.avg_pool`.\n    pool_size: An integer or tuple/list of 1 integer, specifying the size of the pooling window.\n    strides: An integer or tuple/list of 1 integer, specifying the strides of the pooling operation.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, steps, features)` while `channels_first` corresponds to inputs with shape `(batch, features, steps)`.\n    name: Optional name for the layer.\n\"\"\"", "\"\"\"\nPooling2D(layer)\n\nA 2D pooling layer.\n\nArgs:\n    pool_function: A pooling function, one of 'max' or 'avg'.\n    pool_size: An integer or tuple/list of 2 integers, specifying the size of the pooling window.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of the pooling operation.\n    padding: One of 'valid' or 'same' (case-insensitive). 'valid' means no padding. 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. If you never set it, then it will be \"channels_last\".\n    name: A string, the name of the layer.\n\nMethods:\n    call(inputs): The call method performs the pooling operation on the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of the layer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nPooling3D(layer)\n\n3D Pooling layer - AveragePooling3D or MaxPooling3D.\n\nArgs:\n    pool_function: Function to use for pooling. Should be either `tf.nn.max_pool` or `tf.nn.avg_pool`.\n    pool_size: Tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (e.g. (2, 2, 2) will halve the size of each dimension).\n    strides: Tuple of 3 integers, or None. Strides of the pooling operation. (e.g. (2, 2, 2) will move the sliding window two entries down and to the right at each step). If `None`, it will default to `pool_size`.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, depth, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width, depth)`.\n    name: Optional name for the layer.\n\"\"\"", "\"\"\"\nPrincipalComponentAnalysis(_BaseModel)\n\nA class for performing Principal Component Analysis (PCA) on a dataset.\n\nParameters:\n    n_components (int, optional): The number of principal components to keep. If None, all components are kept.\n    solver (str, optional): The solver to use for computing the principal components. Must be one of 'eigen' or 'svd'. Default is 'svd'.\n    whitening (bool, optional): If True, the output is whitened. Default is False.\n\nMethods:\n    fit(X, y=None): Fit the model to the data X.\n    transform(X): Transform the data X to the principal component space.\n\"\"\"", "\"\"\"\nRMSprop optimizer.\n\nThis class implements the RMSprop optimizer. It is a gradient descent method\nthat divides the gradient by a running average of its recent squared values\nto reduce the effect of noisy gradients. It can optionally include momentum\nand centering to improve convergence.\n\nArguments:\n    learning_rate: A float hyperparameter >= 0. Learning rate.\n    rho: A float hyperparameter >= 0. Discount factor for the history/coming gradient.\n    momentum: A float hyperparameter >= 0. Momentum.\n    epsilon: A small constant for numerical stability.\n    centered: A boolean. If `True`, gradients are normalized by an estimate of\n        their variance.\n    name: Optional name prefix for the operations created when applying\n        gradients.  Defaults to \"RMSprop\".\n    **kwargs: keyword arguments. Allowed to be {`learning_rate`, `decay`, `rho`,\n        `momentum`, `epsilon`, `centered`}.\n\"\"\"", "\"\"\"\nA semi-supervised learning classifier that uses self-training to iteratively label unlabeled data.\n\nParameters\n----------\nestimator : object, default=None\n    The base estimator to use for self-training. If None, the `base_estimator` is used.\nbase_estimator : object, default='deprecated'\n    The base estimator to use for self-training. This parameter is deprecated and will be removed in version 1.8. Use `estimator` instead.\nthreshold : float, default=0.75\n    The threshold for selecting samples to label in each iteration when `criterion='threshold'`.\ncriterion : {'threshold', 'k_best'}, default='threshold'\n    The criterion for selecting samples to label in each iteration. 'threshold' selects samples with a prediction probability above the threshold, 'k_best' selects the top k samples with the highest prediction probability.\nk_best : int, default=10\n    The number of samples to select in each iteration when `criterion='k_best'`.\nmax_iter : int, default=10\n    The maximum number of iterations to perform. If None, there is no limit.\nverbose : bool, default=False\n    Whether to print information about the progress of the algorithm.\n\nAttributes\n----------\nestimator_ : object\n    The fitted base estimator.\ntransduction_ : array-like, shape (n_samples,)\n    The predicted labels for each sample.\nlabeled_iter_ : array-like, shape (n_samples,)\n    The iteration in which each sample was labeled.\nn_iter_ : int\n    The number of iterations performed.\ntermination_condition_ : str\n    The reason for termination. Can be 'no_change', 'max_iter', or 'all_labeled'.\nclasses_ : array-like\n    The classes labels.\n\nMethods\n-------\nfit(X, y, **params)\n    Fit the model to the training data.\npredict(X, **params)\n    Predict the class labels for the provided data.\npredict_proba(X, **params)\n    Predict the probability of the provided samples for each class.\ndecision_function(X, **params)\n    For classification, return the decision function values for the samples.\npredict_log_proba(X, **params)\n    Predict the log probability of the provided samples for each class.\nscore(X, y, **params)\n    Return the mean accuracy on the given test data and labels.\nget_metadata_routing()\n    Return the metadata routing for this estimator.\n\"\"\"", "\"\"\"\nA separable convolutional layer. This layer first performs a depthwise convolution\non each input channel, and then applies a pointwise convolution to the resulting\noutput. It supports various arguments such as filter size, strides, padding,\nand activation functions. The layer can also include bias terms and supports\nregularization and constraints on the kernels and biases.\n\nParameters:\n    rank: An integer, the rank of the convolution, e.g. 1 for 1D convolution.\n    filters: Integer, the dimensionality of the output space (i.e. the number of\n             output filters in the convolution).\n    kernel_size: An integer or tuple/list of n integers, specifying the\n                 length of the convolution window.\n    strides: An integer or tuple/list of n integers, specifying the stride length\n             of the convolution. Specifying any stride value != 1 is incompatible\n             with specifying any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n                 The ordering of the dimensions in the inputs. `channels_last`\n                 corresponds to inputs with shape `(batch, ..., channels)` while\n                 `channels_first` corresponds to inputs with shape\n                 `(batch, channels, ...)`. It defaults to the `image_data_format`\n                 value found in your Keras config file at `~/.keras/keras.json`.\n                 If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of n integers, specifying the dilation\n                   rate to use for dilated convolution. Currently, specifying any\n                   `dilation_rate` value != 1 is incompatible with specifying\n                   any `strides` value != 1.\n    depth_multiplier: The number of depthwise convolution output channels for\n                      each input channel. The total number of depthwise convolution\n                      output channels will be equal to `filters_in * depth_multiplier`.\n    activation: Activation function to use. If you don't specify anything, no\n                activation is applied (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\n    pointwise_initializer: Initializer for the pointwise kernel matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to the depthwise kernel matrix.\n    pointwise_regularizer: Regularizer function applied to the pointwise kernel matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")..\n    depthwise_constraint: Constraint function applied to the depthwise kernel matrix.\n    pointwise_constraint: Constraint function applied to the pointwise kernel matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n    trainable: Boolean, if `True` (default) the weights of this layer will be\n               updated during training.\n    name: String, name of the layer.\n\nMethods:\n    build(input_shape): Builds the layer from input shape.\n    call(inputs): To be implemented in subclasses. The logic of the forward pass.\n    get_config(): Returns the config of the layer.\n\"\"\"", "\"\"\"\nSeparableConv1D class for 1D separable convolutional layer.\n\nArgs:\n    filters (int): The number of output filters.\n    kernel_size (int): Length of the convolution kernel.\n    strides (int or tuple, optional): Strides of the convolution. Default is 1.\n    padding (str, optional): Type of padding. Can be 'valid' or 'same'. Default is 'valid'.\n    data_format (str, optional): A string, one of 'channels_last' (default) or 'channels_first'. \n        The ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape \n        (batch, length, channels) while 'channels_first' corresponds to inputs with shape (batch, channels, length).\n        It defaults to the `image_data_format` value found in your Keras config file at `~/.keras/keras.json`. \n        If you never set it, then it will be 'channels_last'.\n    dilation_rate (int or tuple, optional): Dilation rate for the convolution. Default is 1.\n    depth_multiplier (int, optional): The number of depthwise convolution output channels for each input channel. \n        The total number of depthwise convolution output channels will be equal to `filters * depth_multiplier`. \n        Default is 1.\n    activation (str, optional): Activation function to use. Default is None, meaning no activation.\n    use_bias (bool, optional): Whether the layer uses a bias vector. Default is True.\n    depthwise_initializer (str or Initializer, optional): Initializer for the depthwise kernel matrix. \n        Default is 'glorot_uniform'.\n    pointwise_initializer (str or Initializer, optional): Initializer for the pointwise kernel matrix. \n        Default is 'glorot_uniform'.\n    bias_initializer (str or Initializer, optional): Initializer for the bias vector. Default is 'zeros'.\n    depthwise_regularizer (str or Regularizer, optional): Regularizer function applied to the depthwise kernel matrix. \n        Default is None.\n    pointwise_regularizer (str or Regularizer, optional): Regularizer function applied to the pointwise kernel matrix. \n        Default is None.\n    bias_regularizer (str or Regularizer, optional): Regularizer function applied to the bias vector. Default is None.\n    activity_regularizer (str or Regularizer, optional): Regularizer function applied to the output of the layer (its \"activation\"). \n        Default is None.\n    depthwise_constraint (str or Constraint, optional): Constraint function applied to the depthwise kernel matrix. \n        Default is None.\n    pointwise_constraint (str or Constraint, optional): Constraint function applied to the pointwise kernel matrix. \n        Default is None.\n    bias_constraint (str or Constraint, optional): Constraint function applied to the bias vector. Default is None.\n\nMethods:\n    call(inputs): Forward pass of the layer.\n\"\"\"", "\"\"\"\nSeparableConv2D(layer)\n\nA separable 2D convolution layer.\n\nParameters:\n- filters: Integer, the number of filters in the convolution.\n- kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n- strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution. Default is (1, 1).\n- padding: One of 'valid' or 'same' (case-insensitive). 'valid' means no padding. 'same' results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. Default is 'valid'.\n- data_format: A string, one of 'channels_last' (default) or 'channels_first'. It specifies the ordering of the dimensions of the inputs. 'channels_last' corresponds to inputs with shape (batch, height, width, channels) while 'channels_first' corresponds to inputs with shape (batch, channels, height, width). Default is 'channels_last'.\n- dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Default is (1, 1).\n- depth_multiplier: An integer, the number of depthwise convolution output channels for each input channel. Default is 1.\n- activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x). Default is None.\n- use_bias: Boolean, whether the layer uses a bias. Default is True.\n- depthwise_initializer: Initializer for the depthwise kernel matrix. Default is 'glorot_uniform'.\n- pointwise_initializer: Initializer for the pointwise kernel matrix. Default is 'glorot_uniform'.\n- bias_initializer: Initializer for the bias vector. Default is 'zeros'.\n- depthwise_regularizer: Regularizer function applied to the depthwise kernel matrix. Default is None.\n- pointwise_regularizer: Regularizer function applied to the pointwise kernel matrix. Default is None.\n- bias_regularizer: Regularizer function applied to the bias vector. Default is None.\n- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). Default is None.\n- depthwise_constraint: Constraint function applied to the depthwise kernel matrix. Default is None.\n- pointwise_constraint: Constraint function applied to the pointwise kernel matrix. Default is None.\n- bias_constraint: Constraint function applied to the bias vector. Default is None.\n\nMethods:\n- call(inputs): Processes the input through the convolution and applies activation (if specified).\n\n\"\"\"", "\"\"\"\nA sequential feature selector that can perform forward selection, backward elimination, or floating search based on the provided parameters. It can also handle fixed and grouped features.\n\nParameters\n----------\nestimator : object\n    A scikit-learn estimator object that implements `fit` and `predict`.\nk_features : int, tuple, or str, default=1\n    Number of features to select. Can be a positive integer, a tuple of (min, max) values, or 'best' or 'parsimonious' for automatic selection.\nforward : bool, default=True\n    Whether to perform forward selection.\nfloating : bool, default=False\n    Whether to perform floating search.\nverbose : int, default=0\n    Controls the verbosity: the higher, the more messages.\nscoring : str or callable, default=None\n    Scoring function (or loss function) to use for evaluation. If None, the estimator's default scoring method is used.\ncv : int, cross-validation generator, iterable, or None, default=5\n    Determines the cross-validation splitting strategy. Possible inputs for cv are:\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - An iterable yielding train, test splits.\nn_jobs : int, default=1\n    Number of jobs to run in parallel. `-1` means using all processors.\npre_dispatch : int or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:\n        - A positive integer specifying the total number of independent jobs that will be dispatched.\n        - A string, one of ('all', '2*n_jobs').\nclone_estimator : bool, default=True\n    Whether to clone the estimator before applying the feature selection.\nfixed_features : tuple, default=None\n    Fixed features to include in the selection process.\nfeature_groups : list of lists, default=None\n    Groups of features to consider together.\n\nAttributes\n----------\nnamed_estimators : dict\n    A dictionary of named estimators.\nest_ : object\n    The cloned estimator.\nscorer : object\n    The scoring function.\nfitted : bool\n    Whether the selector has been fitted.\ninterrupted_ : bool\n    Whether the fitting process was interrupted.\nk_feature_idx_ : tuple\n    The indices of the selected features.\nk_score_ : float\n    The score of the selected features.\nsubsets_ : dict\n    A dictionary containing the subsets of features and their corresponding scores.\nk_feature_names_ : tuple\n    The names of the selected features.\n\nMethods\n-------\nfit(X, y, groups=None, **fit_params)\n    Fit the SequentialFeatureSelector to the training data.\ntransform(X)\n    Reduce X to the selected features.\nfit_transform(X, y, groups=None, **fit_params)\n    Fit to data, then transform it.\nget_metric_dict(confidence_interval=0.95)\n    Return a dictionary of metrics for the selected features.\n\"\"\"", "\"\"\"\nCreate a Stochastic Gradient Descent (SGD) optimizer.\n\nSGD is a simple optimization algorithm that updates the weights of the model\nby taking the negative gradient of the loss function with respect to the\nweights.\n\nArgs:\n    learning_rate (float): A float value or a constant float tensor.\n        The learning rate.\n    momentum (float): A float value or a constant float tensor, or a callable\n        that returns a float value or a constant float tensor. (default: 0.0)\n        Momentum decay value.\n    nesterov (bool): Boolean. Whether to apply Nesterov momentum.\n        (default: False)\n    name (string): Optional name prefix for the operations created when\n        applying gradients. (default: 'SGD')\n    **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n        gradients by value, `lr` is to override the learning rate\n        if passed in `learning_rate`, `decay` is to decay the learning rate\n        after each update.\n\nMethods:\n    _create_slots: Create slots for the given list of variables.\n    _prepare_local: Prepare the local variables for the given device.\n    _resource_apply_dense: Update variable using SGD with dense gradients.\n    _resource_apply_sparse_duplicate_indices: Update variable using SGD with\n        sparse gradients and duplicate indices.\n    _resource_apply_sparse: Update variable using SGD with sparse gradients.\n    get_config: Returns the config of the optimizer.\n\n\"\"\"", "\"\"\"\nSoftmaxRegression is a class for training and making predictions using softmax regression. It inherits from _BaseModel, _IterativeModel, _Classifier, and _MultiClass. The class is initialized with parameters for learning rate (eta), number of epochs, L2 regularization strength (l2), minibatch size, number of classes, random seed, and print progress.\n\nAttributes:\n    eta (float): Learning rate for gradient descent.\n    epochs (int): Number of passes over the training dataset.\n    l2 (float): L2 regularization parameter.\n    minibatches (int): Number of minibatches to split the dataset into.\n    n_classes (int): Number of classes in the target variable (automatically inferred if not provided).\n    random_seed (int): Seed for the random number generator.\n    print_progress (int): Level of progress printing during training.\n\nMethods:\n    _net_input(X): Computes the net input to the softmax activation function.\n    _softmax_activation(z): Applies the softmax activation function to the input.\n    _cross_entropy(output, y_target): Computes the cross-entropy loss.\n    _cost(cross_entropy): Computes the cost function including L2 regularization.\n    _to_classlabels(z): Converts the output of the softmax function to class labels.\n    _forward(X): Performs the forward pass of the softmax regression model.\n    _backward(X, y_true, y_probas): Computes the gradients of the loss function with respect to the weights and bias.\n    _fit(X, y, init_params=True): Trains the model using gradient descent and backpropagation.\n    predict_proba(X): Predicts the probability of each class for the input data.\n    _predict(X): Predicts the class labels for the input data based on the predicted probabilities.\n\"\"\"", "\"\"\"\nImplements TargetEncoder, a feature transformation technique that encodes categorical features based on the target variable. This encoder supports both binary and multiclass target types and can handle continuous targets. It uses cross-validation to ensure robust encoding and supports smoothing to reduce variance.\n\nParameters\n----------\ncategories : {'auto', list}, default='auto'\n    Categories for each feature. 'auto' infers categories from the training data.\ntarget_type : {'auto', 'continuous', 'binary', 'multiclass'}, default='auto'\n    Type of the target variable. 'auto' infers the type from the data.\nsmooth : {'auto', float}, default='auto'\n    Smoothing factor to reduce variance. 'auto' calculates based on the target variance.\ncv : int, default=5\n    Number of folds for cross-validation.\nshuffle : bool, default=True\n    Whether to shuffle the data before splitting into batches for cross-validation.\nrandom_state : int, RandomState instance or None, default=None\n    Seed for random number generation.\n\nMethods\n-------\nfit(X, y)\n    Fits the encoder to the data.\nfit_transform(X, y)\n    Fits the encoder to the data and transforms the data.\ntransform(X)\n    Transforms the data using the fitted encoder.\nget_feature_names_out(input_features=None)\n    Returns the feature names after transformation.\n\"\"\"", "\"\"\"\nEncode transactions as binary indicators for each item.\n\nThis transformer converts a collection of transactions into a binary\nindicator matrix where each row represents a transaction and each column\nrepresents an item. The presence of an item in a transaction is indicated\nby a '1', and absence by a '0'.\n\nAttributes\n----------\ncolumns_ : list\n    Sorted list of unique items across all transactions.\ncolumns_mapping_ : dict\n    Mapping from item to its corresponding column index.\n\nMethods\n-------\nfit(X)\n    Compute the set of unique items and create a mapping from items to column indices.\ntransform(X, sparse=False)\n    Transform transactions into a binary indicator matrix. If `sparse` is True, return a sparse matrix.\nfit_transform(X, sparse=False)\n    Fit to data, then transform it.\ninverse_transform(array)\n    Convert a binary indicator matrix back to a list of items for each transaction.\nget_feature_names_out()\n    Get output feature names for transformation.\n\"\"\"", "\"\"\"\nUpSampling1D layer performs upsampling on 1D inputs by repeating each time step `size` times.\n\nArgs:\n    size (int): The factor by which to upsample. Default is 2.\n\nInput shape:\n    3D tensor with shape: `(batch_size, steps, input_dim)`.\n\nOutput shape:\n    3D tensor with shape: `(batch_size, size * steps, input_dim)`.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): The logic for the forward pass of the layer.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\"\"\"", "\"\"\"\nUpSampling2D layer for 2D inputs.\n\nThis layer performs upsampling on 2D inputs, increasing the spatial dimensions\nof the input tensor by the specified size factor. It supports two types of\ninterpolation: 'nearest' and 'bilinear'.\n\nArgs:\n    size: Tuple of two integers, specifying the size of the upsampling.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n        It defaults to the `image_data_format` value found in your Keras\n        config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    interpolation: A string, one of `nearest` (default) or `bilinear`.\n        The interpolation method used when upsampling the input.\n\nInput shape:\n    4D tensor with shape:\n    `(batch_size, channels, rows, cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n\nOutput shape:\n    4D tensor with shape:\n    `(batch_size, channels, upsampled_rows, upsampled_cols)` if data_format='channels_first'\n    or 4D tensor with shape:\n    `(batch_size, upsampled_rows, upsampled_cols, channels)` if data_format='channels_last'.\n\nMethods:\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: The logic for the forward pass of the layer.\n    get_config: Returns the configuration of the layer as a JSON-serializable dictionary.\n\"\"\"", "\"\"\"\nUpSampling3D layer for 3D upsampling of the input.\n\nArgs:\n    size: Tuple of 3 integers, factors by which to upsample the input.\n        For example, (2, 2, 2) will double the size of the 3 input dimensions.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to\n        inputs with shape `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your Keras config file.\n        If you never set it, then it will be \"channels_last\".\n\nInput shape:\n    5D tensor with shape:\n    - If `data_format='channels_last'`: `(batch, channels, first_dim, second_dim, third_dim)`\n    - If `data_format='channels_first'`: `(batch, channels, first_dim, second_dim, third_dim)`\n\nOutput shape:\n    5D tensor with shape:\n    - If `data_format='channels_last'`: `(batch, channels, first_dim * size[0], second_dim * size[1], third_dim * size[2])`\n    - If `data_format='channels_first'`: `(batch, channels, first_dim * size[0], second_dim * size[1], third_dim * size[2])`\n\nMethods:\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: Processes the input through the layer.\n    get_config: Returns the config dictionary for the layer.\n\"\"\"", "\"\"\"\nZeroPadding1D adds zeros around the temporal dimension of a 3D tensor.\n\nArgs:\n    padding: Int or tuple of 2 ints, padding value added to both sides of\n        the padding dimension (length dimension).\n\nInput shape:\n    3D tensor with shape: `(batch_size, steps, input_dim)`.\n\nOutput shape:\n    3D tensor with shape: `(batch_size, steps + padding[0] + padding[1], input_dim)`.\n\n\"\"\"", "\"\"\"\nZeroPadding2D layer for 2D padding of zero values around the input.\n\nThis layer can add padding of zero values around the input tensor. The padding is applied to the height and width dimensions of a 4D tensor (batch, channels, height, width).\n\nArgs:\n    padding: An integer, or a tuple of 2 integers (pad_height, pad_width) specifying the amount of padding. If an integer, the same symmetric padding is applied to height and width dimensions. If a tuple, it specifies the padding for the height and width dimensions respectively.\n    data_format: A string, either \"channels_first\" or \"channels_last\". It specifies the ordering of the dimensions of the input tensor.\n\nInput shape:\n    4D tensor with shape: `(batch_size, channels, rows, cols)` if data_format='channels_first' or 4D tensor with shape: `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n\nOutput shape:\n    4D tensor with shape: `(batch_size, channels, rows + pad_height_top + pad_height_bottom, cols + pad_width_left + pad_width_right)`.\n\nMethods:\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: Applies the padding to the input tensor.\n    get_config: Returns the configuration of the layer as a JSON serializable dictionary.\n\"\"\"", "\"\"\"\nZeroPadding3D layer for adding zero padded borders to a 3D input.\n\nArgs:\n    padding: int, tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n        - If int, the same symmetric padding is applied to depth, height, and width.\n        - If tuple of 3 ints, the first int is applied to the depth, the second to the height,\n          and the third to the width.\n        - If tuple of 3 tuples of 2 ints, the first tuple defines the padding for the depth,\n          the second for the height and the third for the width, as (pad_left, pad_right),\n          (pad_top, pad_bottom), (pad_front, pad_back).\n\nKwargs:\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n        while `channels_first` corresponds to\n        inputs with shape `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n        It defaults to the `image_data_format` value found in your Keras config file at\n        `~/.keras/keras.json`. If you never set it, then it will be \"channels_last\".\n\nReturns:\n    A tensor with the same data type as the input.\n\nInput shape:\n    5D tensor with shape:\n    - If `data_format` is `channels_last`:\n        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n    - If `data_format` is `channels_first`:\n        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n\nOutput shape:\n    5D tensor with shape:\n    - If `data_format` is `channels_last`:\n        `(batch_size, spatial_dim1 + padding[0][0] + padding[0][1], spatial_dim2 + padding[1][0] + padding[1][1], spatial_dim3 + padding[2][0] + padding[2][1], channels)`\n    - If `data_format` is `channels_first`:\n        `(batch_size, channels, spatial_dim1 + padding[0][0] + padding[0][1], spatial_dim2 + padding[1][0] + padding[1][1], spatial_dim3 + padding[2][0] + padding[2][1])`\n\"\"\"", "\"\"\"\nBase class for encoders that can handle categorical data. This class provides\ncommon methods for fitting and transforming data, as well as handling\ninfrequent categories.\n\nMethods:\n    _fit(X, handle_unknown='error', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False):\n        Fits the encoder to the data X, determining the categories for each\n        feature and handling unknown categories based on the specified\n        parameters.\n    _transform(X, handle_unknown='error', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None):\n        Transforms the data X using the fitted encoder, encoding categorical\n        features and handling unknown categories based on the specified\n        parameters.\n    infrequent_categories_:\n        Returns the infrequent categories for each feature after fitting.\n    _check_infrequent_enabled():\n        Checks if infrequent category handling is enabled based on the\n        parameters set.\n    _identify_infrequent(category_count, n_samples, col_idx):\n        Identifies infrequent categories based on the specified criteria.\n    _fit_infrequent_category_mapping(n_samples, category_counts, missing_indices):\n        Fits the mapping for infrequent categories to the data.\n    _map_infrequent_categories(X_int, X_mask, ignore_category_indices):\n        Maps infrequent categories in the transformed data.\n    _more_tags():\n        Returns metadata tags for the encoder, including supported data types\n        and handling of NaN values.\n\"\"\""]