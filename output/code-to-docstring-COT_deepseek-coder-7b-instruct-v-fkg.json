["\"\"\"\nAdamax optimizer.\n\nAdamax is an optimizer that extends the Adam algorithm by using the infinity norm\n(maximum absolute value) of the gradient instead of the 2-norm.\n\nThis optimizer is based on the premise that the maximum absolute value of the\ngradient is a better measure of the largest change that the weights have experienced\nin the current update step.\n\nThe Adamax optimizer incorporates momentum from the method of Nesterov (Ioffe and\nSzegedy, 2015) and is designed to be efficient and have little memory requirement.\n\nArguments:\n    learning_rate: A `Tensor` or a floating point value. The learning rate.\n    beta_1: A `Tensor` or a floating point value. The exponential decay rate\n        for the 1st moment estimates.\n    beta_2: A `Tensor` or a floating point value. The exponential decay rate\n        for the 2nd moment estimates.\n    epsilon: A `Tensor` or a floating point value. A small constant for\n        numerical stability.\n    name: Optional name for the operations. Defaults to 'Adamax'.\n\nReferences:\n    - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980)\n    - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n\"\"\"\n", "\"\"\"\nAgglomerationTransform is a transformer class that applies agglomerative clustering to data.\n\nMethods:\n\n    transform(X):\n        Applies a specified pooling function to the input data based on the labels from the agglomerative clustering.\n        If the pooling function is set to mean and the data is not sparse, it computes the mean of each cluster.\n        Otherwise, it applies the pooling function to each unique label in the data.\n\n    inverse_transform(X=None, *, Xt=None):\n        Reconstructs the original data from the transformed data.\n        It uses the unique labels and their corresponding inverse to reconstruct the original data.\n\"\"\"\n", "\"\"\"\nClass `AveragePooling1D` is a subclass of `Pooling1D` designed to perform 1D average pooling operations on input data.\n\nAttributes:\n    - `pool_size` (int): The size of the pooling window. This is an integer value that determines the height of the window.\n    - `strides` (int or None): The stride values for moving the pooling window. If None, it defaults to the pool_size.\n    - `padding` (str): Padding mode, either 'valid' or 'same'. Determines how the input is padded.\n    - `data_format` (str): The ordering of the dimensions in the input, either 'channels_last' or 'channels_first'.\n\nMethods:\n    - `__init__(self, pool_size=2, strides=None, padding='valid', data_format='channels_last', **kwargs)`:\n        Initializes an instance of `AveragePooling1D` with the specified parameters. It uses a partial function to apply the backend's average pooling operation with the pool mode set to 'avg'.\n\"\"\"\n", "\"\"\"\nClass `AveragePooling2D` is a subclass of `Pooling2D`. It is used to apply average pooling operation on 2D data.\n\nAttributes:\n    - `pool_size` (tuple): A tuple of 2 integers, representing the factors by which to downscale the input in the vertical and horizontal dimensions. For example, a pool_size of (2, 2) will halve the input in both dimensions.\n    - `strides` (tuple): A tuple of 2 integers, representing the strides for the pooling operation. Strides determine the step length in the input matrix.\n    - `padding` (str): A string indicating the type of padding to apply. It can be either 'valid' or 'same'. 'valid' means no padding is applied, while 'same' means padding is added to ensure the output has the same spatial dimensions as the input.\n    - `data_format` (str): A string indicating the ordering of the dimensions in the input data. It can be either 'channels_last' or 'channels_first'. 'channels_last' corresponds to inputs with shape (batch, height, width, channels), while 'channels_first' corresponds to inputs with shape (batch, channels, height, width).\n\nMethods:\n    - `__init__(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)`: Initializes the `AveragePooling2D` object with the specified parameters. It calls the constructor of the superclass `Pooling2D` with the appropriate arguments.\n\"\"\"\n", "\"\"\"\nClass AveragePooling3D is a subclass of Pooling3D, specialized in performing 3D average pooling operations on input data.\n\nArgs:\n    pool_size (tuple): A tuple of three integers, representing the factors by which the input data will be downsampled in each dimension. For example, a pool_size of (2, 2, 2) will halve the size of the input data in each of the three dimensions.\n    strides (tuple): A tuple of three integers, representing the number of steps to take in each dimension during the pooling operation. If not specified, it defaults to the pool_size.\n    padding (str): A string indicating the type of padding to apply. 'valid' means no padding, and 'same' means padding is added so that the output has the same spatial dimensions as the input.\n    data_format (str): A string, one of 'channels_last' (default) or 'channels_first'. This parameter is used to specify the ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape (batch, depth, height, width, channels), while 'channels_first' corresponds to inputs with shape (batch, channels, depth, height, width).\n\nAttributes:\n    pool_size (tuple): The tuple of three integers representing the downsampling factors in each dimension.\n    strides (tuple): The tuple of three integers representing the number of steps in each dimension during pooling.\n    padding (str): The padding method used.\n    data_format (str): The ordering of the dimensions in the inputs.\n\nMethods:\n    __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        Initializes the AveragePooling3D object with the specified parameters.\n\"\"\"\n", "```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python\n```python, python, python\n```python, python, generate\u51fd\u6570\npython\n```python\n```python\n```python\n```python\ncomments.\n```python\ncomments.\nother\npython\npython\n```python\n```python\n```python\n```\n```python\n```python\n```\n```\n```python\n```, python\n```, python\n```, python\n```, python\n```\n```, `python\n```\n```, `, `python\n```\n```, python, `python\n```\n```\n```\n\n\n   \n```\n```\n```\n```o\n```\n```\n```\n```\n   \n   \n```\n```\n```\n```\n\n   \n\n\n\n\n```\n\n\n\n\n```\n```\n\n\n    \n   \n    \n\n\n\n\n\n\n\n\n\n\n```\n```\n```\n\n\n\n\n```\n\n\n\n\n```,\n\n\n```\n```\n```\n   \n```\n```\n\n\n\n\n```\n```\n    \n```\n```\n```\n\n\n    \n    \n\n    \n    \n   \n   \n   \n   \n    \n   \n    \n   \n   \n    \n    \n   \n   \n   \n   \n    \n   \ntext of\n    \n\n   \ntext,sdeta\n\n    \n    pit, a\n    \n    singleton\n    \n    \n    \n     a\nmpl\n\n    \n    \n    \n    uri\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    valuri\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    od/\n    \n    \n    \n    \n    \n    valval\n    \n    classmethod.0,\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    valdef0   valmethodvallowercluttervalmethodvalmethod_methodclask-like-method_doc-1.method-methodvalvecvalreadyvalmethodvalister-1.\n    \n   val-method-like-method-method_docri.docmethod\n    \n\n    docri.\n   \nmplacann'like-like-like   \n   \n    \n   \n   \n    \n    \n   \n    \n    \n    \n    \n   \n    \n   \n   \n   \n    docri.doc-doc-basedroidacann'sccann's/docration_docendacann'ite\ncode-doc-docri.doc-like-method_doc,\n    \n   \n\n   \n    \n   \n   \n_method_method_doc_doc_docri.doc.\n\n   ig.doc,docend,doc-basedval.doc.docvalgerval.doc.methodval.method-1.doc:docri.docri.doc-likeval.doc:method_method_doc_bd_docri.doc.doc.docri.0.\n   \n   \n   \n   \n   \n   \n   \n   val_doc-10, v. It.0.doc-0.0.doc-10.doc\n\n\n\n\n\ncorger\n\n    \nacannac\nacannacannac_\n   \nmethod_\n   docannac.doc-method_bd.\n   doc_docann_doc_method_method_doc_method_method_bd_bd_method_bd-bd_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n   \n\n\n   \n   \n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "\"\"\"\nThe `Conv` class is a custom layer in a deep learning framework, specifically designed to represent a 2D convolution layer commonly used in Convolutional Neural Networks (CNNs). This layer applies a set of learnable filters to the input data, performing a convolution operation to extract hierarchical features.\n\nThe class is initialized with a variety of parameters to customize the layer's behavior:\n- `filters`: The number of output filters, i.e., the number of output channels.\n- `kernel_size`: The size of the convolution kernel, determining the spatial dimensions of the filters.\n- `strides`: The stride of the convolution, controlling the step size of the convolution operation.\n- `padding`: Padding strategy to handle the borders of the input, either 'valid' for no padding or 'same' for symmetric padding.\n- `data_format`: The ordering of the dimensions in the input data, either 'channels_last' (default for TensorFlow) or 'channels_first'.\n- `dilation_rate`: The rate at which to dilate the kernel, controlling the spacing of the filters.\n- `groups`: The number of groups to divide the input and output channels, used for grouped convolution.\n- `activation`: The activation function to apply to the output, defaulting to 'linear' if none specified.\n- `use_bias`: A boolean indicating whether to include a bias vector in the layer.\n- `kernel_initializer`: Initializer for the kernel weights matrix.\n- `bias_initializer`: Initializer for the bias vector.\n- `kernel_regularizer`: Regularizer for the kernel weights matrix.\n- `bias_regularizer`: Regularizer for the bias vector.\n- `activity_regularizer`: Regularizer for the layer's output.\n- `kernel_constraint`: Constraint function applied to the kernel weights matrix.\n- `bias_constraint`: Constraint function applied to the bias vector.\n\nThe layer builds its internal parameters in the `build` method and applies the convolution operation in the `call` method. The `compute_output_shape` method is used to determine the output shape of the layer.\n\nThe `_validate_init` method validates the initial parameters of the layer, ensuring they are valid and compatible. The `_compute_causal_padding` method computes the padding needed for causal convolution, and `_get_channel_axis` and `_get_input_channel` methods help in getting the channel axis and the number of input channels respectively. The `_get_padding_op` method returns the padding operation.\n\"\"\"\n", "\"\"\"\nClass Conv1D\n\nThe Conv1D class represents a one-dimensional convolution layer, which is a type of layer used in neural networks, particularly convolutional neural networks (CNNs). It is a subclass of the Conv class and inherits its properties and methods.\n\nArgs:\n    filters: Integer, the dimensionality of the output space, i.e., the number of output filters in the convolution.\n    kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Defaults to 1.\n    padding: String, one of 'valid' or 'same' (case-insensitive), indicating the type of padding to be used.\n    data_format: String, one of 'channels_last' or 'channels_first', specifying the ordering of the dimensions in the input data.\n    dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution.\n    groups: Integer, the number of groups to divide the input into when applying convolution.\n    activation: String or None, activation function to use. If None, no activation is applied (linear activation).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: String or Initializer, initializer for the kernel weights matrix.\n    bias_initializer: String or Initializer, initializer for the bias vector.\n    kernel_regularizer: String or Regularizer, regularizer function applied to the kernel weights matrix.\n    bias_regularizer: String or Regularizer, regularizer function applied to the bias vector.\n    activity_regularizer: String or Regularizer, regularizer function applied to the layer's output.\n    kernel_constraint: String or Constraint, constraint function applied to the kernel weights matrix.\n    bias_constraint: String or Constraint, constraint function applied to the bias vector.\n\nThe Conv1D class is used to create a 1D convolution layer in a neural network model. It is used to extract local features from one-dimensional data such as time series or audio signals. The layer's parameters allow for customization of the convolution process, including the number of filters, kernel size, stride, padding, dilation rate, and constraints on the kernel and bias. The activation function can also be specified, with 'linear' activation as the default.\n\"\"\"\n", "\"\"\"\nA Keras layer that implements a 1D transposed convolution (deconvolution).\n\nThis layer is a subclass of `Conv1D` and is used to increase the spatial dimensions (width) of the input. It is the opposite of Conv1D, where it upsamples the input by applying a convolution operation.\n\n**Arguments**\n\n- `filters`: Integer, the dimensionality of the output space (i.e., the number of output filters in the convolution).\n- `kernel_size`: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n- `strides`: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Default is 1.\n- `padding`: One of `\"valid\"` or `\"same\"` (case-insensitive). Default is `\"valid\"`.\n- `output_padding`: An integer or tuple/list of a single integer, specifying the amount of padding along the output dimension. Default is `None`.\n- `data_format`: A string, one of `channels_last` (default) or `channels_first`.\n- `dilation_rate`: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Default is 1.\n- `activation`: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x). Default is None.\n- `use_bias`: Boolean, whether the layer uses a bias vector. Default is True.\n- `kernel_initializer`: Initializer for the kernel weights matrix. Default is 'glorot_uniform'.\n- `bias_initializer`: Initializer for the bias vector. Default is 'zeros'.\n- `kernel_regularizer`: Regularizer function applied to the kernel weights matrix. Default is None.\n- `bias_regularizer`: Regularizer function applied to the bias vector. Default is None.\n- `activity_regularizer`: Regularizer function applied to the output of the layer (its \"activation\"). Default is None.\n- `kernel_constraint`: Constraint function applied to the kernel weights matrix. Default is None.\n- `bias_constraint`: Constraint function applied to the bias vector. Default is None.\n\n**Input shape**\n\n- 3D tensor with shape: `(batch_size, steps, input_dim)`, if `data_format='channels_last'`\n  or `(batch_size, input_dim, steps)`, if `data_format='channels_first'`.\n\n**Output shape**\n\n- 3D tensor with shape: `(batch_size, new_steps, filters)`, if `data_format='channels_last'`\n  or `(batch_size, filters, new_steps)`, if `data_format='channels_first'`.\n  `new_steps` is determined by the arguments `strides`, `padding`, `kernel_size`, `output_padding` and `dilation_rate`.\n\n**Raises**\n\n- ValueError: if `strides` is set to None.\n\"\"\"\n", "\"\"\"\nClass Conv2D\n\nThe Conv2D class is a specialized layer for performing 2D convolutions in neural networks. It is a subclass of the more general Conv class and is designed to handle input data with two spatial dimensions (e.g., image data).\n\nArgs:\n    filters: Integer, the number of convolution filters to use. This is the number of output channels.\n    kernel_size: Integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n    strides: Integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\n    padding: String, one of 'valid' or 'same'. Determines the type of padding algorithm to use.\n    data_format: String, one of 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs.\n    dilation_rate: Integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n    groups: Integer, the number of groups to divide the input and output channels into, for grouped convolution.\n    activation: String or callable, the activation function to use. Set it to None for linear activation.\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: String or callable, the initializer to use for the kernel weights matrix.\n    bias_initializer: String or callable, the initializer to use for the bias vector.\n    kernel_regularizer: String or callable, the regularizer to use for the kernel weights matrix.\n    bias_regularizer: String or callable, the regularizer to use for the bias vector.\n    activity_regularizer: String or callable, the regularizer to use for the layer's output.\n    kernel_constraint: String or callable, the constraint to use for the kernel weights matrix.\n    bias_constraint: String or callable, the constraint to use for the bias vector.\n\nThe Conv2D layer applies a set of learnable filters to the input data, producing a feature map as output. It supports various options for padding, strides, and dilation rates to control the spatial dimensions of the output. The layer also allows for optional activation, bias, and regularization functions to further process the output.\n\"\"\"\n", "\"\"\"\nThis class defines a 2D transpose convolution layer, also known as a deconvolution layer, which is the inverse of a 2D convolution layer. It is used for upsampling in convolutional neural networks. The class inherits from the Conv2D class and overrides its methods to handle the specifics of transpose convolution.\n\nArgs:\n    filters: Integer, the number of output filters in the convolution.\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    output_padding: An integer or tuple/list of 2 integers, specifying the amount of padding along the height and width of the output tensor.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs.\n    dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n    activation: Activation function to use. If not specified, no activation is applied (linear activation).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the kernel weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the layer.\n    kernel_constraint: Constraint function applied to the kernel weights matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\nMethods:\n    build: Builds the layer, initializing weights and biases if necessary.\n    call: Computes the output of the layer for given inputs.\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    get_config: Returns the configuration of the layer.\n\nThis docstring provides a clear and concise overview of the Conv2DTranspose class, detailing its purpose, arguments, methods, and their functionalities. It is designed to be informative and user-friendly, providing a clear understanding of the class's functionality and usage.\n\"\"\"\n", "\"\"\"\nClass Conv3D\n\nThe Conv3D class represents a 3D convolution layer, which is a type of layer used in deep learning for processing 3D data. It is a subclass of the Conv class and inherits its properties and methods.\n\nArgs:\n    filters: Integer, the dimensionality of the output space, i.e., the number of output filters in the convolution.\n    kernel_size: An integer or tuple/list of 3 integers, specifying the depth, height, and width of the 3D convolution window.\n    strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along the depth, height, and width.\n    padding: String, one of 'valid' or 'same', indicating the type of padding to be used.\n    data_format: String, one of 'channels_last' or 'channels_first', specifying the ordering of the dimensions in the input data.\n    dilation_rate: An integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution.\n    groups: Integer, the number of groups to divide the input into when applying convolution.\n    activation: String, the activation function to use. If None, no activation is applied.\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: String, specifying the initializer for the kernel weights matrix.\n    bias_initializer: String, specifying the initializer for the bias vector.\n    kernel_regularizer: String, specifying the regularizer function applied to the kernel weights matrix.\n    bias_regularizer: String, specifying the regularizer function applied to the bias vector.\n    activity_regularizer: String, specifying the regularizer function applied to the layer's output.\n    kernel_constraint: String, specifying the constraint function applied to the kernel weights matrix.\n    bias_constraint: String, specifying the constraint function applied to the bias vector.\n\nThe Conv3D layer performs a 3D convolution operation on the input tensor, with optional bias and an activation function.\n\"\"\"\n", "```\nclass Conv3DTranspose(Conv3D):\n    \"\"\"\n    Conv3DTranspose is a subclass of Conv3D that implements a 3D transpose convolution layer.\n    This layer is the counterpart to Conv3D and is used to upscale the input volume.\n\n    Args:\n        filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 3 integers, specifying the height, width, and depth of the 3D convolution window.\n        strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along the height, width, and depth.\n        padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n        output_padding: An integer or tuple/list of 3 integers, specifying the amount of padding along the height, width, and depth.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n        dilation_rate: An integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution.\n        activation: Activation function to use. If None, no activation is applied.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the kernel weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n        kernel_constraint: Constraint function applied to the kernel weights matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    Raises:\n        ValueError: If the `strides` or `dilation_rate` argument value is not a tuple or integer, or if the `padding` argument value is not a string.\n\n    Attributes:\n        output_padding: An integer or tuple/list of 3 integers, specifying the amount of padding along the height, width, and depth.\n    \"\"\"\n```\n", "\"\"\"\nClass for cropping one-dimensional data.\n\nThis class is a layer that is used to remove a specified number of elements from the beginning and end of one-dimensional data, such as time-series data. The cropping is applied along the time dimension (the second dimension of the input data).\n\nThe amount of cropping can be specified by passing a tuple of two integers to the `Cropping1D` constructor. The first integer in the tuple represents the number of elements to be cropped from the start of the sequence, and the second integer represents the number of elements to be cropped from the end of the sequence.\n\nAttributes:\n    cropping (tuple): A tuple of two integers defining the number of elements to be cropped from the start and end of the sequence, respectively.\n\nMethods:\n    __init__(self, cropping=(1, 1), **kwargs):\n        Initializes the Cropping1D layer with the specified cropping values and any additional keyword arguments.\n\n    compute_output_shape(self, input_shape):\n        Computes the output shape of the layer given the input shape. The output shape is derived by subtracting the cropping values from the input shape along the time dimension.\n\n    call(self, inputs):\n        Applies the cropping to the input data. The cropping is performed by slicing the input data along the time dimension.\n\n    get_config(self):\n        Returns the configuration of the layer as a dictionary. This includes the cropping values.\n\"\"\"\n", "\"\"\"\nClass for cropping 2D layers in a neural network.\n\nThis class is a custom layer for Keras that allows for cropping of 2D input data. The cropping can be specified in a symmetric or asymmetric manner for both height and width dimensions. The class supports both 'channels_first' and 'channels_last' data formats.\n\nAttributes:\n    cropping: A tuple of tuples specifying the amount of cropping to apply to the height and width dimensions. Each inner tuple should contain two integers, where the first integer is the amount of symmetric cropping to apply from the top/left side, and the second integer is the amount of symmetric cropping to apply from the bottom/right side. Alternatively, it can be an integer to apply symmetric cropping in all dimensions.\n    data_format: A string, one of 'channels_first' or 'channels_last', specifying the ordering of the dimensions in the input data.\n\nMethods:\n    compute_output_shape: Computes the output shape of the layer given the input shape.\n    call: Applies the cropping to the input data.\n    get_config: Returns the configuration of the layer.\n\"\"\"\n", "```python\nclass Cropping3D(Layer):\n    \"\"\"\n    A Keras layer that performs 3D cropping on the input tensor along the depth, height, and width dimensions.\n\n    The cropping can be specified in several ways:\n    - As a single integer: Symmetric cropping is applied to all dimensions.\n    - As a tuple of three integers: Individual cropping values are applied to the depth, height, and width dimensions.\n    - As a tuple of three tuples of two integers: Different amounts of cropping can be specified for the left and right sides of each dimension.\n\n    This layer is particularly useful when dealing with padded data, allowing you to remove the padding before feeding the data into a neural network.\n\n    # Arguments\n        cropping: Integer, tuple of 3 integers, or tuple of 3 tuples of 2 integers.\n            - Integer: Symmetric cropping is applied to all dimensions.\n            - Tuple of 3 integers: Specific cropping values are applied to depth, height, and width.\n            - Tuple of 3 tuples of 2 integers: Different amounts of cropping can be specified for the left and right sides of each dimension.\n        data_format: String, one of `'channels_last'` (default) or `'channels_first'`.\n            The ordering of the dimensions in the inputs. `'channels_last'` corresponds to inputs with shape\n            `(batch, depth, height, width, channels)` while `'channels_first'` corresponds to inputs with shape\n            `(batch, channels, depth, height, width)`. It defaults to the `image_data_format` value found in your\n            Keras config file.\n\n    # Input shape\n        - If `data_format='channels_first'`, the input tensor should have the shape\n          `(batch_size, channels, depth, height, width)`.\n        - If `data_format='channels_last'`, the input tensor should have the shape\n          `(batch_size, depth, height, width, channels)`.\n\n    # Output shape\n        The output shape is calculated based on the input shape and the applied cropping. If `data_format='channels_first'`,\n        the output shape will be `(batch_size, channels, adjusted_depth, adjusted_height, adjusted_width)`.\n        If `data_format='channels_last'`, the output shape will be `(batch_size, adjusted_depth, adjusted_height, adjusted_width, channels)`,\n        where `adjusted_depth = depth - 2 * cropping_depth`, `adjusted_height = height - 2 * cropping_height`,\n        and `adjusted_width = width - 2 * cropping_width`.\n    \"\"\"\n```\n", "\"\"\"\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm. It is used for spatial data analysis and can discover clusters of various shapes and sizes.\n\nThe `DBSCAN` class in this implementation is a scikit-learn estimator, adhering to the scikit-learn API. It is initialized with a set of parameters that control its behavior.\n\nParameters:\n- `eps` (float): The maximum distance between two samples for them to be considered as in the same neighborhood.\n- `min_samples` (int): The number of samples in a neighborhood for a point to be considered as a core point.\n- `metric` (str or callable): The distance metric to use. If 'precomputed', the training data is expected to be a distance matrix.\n- `metric_params` (dict, optional): Additional keyword arguments for the metric function.\n- `algorithm` (str): The algorithm to use for nearest neighbors search.\n- `leaf_size` (int): Leaf size passed to BallTree or KDTree.\n- `p` (float, optional): The power of the Minkowski metric.\n- `n_jobs` (int, optional): The number of parallel jobs to run for neighbors search.\n\nThe `fit` method fits the DBSCAN model to the input data. It computes the neighborhoods of each point and then applies the DBSCAN algorithm to assign labels to the points.\n\nThe `fit_predict` method combines the `fit` and `predict` steps into one, fitting the model and then returning the cluster labels.\n\nThe `_more_tags` method returns a dictionary of tags that can be used to define additional properties of the estimator. In this case, it indicates that the estimator supports pairwise computation if the metric is set to 'precomputed'.\n\"\"\"\n", "\"\"\"\nDepthwiseConv2D Layer\n\nThe `DepthwiseConv2D` layer is a specialized type of convolution layer that applies a different set of filters to each input channel, resulting in a number of output channels equal to the number of input channels multiplied by the depth multiplier. This is a key feature of depthwise convolution, which is used to reduce computational complexity and increase model efficiency.\n\n**Arguments:**\n- `kernel_size`: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n- `strides`: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\n- `padding`: One of `\"valid\"` or `\"same\"` (case-insensitive), indicating the type of padding to be used.\n- `depth_multiplier`: The number of depthwise convolution output channels for each input channel.\n- `data_format`: A string, one of `channels_last` (default) or `channels_first`, specifying the ordering of the dimensions in the input data.\n- `dilation_rate`: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n- `activation`: Activation function to use. If not specified, no activation is applied (linear activation).\n- `use_bias`: Boolean, whether the layer uses a bias vector.\n- `depthwise_initializer`: Initializer for the depthwise kernel weights matrix.\n- `bias_initializer`: Initializer for the bias vector.\n- `depthwise_regularizer`: Regularizer function applied to the depthwise kernel weights matrix.\n- `bias_regularizer`: Regularizer function applied to the bias vector.\n- `activity_regularizer`: Regularizer function applied to the output of the layer.\n- `depthwise_constraint`: Constraint function applied to the depthwise kernel weights matrix.\n- `bias_constraint`: Constraint function applied to the bias vector.\n\n**Input shape:**\n4D tensor with shape: `(batch_size, channels, rows, cols)` if `data_format='channels_first'`, or `(batch_size, rows, cols, channels)` if `data_format='channels_last'`.\n\n**Output shape:**\n4D tensor with shape: `(batch_size, channels * depth_multiplier, new_rows, new_cols)` if `data_format='channels_first'`, or `(batch_size, new_rows, new_cols, channels * depth_multiplier)` if `data_format='channels_last'`, where `new_rows` and `new_cols` are calculated based on the original `rows` and `cols` with the given padding, kernel size, strides, and dilation rate.\n\"\"\"\n", "\"\"\"\nEmbedding Layer\n\nThe Embedding layer is a fundamental layer in deep learning models, particularly in natural language processing tasks. It is used to convert positive integer indices into dense vectors of fixed size, often representing words in a text corpus.\n\nArguments:\n- `input_dim`: Integer. This is the size of the vocabulary, which is the maximum integer index + 1.\n- `output_dim`: Integer. This is the size of the dense embedding vector for each word.\n- `embeddings_initializer`: Initializer for the `embeddings` weights matrix. Default is 'uniform'.\n- `embeddings_regularizer`: Regularizer function applied to the `embeddings` weights matrix. Default is None.\n- `activity_regularizer`: Regularizer function applied to the output of the layer (its \"activation\"). Default is None.\n- `embeddings_constraint`: Constraint function applied to the `embeddings` weights matrix. Default is None.\n- `mask_zero`: Boolean. Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. Default is False.\n- `input_length`: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed). Default is None.\n\nAttributes:\n- `embeddings`: The embeddings matrix.\n\nMethods:\n- `build`: Builds the layer.\n- `compute_mask`: Computes an output mask tensor.\n- `compute_output_shape`: Computes the output shape of the layer.\n- `call`: Calls the layer on new raw inputs.\n- `get_config`: Retrieves the configuration of the layer.\n\nThis layer is particularly useful in text processing tasks where words are represented as integers. It allows the model to understand the semantic meaning of words by mapping them to dense vectors.\n\"\"\"\n", "   bscl.sclclclcl.g.s.e.s.sgrucer.s.s.s.s.s.s.s.s.s.s.s.s's.sghs =e=0.s.s =sbt.s.s_sbscl'scl's.s's.s.s.s.s.s.s.sbs.s.sghx.s.s.scl.s.s.s's.s.sib's.s.s.\n\n\n\n   py.e.e.s.py.py.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.e.s.s.s.s.pyche.s.s.s.s.s.s.s.s.s:s.s.s.e.pybs.s.s.pybs.s.s.s.e.s.s.sibbsclba:sib.s.s.s.b.b.sib.s.s.\n\n\n   b.s.s.s.s.s.s.s.\n\n   bscl's.b.s.e.s.s.s.s.py.s.\n\n\ncl.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.b.s.s.0.s.s.   's.0.s.s.py.s.s.s.s.s.pyger.s.s.s.s.s.s.s.s.s.s.pse.s.s.s.s.s.s.s.s.t.sger_x.s's's.s.s.sbs.sbs's.s.s.s.t.b.s.t.sbs.tbs.s.s.bghsghsgh.tbsgh.s.s.s's's'x's.s.s.s.s.eula:pygh.s.s.e.x.sgh.s.s.s'e.s.s.s.sula's.cl's.sula:sula's.s.s.juretscl'xbr.s.eacyb.s.x.bacykbs.bisbs, \\t.b.s.t.e.b.0.e.s.s.x.s.s.0.x.pycer.e.s.s.bis.t.ts.s.x.re.s.s.s.s.x.s.ts.sula_x0x.siderbisghx.xula_turets.siderts.s.e.sider_sbsula0xbs.sbs.0x.turetsigen_x.xbs.0x.turex.e.xbisbisot.pytrurebisghx.s.torets.tothts.t.s'xoth00x'xbr'xbrbisbisbisghxigerbisbisbisbisbisger.wothx.b'xrubisghx.bothx's.sger'xbs.xbs.xbs0bothb'x.0x.turepyts'x'x's's'x's.0xoth\\x_tureb0x0xbtacer_xbturetracerpyts0x.torepytsacerpyta_turex.x.t.x.x.0x0x.x.x.x.x.xru.w.xru.xure.xcer_turexcerure_ture_turepycerpy0x_ture_ture_ture_tureb_ture_turetyra_ture_turetsurets_wuretsak_turetserture_turetaur_ture_ture.", "```\n\"\"\"\nFunctionTransformer is a class in sklearn that allows the application of arbitrary\nfunctions to both the data and the target during the model fitting process. It is a transformer that is used to modify the data before it is passed to a machine learning model.\n\nParameters:\n    func (callable, optional): The function to apply to the data. If None, the identity function is used.\n    inverse_func (callable, optional): The function to apply to the data to invert the transformation. If None, the identity function is used.\n    validate (boolean): Whether to validate the input data.\n    accept_sparse (boolean): Whether to accept sparse data.\n    check_inverse (boolean): Whether to check if the transformation and its inverse are consistent.\n    feature_names_out (callable or {'one-to-one'}, optional): The function to generate output feature names. If 'one-to-one', the input feature names are used. If callable, it should take two arguments: the function transformer and an array-like of input feature names.\n    kw_args (dict, optional): Additional arguments to pass to the function.\n    inv_kw_args (dict, optional): Additional arguments to pass to the inverse function.\n\nAttributes:\n    func (callable): The function applied to the data during the transformation.\n    inverse_func (callable): The function applied to the data to invert the transformation.\n    validate (boolean): Flag indicating whether input data should be validated.\n    accept_sparse (boolean): Flag indicating whether sparse data is accepted.\n    check_inverse (boolean): Flag indicating whether the transformation and its inverse should be checked for consistency.\n    feature_names_out (callable or {'one-to-one'}): The function to generate output feature names.\n    kw_args (dict): Additional arguments passed to the function during transformation.\n    inv_kw_args (dict): Additional arguments passed to the inverse function.\n\nMethods:\n    fit(X, y=None): Fit the transformer to the data.\n    transform(X): Apply the transformation to the data.\n    inverse_transform(X): Apply the inverse transformation to the data.\n    get_feature_names_out(input_features=None): Get the output feature names.\n    _transform(X, func=None, kw_args=None): Apply the transformation to the data.\n    __sklearn_is_fitted__(): Check if the transformer is fitted.\n    _more_tags(): Additional tags for the transformer.\n    set_output(transform=None): Set the output type of the transformer.\n\"\"\"\n```\n", "\"\"\"\nThe `GaussianMixture` class is a subclass of `BaseMixture` and is used for modeling data using a mixture of Gaussian distributions. It provides functionalities for initializing, training, and using a Gaussian mixture model.\n\nAttributes:\n- `covariance_type`: A string that specifies the type of covariance matrices to use for each component. It can be 'full', 'tied', 'diag', or 'spherical'.\n- `weights_init`: Optional initial weights for the mixture components.\n- `means_init`: Optional initial means for the mixture components.\n- `precisions_init`: Optional initial precisions (inverse of the covariance matrices) for the mixture components.\n\nMethods:\n- `__init__`: Initializes the GaussianMixture instance with the specified parameters.\n- `_check_parameters`: Checks the validity of the provided parameters and initializes them if necessary.\n- `_initialize_parameters`: Initializes the parameters of the Gaussian mixture model.\n- `_initialize`: Initializes the weights, means, and precisions of the Gaussian mixture model.\n- `_m_step`: Performs the M-step of the EM algorithm to update the parameters of the Gaussian mixture model.\n- `_estimate_log_prob`: Estimates the log probability of the data given the Gaussian mixture model.\n- `_estimate_log_weights`: Estimates the log weights of the Gaussian mixture components.\n- `_compute_lower_bound`: Computes the lower bound on the log-likelihood.\n- `_get_parameters`: Returns the current parameters of the Gaussian mixture model.\n- `_set_parameters`: Sets the parameters of the Gaussian mixture model.\n- `_n_parameters`: Computes the number of parameters in the Gaussian mixture model.\n- `bic`: Computes the Bayesian Information Criterion for the model.\n- `aic`: Computes the Akaike Information Criterion for the model.\n\nThe class also inherits methods from `BaseMixture` for training the model, predicting the responsibilities of the data points to each component, and scoring the model's log-likelihood.\n\"\"\"\n", "\"\"\"\nGlobalAveragePooling1D class\n\nImplements a 1D global average pooling layer for 3D inputs. This layer computes the average of the width dimension for each channel in the input.\n\nMethods:\n\n    __init__(data_format='channels_last', **kwargs):\n        Initializes the GlobalAveragePooling1D layer with the specified data format.\n\n    call(inputs, mask=None):\n        Computes the average pool over the width dimension for each channel in the input. If a mask is provided, it is used to mask out certain elements before computing the average.\n\n    compute_mask(inputs, mask=None):\n        Does not affect the mask, returning None.\n\nAttributes:\n\n    supports_masking:\n        A boolean indicating whether this layer supports masking.\n\"\"\"\n", "\"\"\"\nGlobalAveragePooling2D is a subclass of GlobalPooling2D that applies average pooling over the spatial dimensions (height and width) of the input tensor. It is commonly used in convolutional neural networks (CNNs) to reduce the spatial dimensions before the final dense layer.\n\nAttributes:\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`. It specifies whether the input tensor should be viewed as (batch, height, width, channels) or (batch, channels, height, width).\n    keepdims (bool): If true, the spatial dimensions (height and width) are preserved with size 1.\n\nMethods:\n    call(inputs): This method is called when the layer is invoked at inference time. It computes the average of the input tensor across the spatial dimensions, effectively reducing the tensor from 4D to 2D.\n        inputs (tensor): A 4D tensor of shape (batch_size, height, width, channels) depending on `data_format`.\n        Returns: A 2D tensor of shape (batch_size, channels) after applying global average pooling.\n\"\"\"\n", "\"\"\"\nGlobalAveragePooling3D is a subclass of GlobalPooling3D, designed to perform global average pooling operations on 3D data.\n\nThe `call` method is responsible for executing the actual operation. It takes 'inputs', the data on which the operation will be performed, as an argument.\n\nThe method calculates the average over the spatial dimensions (height, width, depth) of the input data. If the data is in 'channels_last' format, the dimensions 1, 2, and 3 are reduced (axis=[1, 2, 3]). Conversely, if the data is in 'channels_first' format, the dimensions 2, 3, and 4 are reduced (axis=[2, 3, 4]).\n\nThe 'keepdims' parameter is set to True, ensuring that the dimensions that are reduced are maintained in the output. This means the output will have the same number of dimensions as the input, maintaining the batch size and the number of channels.\n\"\"\"\n", "\"\"\"\nGlobalMaxPooling1D is a class that implements a global max pooling operation for one-dimensional temporal data.\n\nThis class is designed to perform global max pooling, which is a pooling operation that downsamples the input by taking the maximum value over the entire temporal dimension. It is particularly useful in convolutional neural networks (CNNs) for reducing the spatial size of the output before passing the data to the next layer.\n\nMethods:\n    call(inputs): This method executes the global max pooling operation on the input data. It takes the input tensor and returns a tensor of the same rank as the input but with the temporal dimension reduced to 1.\n\nAttributes:\n    data_format (str): A string, one of 'channels_last' (default) or 'channels_first'. It specifies the ordering of the dimensions in the input tensor.\n    keepdims (bool): If True, the reduced dimensions are retained with length 1. If False, the dimensions are removed.\n\"\"\"\n", "\"\"\"\nGlobalMaxPooling2D is a class that performs 2D global max pooling on input data.\n\nThis class inherits from the GlobalPooling2D base class and overrides the `call` method to implement the specific operation of max pooling across the spatial dimensions of the input tensor. The `call` method takes an input tensor and applies the max pooling operation, reducing the spatial dimensions (height and width) to a single value for each channel.\n\nThe axis parameter of the max function is determined by the `data_format` attribute. If `data_format` is set to 'channels_last', the max operation is performed across the last two dimensions (height and width). Conversely, if `data_format` is 'channels_first', the operation is performed across the first two dimensions (height and width).\n\nThe `keepdims` parameter of the max function is set to the value of the `keepdims` attribute of the class. If `keepdims` is True, the output tensor will maintain the same number of dimensions as the input tensor, with singleton dimensions inserted after the spatial dimensions. This means that the output will have the shape (batch_size, channels, 1, 1) if `data_format` is 'channels_last', or (batch_size, 1, 1, channels) if `data_format` is 'channels_first'.\n\nIn summary, this class provides a way to globally pool the maximum values from each spatial location in the input tensor, reducing the spatial dimensions while preserving the channel information.\n\"\"\"\n", "\"\"\"\nGlobalMaxPooling3D is a subclass of GlobalPooling3D, designed to perform down-sampling by taking the maximum value over the spatial dimensions (height, width, and depth).\n\nThe `call` method is responsible for applying the max pooling operation to the input tensor. It takes the input tensor as an argument and determines the axis for pooling based on the `data_format`. If `data_format` is set to 'channels_last', the method pools over the dimensions corresponding to the last three axes (height, width, and depth). Conversely, if `data_format` is 'channels_first', the method pools over the dimensions corresponding to the first three axes (channels, height, width).\n\nThe `keepdims` parameter is set to True, ensuring that the output tensor maintains the same number of dimensions as the input tensor after pooling.\n\"\"\"\n", "\"\"\"\nGlobalPooling1D is a Keras layer that performs global pooling over the temporal dimension of 3D inputs.\n\nThis layer is designed to reduce the spatial dimensions of the input by taking the maximum value over the entire temporal dimension. The output shape is determined by the 'keepdims' parameter: if it's True, the layer will return an output with the same number of features but with the temporal dimension reduced to 1; if False, the temporal dimension is completely removed.\n\nInitialization parameters:\n- data_format: A string, either 'channels_last' (default) or 'channels_first'. This parameter specifies the ordering of the dimensions in the input data. 'channels_last' corresponds to inputs with shape (batch_size, steps, features), while 'channels_first' corresponds to inputs with shape (batch_size, features, steps).\n- keepdims: A boolean value, indicating whether to keep the temporal dimension in the output.\n\nThe compute_output_shape method calculates the output shape based on the input shape and the data_format and keepdims parameters.\n\nThe call method is where the actual computation happens, but it is currently just raising a NotImplementedError, as it needs to be implemented by the subclass.\n\nThe get_config method returns the configuration dictionary for the layer, including the data_format and keepdims parameters.\n\"\"\"\n", "\"\"\"\nClass GlobalPooling2D\n\nThis class represents a 2D global pooling layer for 4D inputs (batch_size, height, width, channels).\nIt is a subclass of `Layer` and is used to reduce the spatial dimensions of the input by taking the maximum or average value over the height and width.\n\nMethods:\n\n    __init__(data_format=None, keepdims=False, **kwargs):\n        Initializes the GlobalPooling2D layer.\n        - data_format: A string, one of 'channels_last' (default) or 'channels_first'.\n        - keepdims: A boolean, whether to keep the spatial dimensions or not. Default is False.\n        - **kwargs: Additional keyword arguments.\n\n    compute_output_shape(input_shape):\n        Computes the output shape of the layer given the input shape.\n        - input_shape: A list of integers, the input shape in the form (batch_size, height, width, channels).\n\n    call(inputs):\n        Raises a NotImplementedError. This method should be implemented by subclasses.\n\n    get_config():\n        Returns the configuration of the layer as a dictionary.\n\n\"\"\"\n", "\"\"\"\nClass GlobalPooling3D\n\nThis class represents a 3D global pooling layer for 5D input tensors. The global pooling operation takes the maximum or average value of the entire tensor.\n\nAttributes:\n    data_format (str): A string, one of 'channels_last' (default) or 'channels_first'. \n        Defines the ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape \n        '(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)' while 'channels_first' corresponds to \n        inputs with shape '(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)'.\n    keepdims (bool): If True, the spatial dimensions are preserved with size 1.\n\nMethods:\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    call(inputs): This method should be overridden when subclassing the Layer class. It is used to define \n        the computation graph of the layer. In this case, it raises a NotImplementedError because the \n        actual implementation of the pooling operation is not provided.\n    get_config(): Returns the configuration of the layer.\n\"\"\"\n", "\"\"\"\nGroupTimeSeriesSplit is a custom cross-validation iterator for time series data. It is designed to handle data where each group represents a unique time series, and it generates a sequence of train/test splits based on specified parameters.\n\nThe iterator is particularly useful for time series forecasting tasks where the order of data points is important. It allows for the creation of a rolling or expanding window for training and testing, with a specified gap between the training and testing sets.\n\nParameters:\n    test_size (int): The size of the test set in each split.\n    train_size (int, optional): The size of the training set in each split. If not provided, it is calculated based on the number of splits and the window type.\n    n_splits (int, optional): The number of splits to generate. If not provided, it is calculated based on the training size and the window type.\n    gap_size (int): The size of the gap between the training and testing sets in each split.\n    shift_size (int): The size of the shift for each subsequent split.\n    window_type (str): The type of window to use for the cross-validation. Must be either 'rolling' or 'expanding'.\n\nMethods:\n    split(X, y=None, groups=None): Generate indices to split data into training and test sets.\n    get_n_splits(X=None, y=None, groups=None): Return the number of splitting iterations in the cross-validator.\n    _calculate_split_params(): Calculate the parameters for the split.\n\nRaises:\n    ValueError: If neither train_size nor n_splits is specified, or if the window type is not 'rolling' or 'expanding', or if the groups are not specified, or if the groups are not consecutive.\n\nNote:\n    The groups parameter must be a sequence of unique identifiers for each time series group. The groups must be consecutive to ensure proper splitting of the data.\n\"\"\"\n```python\nclass GroupTimeSeriesSplit:\n    \n    def __init__(self, test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling'):\n        # ... (rest of the __init__ method)\n\n    def split(self, X, y=None, groups=None):\n        # ... (rest of the split method)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        # ... (rest of the get_n_splits method)\n\n    def _calculate_split_params(self):\n        # ... (rest of the _calculate_split_params method)\n```\n```\n", "\"\"\"\nKMeans is a class that implements the K-Means clustering algorithm.\n\nThe K-Means algorithm is an unsupervised learning algorithm used for partitioning a given dataset into a specified number (k) of clusters. Each observation belongs to the cluster with the nearest mean, which is recomputed based on the new members.\n\nAttributes:\n    k (int): The number of clusters to form.\n    max_iter (int): Maximum number of iterations allowed for the algorithm to converge.\n    convergence_tolerance (float): Relative tolerance with regards to Frobenius norm of the difference in the centroids for convergence.\n    random_seed (int): Random seed for the random initialization of centroids.\n    print_progress (int): If > 0, the algorithm will print progress information during fitting.\n    _is_fitted (bool): Indicates whether the model has been fitted with data.\n\nMethods:\n    __init__: Initializes the KMeans object with the specified parameters.\n    _fit: Fits the model to the input data by performing the K-Means clustering algorithm.\n    _get_cluster_idx: Helper method to get the cluster index for each data point.\n    _predict: Predicts the cluster index for each data point in the input data.\n\"\"\"\n", "\"\"\"\nLabelBinarizer is a scikit-learn transformer that converts multiclass integer labels into binary format. It is used for multilabel classification tasks.\n\nParameters:\n- neg_label (int, optional): The label to represent as -1 (default is 0).\n- pos_label (int, optional): The label to represent as 1 (default is 1).\n- sparse_output (bool, optional): Whether to return a sparse matrix or a dense matrix (default is False).\n\nMethods:\n- `fit(y)`: Fits the LabelBinarizer to the multiclass labels in `y`. Ensures the `neg_label` is less than the `pos_label`, checks `sparse_output` compatibility, and verifies `y` has samples. Identifies unique classes in `y`.\n- `fit_transform(y)`: Fits the LabelBinarizer to `y` and then transforms `y` into a binary matrix.\n- `transform(y)`: Transforms the multiclass labels in `y` into a binary matrix.\n- `inverse_transform(Y, threshold=None)`: Transforms the binary matrix `Y` back into multiclass labels. If `threshold` is not provided, it defaults to `(neg_label + pos_label) / 2.0`.\n\nThe class also provides a `_more_tags` method that returns a dictionary of tags indicating the type of input data the class can handle.\n\"\"\"\n", "\"\"\"\nLabelEncoder is a class used for encoding categorical labels into numerical values. It is a part of the sklearn library and is used in machine learning algorithms that require numerical input.\n\nMethods:\n- `fit(self, y)`: This method fits the LabelEncoder to the data by identifying unique classes.\n- `fit_transform(self, y)`: This method fits the LabelEncoder to the data and then transforms it into numerical values.\n- `transform(self, y)`: This method transforms the data by encoding the categorical labels into integers.\n- `inverse_transform(self, y)`: This method performs the inverse transformation, converting the encoded integers back into their original categorical labels.\n- `_more_tags(self)`: This method returns a dictionary of tags describing the input and output types of the LabelEncoder.\n\nNote: The LabelEncoder is not suitable for regression tasks and will raise an error if used for such tasks.\n\"\"\"\n", "\"\"\"\nA class for Linear Regression, offering different methods for fitting the model.\n\nMethods:\n- Direct: Solves for the weights using the normal equation.\n- SGD: Optimizes parameters using Stochastic Gradient Descent.\n- QR: Computes weights using QR decomposition.\n- SVD: Determines weights using Singular Value Decomposition.\n\nAttributes:\n- eta: Learning rate (between 0.0 and 1.0)\n- epochs: Number of passes over the entire dataset (iterations)\n- minibatches: Size of minibatches for SGD\n- random_seed: Seed for the random number generator for reproducibility\n- print_progress: Prints progress during training\n- _is_fitted: Boolean indicating if the model has been fitted\n- method: The method to use for fitting the model\n\nMethods:\n- _fit: Trains the model on the data\n- _normal_equation: Calculates weights using the normal equation\n- _net_input: Computes the net input\n- _predict: Makes a prediction\n- _sum_squared_error_cost: Calculates the sum of squared errors cost\n\"\"\"\n", "\"\"\"\nLogistic Regression Class\n\nImplements a Logistic Regression model for binary classification tasks. The model uses gradient descent to minimize the log-loss cost function, with optional L2 regularization.\n\nAttributes:\n    eta (float): Learning rate for gradient descent, controlling the step size during optimization.\n    epochs (int): Number of iterations over the entire dataset, determining the number of passes through the training data.\n    l2_lambda (float): Regularization parameter for L2 regularization, controlling the strength of the regularization term.\n    minibatches (int): Number of minibatches for gradient descent, determining the number of smaller batches of the data to use for each step.\n    random_seed (int): Seed for random number generation, for reproducibility of results.\n    print_progress (int): Print progress every n epochs, controlling the verbosity of the training process.\n    _is_fitted (bool): Indicates whether the model has been fitted to data, used to check if the model is ready for prediction.\n\nMethods:\n    __init__: Initializes the Logistic Regression model with specified parameters.\n    _forward: Computes the forward pass of the model, predicting the output probabilities.\n    _backward: Computes the gradient of the loss function with respect to the model parameters.\n    _fit: Trains the model on the given data, updating the model parameters to minimize the cost function.\n    _predict: Makes predictions on new data, converting output probabilities to binary class labels.\n    _net_input: Computes the net input to the activation function.\n    predict_proba: Returns the probability estimates for the test data X.\n    _logit_cost: Computes the log-loss cost function, the objective function to be minimized during training.\n    _sigmoid_activation: Applies the sigmoid activation function to the net input.\n\"\"\"\n", "\"\"\"\nA base class for defining loss functions in Keras, providing a framework for creating custom loss functions.\n\nThis class handles the conversion of input tensors to the appropriate data type, the application of masks, and the reduction of losses according to the specified reduction strategy.\n\n**Methods:**\n- `__init__(name=None, reduction='sum_over_batch_size', dtype=None)`: Initializes the loss function with an optional name, reduction strategy, and data type.\n- `dtype`: Returns the data type of the loss function.\n- `__call__(y_true, y_pred, sample_weight=None)`: Computes the loss given the true and predicted values, and optionally a sample weight.\n- `call(y_true, y_pred)`: Abstract method to be implemented by subclasses to compute the loss.\n- `get_config()`: Returns the configuration of the loss function as a dictionary.\n- `from_config(config)`: Creates a new instance of the loss function from its configuration.\n- `_obj_type()`: Returns the object type of the loss function.\n\n**Attributes:**\n- `name`: The name of the loss function.\n- `reduction`: The strategy for reducing the loss values.\n- `_dtype_policy`: The data type policy for the loss function.\n- `_dtype`: The data type of the loss function.\n\"\"\"\n", "\"\"\"\nMaxPooling1D is a subclass of Pooling1D, specialized for 1D max pooling operations.\n\nThe `__init__` method initializes a MaxPooling1D layer with the following parameters:\n\n- `pool_size`: An integer specifying the size of the window for max pooling. Default is 2.\n- `strides`: An integer or None specifying the stride of the pooling operation. If None, it defaults to `pool_size`.\n- `padding`: A string indicating the type of padding to be used. Can be 'valid' or 'same'. Default is 'valid'.\n- `data_format`: A string indicating the ordering of dimensions in the input data. Can be 'channels_last' or 'channels_first'. Default is 'channels_last'.\n\nThe constructor calls the constructor of the superclass (Pooling1D) with a partial function that applies 2D pooling using the 'max' mode.\n\"\"\"\n", "\"\"\"\nMaxPooling2D class is a subclass of Pooling2D, implementing 2D max pooling operation. It downsamples the input along its spatial dimensions (height and width) by taking the maximum value over the window defined by pool_size for each patch of input.\n\nArgs:\n    pool_size (tuple): Tuple of 2 integers, specifying the height and width of the pooling window. Default is (2, 2).\n    strides (tuple): Strides of the pooling operation. It is a tuple of 2 integers, specifying the strides along the height and width. If None, it will default to pool_size.\n    padding (str): Padding method. It can be 'valid' or 'same'. 'valid' means no padding, and 'same' means padding with zeros evenly to the left/right or up/down of the input so that the output has the same height and width as the input. Default is 'valid'.\n    data_format (str): The ordering of the dimensions in the inputs. It can be 'channels_last' or 'channels_first'. 'channels_last' corresponds to inputs with shape (batch, height, width, channels), while 'channels_first' corresponds to inputs with shape (batch, channels, height, width). Default is None, which means to use the backend default.\n\nAttributes:\n    pool_size (tuple): The size of the pooling window.\n    strides (tuple): The strides of the pooling operation.\n    padding (str): The padding method.\n    data_format (str): The ordering of the dimensions in the inputs.\n\"\"\"\n", "\"\"\"\nMaxPooling3D is a subclass of Pooling3D, designed for performing 3D max pooling operations.\n\nArgs:\n    pool_size (tuple): A tuple of three integers representing the size of the window for each dimension of the input tensor.\n    strides (tuple): A tuple of three integers representing the stride for the pooling operation.\n    padding (str): A string indicating the type of padding to be used. It can be 'valid' or 'same'.\n    data_format (str): A string indicating the ordering of the dimensions in the input tensor. It can be 'channels_last' or 'channels_first'.\n\nAttributes:\n    pool_size (tuple): A tuple of three integers representing the size of the window for each dimension of the input tensor.\n    strides (tuple): A tuple of three integers representing the stride for the pooling operation.\n    padding (str): A string indicating the type of padding to be used. It can be 'valid' or 'same'.\n    data_format (str): A string indicating the ordering of the dimensions in the input tensor. It can be 'channels_last' or 'channels_first'.\n\nMethods:\n    __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs):\n        Initializes the MaxPooling3D object with the specified parameters.\n\"\"\"\n", "\"\"\"\nThe `Metric` class in Keras is a base class for creating custom metrics for model evaluation. It provides a framework for tracking and updating the state of metrics during training and evaluation.\n\nMethods:\n- `__init__(dtype=None, name=None)`: Initializes the metric with an optional name and data type. It sets up the necessary variables and trackers.\n- `reset_state()`: Resets the state of the metric to its initial state, typically used at the start of each epoch or each batch.\n- `update_state(*args, **kwargs)`: Updates the state of the metric with new data. This method must be implemented by subclasses to define the specific logic for updating the metric.\n- `stateless_update_state(metric_variables, *args, **kwargs)`: Updates the state of the metric in a stateless manner using pre-computed variables.\n- `result()`: Returns the result of the metric computation. This method must be implemented by subclasses to define the specific logic for computing the metric.\n- `stateless_result(metric_variables)`: Returns the result of the metric computation in a stateless manner using pre-computed variables.\n- `stateless_reset_state()`: Resets the state of the metric in a stateless manner.\n- `dtype`: Returns the data type of the metric.\n- `add_variable(shape, initializer, dtype=None, aggregation='sum', name=None)`: Adds a new variable to the metric.\n- `add_weight(shape=(), initializer=None, dtype=None, name=None)`: A convenience method for adding a new variable to the metric.\n- `variables`: Returns the list of variables tracked by the metric.\n- `__call__(*args, **kwargs)`: Updates the state of the metric and returns its result.\n- `get_config()`: Returns the configuration of the metric.\n- `from_config(config)`: Creates a new instance of the metric from its configuration.\n- `__setattr__(name, value)`: Sets an attribute of the metric, tracking it if necessary.\n- `_check_super_called()`: Checks if the superclass's `__init__()` method has been called.\n- `__repr__()`: Returns a string representation of the metric.\n- `__str__()`: Returns a string representation of the metric.\n\nSubclasses of `Metric` should implement the `update_state` and `result` methods to define the specific behavior of the metric.\n\"\"\"\n", "```\n\"\"\"\nMultiLabelBinarizer\n\nThe `MultiLabelBinarizer` is a transformer used to convert multilabel classification labels into a binary format. It is particularly useful for handling datasets where each sample can have multiple labels.\n\nParameters:\n- `classes` (array-like, optional): A list of class labels to consider when binarizing the labels. If not provided, it will be inferred from the input data. Default is None.\n- `sparse_output` (bool, optional): If True, the output will be a sparse matrix. Otherwise, it will be a dense numpy array. Default is False.\n\nMethods:\n- `fit(y)`: Fit the transformer to the target data.\n- `fit_transform(y)`: Fit the transformer to the target data and transform it.\n- `transform(y)`: Transform the target data into binary format.\n- `inverse_transform(yt)`: Inverse transform the binary data back to the original format.\n\nAttributes:\n- `classes_`: The unique classes found in the data.\n\nNotes:\n- The target data is expected to be a list of labels for each sample.\n- The class labels should be hashable and comparable.\n- The transformer will raise a ValueError if duplicate class labels are provided.\n- The transformer will raise a ValueError if the binary data contains values other than 0 and 1.\n\"\"\"\n```\n", "\"\"\"\nThe `OneHotEncoder` class is a preprocessing step used in machine learning pipelines to convert categorical data into a format that can be provided to machine learning algorithms. It transforms the data into a binary format, where each category is represented as a separate binary column.\n\nParameters:\n- `categories` (default='auto'): Determines the categories to use for encoding. If 'auto', categories are determined by unique values in each column. If a list is provided, it should be a list of lists, with each sublist containing the categories for each column.\n- `sparse_output` (default=True): Whether to output sparse matrices. If False, the output will be a dense numpy array.\n- `dtype`: The data type of the output.\n- `handle_unknown`: Determines how to handle unknown categories during encoding. Options include 'error' (raise an error), 'ignore' (skip unknown categories), and 'infrequent_if_exist' (treat infrequent categories as unknown).\n- `min_frequency`: The minimum frequency required for a category to be considered frequent.\n- `max_categories`: The maximum number of categories to consider.\n- `feature_name_combiner`: Determines how to combine feature names and category names. Options include 'concat' (concatenate names with an underscore).\n\nMethods:\n- `fit(X, y=None)`:\n- `transform(X)`\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None)`:\n- `inverse_transform(X, y=None, y=None\n- `_transform(X, y=None`.\n- `inverse(X, y=None` and y=None`.\n-transformer`y=None`.\n-transformer(X, y=None` and, y=None`.\n-transformer(X, y=None`.\n-transformer(X, y=None`.\n-transformer(X, y=None.\n-transformer(X, y=None.\n-transformer(X, y=None`.\n-transformer(X, y=None.\ny, y=None, y=None, y=None`.y, y=None, y=None, y=None, y=None, y=None, y=None.\ny, y=None, y=None, y=None\ny, y=None, y=None\ny, y=None\ny, y=None, y=None, y=None,py, y=None\ny, y=None\ny, y, y, y,py.\ny, y.\ny, y, y, y,y, y, y, y, y_string_in_\ny, y, y, y, y, y, y, y, y, y, y, y, y, y,1,py, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y,\u300a\ny, _y, y,y, y, y, y,\u300averb\ny\ny\ny\ny\n\n\n\n\n\ny, y,\n\n    \n    None\ny,\ny, y,\n\n\n\n\n   \n\n\n   \n\n   \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nac,\n   \n   \n   \n   \n\n\n   \n    \n\n   \nval, y,\n   \n    \n\n   \n   \n   \n    \n    \n    \n    \n    \n   ero andkel.py, a real, a single, a and it.\n    \n    kel, andocann-in thenic, a,ac\n    \n    \n    \n    \n   \n   \n    \n", "\"\"\"\nOPTICS (Ordering Points To Identify the Clustering Structure) is an algorithm that builds a hierarchical clustering tree from a dataset. It is a density-based clustering method that identifies clusters based on the density of data points. This class implements the OPTICS algorithm and provides methods to fit the model to data and extract clustering information.\n\nParameters:\n    min_samples (int or float, optional): The number of samples in a neighborhood for a point to be considered as a core point.\n    max_eps (float, optional): The maximum value of eps to use in the algorithm.\n    metric (str or callable, optional): The distance metric to use. If a callable is passed, it is used to calculate the distance between instances.\n    p (int, optional): The parameter for the Minkowski metric.\n    metric_params (dict, optional): Additional keyword arguments for the metric function.\n    cluster_method (str, optional): The method to use for extracting clusters. Can be 'xi' or 'dbscan'.\n    eps (float, optional): The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n    xi (float, optional): The parameter for the extraction of clusters.\n    predecessor_correction (bool, optional): Whether to correct the predecessor.\n    min_cluster_size (int or float, optional): The minimum number of samples in a cluster.\n    algorithm (str, optional): The algorithm to use for finding nearest neighbors.\n    leaf_size (int, optional): A leaf node is represented as a cluster if it has less than this number of points.\n    memory (str or object with 'cache' method, optional): Location to store the precomputed tree.\n    n_jobs (int, optional): The number of parallel jobs to run for neighbors search.\n\nAttributes:\n    ordering_ (array): The order of the points in the reachability-plot.\n    core_distances_ (array): The core distances of the points.\n    reachability_ (array): The reachability distances of the points.\n    predecessor_ (array): The predecessors of the points.\n    labels_ (array): The cluster labels for each point.\n    cluster_hierarchy_ (array): The cluster hierarchy for each point.\n\nMethods:\n    fit(X, y=None): Fit the model to the data and compute the clustering structure.\n\"\"\"\n", "\"\"\"\nOrdinalEncoder is a feature encoding class that transforms categorical data into ordinal integers. It is a subclass of OneToOneFeatureMixin and _BaseEncoder, and is initialized with parameters such as categories, dtype, handle_unknown, unknown_value, encoded_missing_value, min_frequency, and max_categories.\n\nThe `fit` method is used to learn the categories of each feature and handle unknown categories according to the specified strategy. It also validates the dtype parameter and handles missing values.\n\nThe `transform` method encodes the input data by converting categories into ordinal integers, handling missing values, and unknown categories based on the parameters set during initialization.\n\nThe `inverse_transform` method converts the encoded data back into the original categorical format. It checks the number of columns in the input data and raises an error if it does not match the expected number of features.\n\nPlease note that the class uses private methods and attributes for its operations, such as _fit, _transform, _get_mask, and _missing_indices, which are not part of the public interface of the class and are not documented here.\n\"\"\"\n", "\"\"\"\nClass for 1D Pooling Layer\n\nThis class represents a 1D pooling layer for neural networks, which is a type of downsampling operation that reduces the spatial dimensions of the input tensor. It is a subclass of the `Layer` class from Keras, and it is designed to operate on temporal data.\n\nThe `Pooling1D` class applies a pooling function to the input tensor along its temporal dimension, reducing the temporal length while maintaining the spatial dimensions (e.g., width and height). The pooling operation is parameterized by the `pool_function`, `pool_size`, `strides`, `padding`, and `data_format` attributes.\n\nAttributes:\n    pool_function (function): The pooling function to be applied. It should accept the input tensor, pool size, strides, padding, and data format as parameters.\n    pool_size (tuple): The size of the pooling window along the temporal dimension.\n    strides (tuple): The stride of the pooling operation along the temporal dimension.\n    padding (str): The type of padding to be used ('valid' or 'same').\n    data_format (str): The ordering of the dimensions in the inputs ('channels_last' or 'channels_first').\n    input_spec (InputSpec): The specification of the input tensor, indicating the expected number of dimensions.\n\nMethods:\n    call(inputs): Performs the pooling operation on the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of the layer.\n\nThe `call` method applies the pooling function to the input tensor, and the `compute_output_shape` method calculates the output shape of the layer given the input shape. The `get_config` method returns the configuration of the layer, which includes the pooling parameters and the data format.\n\nThis class is useful for reducing the temporal complexity of the input data, helping to control overfitting and improve the generalization ability of the model.\n\"\"\"\n", "\"\"\"\nClass representing a 2D Pooling Layer in Neural Networks.\n\nThe `Pooling2D` class is a fundamental component of convolutional neural networks (CNNs) that reduces the spatial dimensions of the input feature maps. It is used to downsample the input, which helps in reducing the computational complexity, preventing overfitting, and extracting more robust features.\n\nThe `Pooling2D` layer applies a pooling operation to the input tensor, which is typically a 4D tensor representing a batch of images. The pooling operation is defined by the `pool_function` attribute, which should be a function that performs the pooling operation given the input tensor, kernel size, strides, padding, and data format.\n\nAttributes:\n    pool_function (function): The function to use for pooling. It should take the input tensor, kernel size, strides, padding, and data format as arguments.\n    pool_size (tuple): The size of the pooling window, specified as a tuple of two integers (rows, columns).\n    strides (tuple): The stride of the pooling operation, specified as a tuple of two integers (rows, columns). If None, it defaults to the pool_size.\n    padding (str): The padding method to use ('valid' or 'same'). Default is 'valid'.\n    data_format (str): The ordering of the dimensions in the input tensor. Can be 'channels_first' (batch_size, channels, height, width) or 'channels_last' (batch_size, height, width, channels). If None, it is inferred from the backend configuration.\n    input_spec (InputSpec): An InputSpec object that specifies the expected input shape for this layer.\n\nMethods:\n    call(inputs): Applies the pooling operation to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns the configuration of this layer as a dictionary.\n\nThis layer is particularly useful for reducing the spatial dimensions of the feature maps, which can help in making the network more efficient and reducing the risk of overfitting.\n\"\"\"\n", "\"\"\"\nClass for 3D Pooling Layer in Keras.\n\nThis class defines a custom 3D pooling layer for use in Keras models. The layer performs a pooling operation on the input tensor, reducing its spatial dimensions.\n\nAttributes:\n    pool_function (function): The pooling function to be applied. It should accept the input tensor, kernel size, strides, and padding as parameters.\n    pool_size (tuple): The size of the pooling window along each dimension of the input tensor.\n    strides (tuple): The stride length for the pooling operation along each dimension.\n    padding (str): The type of padding to be applied ('valid' or 'same').\n    data_format (str): The ordering of dimensions in the input tensor ('channels_first' or 'channels_last').\n    input_spec (InputSpec): The specification of the input tensor shape.\n\nMethods:\n    call(inputs): Performs the pooling operation on the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer for a given input shape.\n    get_config(): Returns the configuration of the layer.\n\"\"\"\n", "\"\"\"\nPrincipal Component Analysis (PCA)\n\nPCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information in the large set. It achieves this by identifying the directions (principal components) in which the data varies the most.\n\nMethods:\n- fit(X, y=None):\n  Fit the model with the input data X.\n\n- transform(X):\n  Apply the fitted model to transform the input data X to a new lower-dimensional space.\n\n- _covariance_matrix(X):\n  Compute the covariance matrix of the input data X.\n\n- _decomposition(mat, n_samples):\n  Compute the eigenvalues and eigenvectors of the covariance matrix or use the specified solver to get the decomposition.\n\n- _loadings():\n  Compute the loadings of the principal components based on the eigenvectors and eigenvalues.\n\n- _projection_matrix(eig_vals, eig_vecs, whitening, n_components):\n  Construct the projection matrix using the eigenvectors and optionally whiten the data.\n\nAttributes:\n- solver: The method used to compute the eigenvalues and eigenvectors, either 'eigen' for numerical methods or 'svd' for singular value decomposition.\n- n_components: The number of principal components to retain. Must be > 1 or None for all components.\n- _is_fitted: A boolean indicating whether the model has been fitted to data.\n- whitening: A boolean indicating whether to apply whitening to the transformed data.\n- e_vals_: The eigenvalues of the covariance matrix.\n- e_vecs_: The eigenvectors of the covariance matrix.\n- w_: The projection matrix derived from the eigenvectors.\n- e_vals_normalized_: The normalized eigenvalues.\n- loadings_: The loadings of the principal components, which are the eigenvectors scaled by their eigenvalues.\n\"\"\"\n", "\"\"\"\nRMSprop optimizer implementation.\n\nRMSprop is a popular optimization algorithm that addresses the problem of the\nvanishing or exploding gradients in gradient-based optimization algorithms.\nIt divides the gradient by an estimation of the root mean square (RMS) of the\ngradients, which helps to stabilize the learning process.\n\nThis implementation supports the inclusion of momentum, a technique that can\nenhance the convergence speed and stability of the optimization process.\n\nAn advanced version of RMSprop, known as RMSprop++, maintains a moving average\nof the squared gradients. This modification can improve the performance of the\noptimizer, especially in the presence of noisy gradients.\n\nArguments:\n    learning_rate: A `Tensor` or a floating point value. The learning rate\n        determines the step size during each iteration while moving towards\n        a minimum of a loss function.\n    rho: A `Tensor` or a floating point value. The decay rate for the moving\n        average of the squared gradients. It controls the bias-correction\n        term in the denominator of the update rule.\n    momentum: A `Tensor` or a floating point value. The momentum value for\n        the momentum optimization technique. It helps to speed up convergence\n        and can also act as a regularizer.\n    epsilon: A `Tensor` or a floating point value. A small constant for numerical\n        stability to avoid division by zero. It is added to the denominator\n        of the update rule to prevent division by zero errors.\n    centered: A boolean. If `True`, the centered version of RMSprop is used.\n        This version maintains a moving average of the squared gradients and\n        the squared gradient itself, which can improve the stability of the\n        optimization process.\n    name: Optional name for the operations.\n\n\"\"\"\n", "\"\"\"\nSelfTrainingClassifier\n\nThe SelfTrainingClassifier is a meta-estimator that iteratively trains a base estimator on new labels generated from the model's predictions. It is designed for semi-supervised learning scenarios where the dataset contains both labeled and unlabeled samples.\n\nParameters:\n-----------\nestimator : object, optional (default=None)\n    The base estimator to use for learning. If None, the `base_estimator` parameter is used instead.\n\nbase_estimator : object, optional (default='deprecated')\n    The base estimator to use for learning. This parameter is deprecated and will be removed in future versions. Use `estimator` instead.\n\nthreshold : float, optional (default=0.75)\n    The confidence threshold for deciding whether a prediction is correct. Predictions with probabilities above this threshold are considered correct.\n\ncriterion : str, optional (default='threshold')\n    The criterion to select unlabeled samples to label. Can be either 'threshold' or 'k_best'. If 'threshold', samples are labeled if their highest predicted probability is above the threshold. If 'k_best', the top 'k_best' samples with highest predicted probabilities are labeled.\n\nk_best : int, optional (default=10)\n    The number of unlabeled samples to label when `criterion` is set to 'k_best'.\n\nmax_iter : int, optional (default=10)\n    The maximum number of iterations to perform. If None, iterations will continue until all samples are labeled.\n\nverbose : bool, optional (default=False)\n    Whether to print out information about the progress of the self-training process.\n\nAttributes:\n-----------\nestimator_ : object\n    The fitted base estimator.\n\ntransduction_ : ndarray, shape (n_samples,)\n    The labels assigned to the samples, consisting of the true labels for labeled samples and the predicted labels for unlabeled samples.\n\nlabeled_iter_ : ndarray, shape (n_samples,)\n    The iteration number at which each sample was labeled.\n\nn_iter_ : int\n    The number of iterations performed.\n\ntermination_condition_ : str\n    The reason the self-training process terminated. Can be 'no_change', 'max_iter', or 'all_labeled'.\n\nclasses_ : ndarray, shape (n_classes,)\n    The classes found by the base estimator.\n\nMethods:\n--------\nfit(X, y)\n    Fit the model according to the given training data and labels.\n\npredict(X)\n    Perform classification on samples in X.\n\npredict_proba(X)\n    Return probability estimates for the test samples X.\n\ndecision_function(X)\n    Compute the decision function for samples in X.\n\npredict_log_proba(X)\n    Compute log probability estimates for the test samples X.\n\nscore(X, y)\n    Returns the mean accuracy on the given test data and labels.\n\nget_metadata_routing()\n    Returns the metadata routing for the estimator.\n\"\"\"\n", "\"\"\"\nSeparableConv Class\n\nThis class represents a custom separable convolution layer, a type of convolution layer that uses a depthwise convolution followed by a pointwise convolution. The depthwise convolution applies a different filter along each input channel, while the pointwise convolution combines the results of the depthwise convolution across channels. This architecture is more efficient and reduces the number of parameters compared to a standard convolution layer.\n\nAttributes:\n    rank (int): The rank (number of dimensions) of the input tensor.\n    filters (int): The number of output filters.\n    kernel_size (int or tuple): The size of the convolution kernel.\n    strides (int or tuple, optional): The stride of the convolution. Default is 1.\n    padding (str, optional): The type of padding to use. Default is 'valid'.\n    data_format (str, optional): The ordering of the dimensions in the input tensor. Default is None.\n    dilation_rate (int or tuple, optional): The rate at which to apply the dilation. Default is 1.\n    depth_multiplier (int, optional): The multiplier for the number of depthwise filters. Default is 1.\n    activation (str or callable, optional): The activation function to use. Default is None.\n    use_bias (bool, optional): Whether to use a bias vector. Default is True.\n    depthwise_initializer (str or Initializer, optional): Initializer for the depthwise kernel. Default is 'glorot_uniform'.\n    pointwise_initializer (str or Initializer, optional): Initializer for the pointwise kernel. Default is 'glorot_uniform'.\n    bias_initializer (str or Initializer, optional): Initializer for the bias vector. Default is 'zeros'.\n    depthwise_regularizer (str or Regularizer, optional): Regularizer for the depthwise kernel. Default is None.\n    pointwise_regularizer (str or Regularizer, optional): Regularizer for the pointwise kernel. Default is None.\n    bias_regularizer (str or Regularizer, optional): Regularizer for the bias vector. Default is None.\n    activity_regularizer (str or Regularizer, optional): Regularizer for the output of the layer. Default is None.\n    depthwise_constraint (str or Constraint, optional): Constraint for the depthwise kernel. Default is None.\n    pointwise_constraint (str or Constraint, optional): Constraint for the pointwise kernel. Default is None.\n    bias_constraint (str or Constraint, optional): Constraint for the bias vector. Default is None.\n    trainable (bool, optional): Whether the layer weights will be updated during training. Default is True.\n    name (str, optional): The name of the layer. Default is None.\n\nMethods:\n    build(input_shape): Constructs the layer according to the input shape.\n    call(inputs): Computes the output of the layer given the input tensor.\n    get_config(): Returns the configuration of the layer as a dictionary.\n\nNote:\n    This class is a subclass of Conv and inherits its properties and methods. The implementation of the depthwise and pointwise convolutions is left to the subclass.\n\"\"\"\n", "\"\"\"\nA class representing a 1D separable convolution layer, extending the functionality of the `SeparableConv` class.\n\nThe `SeparableConv1D` class is designed to perform 1D separable convolutions, which are a combination of depthwise convolution and pointwise convolution. Depthwise convolution applies the same filter across all input channels, while pointwise convolution combines the results of depthwise convolution across all channels.\n\nThis class is particularly useful for reducing the number of parameters in the model, which can lead to faster training times and potentially better generalization.\n\nArguments:\n    filters: Integer, the dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n    strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive), indicating the type of padding to be used.\n    data_format: A string, one of `channels_last` (default) or `channels_first`. It specifies the ordering of the dimensions in the input data.\n    dilation_rate: An integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution.\n    depth_multiplier: The number by which to multiply the depth of the input before applying the depthwise convolution.\n    activation: Activation function to use. If not specified, no activation is applied (linear activation).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel weights matrix.\n    pointwise_initializer: Initializer for the pointwise kernel weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to the depthwise kernel weights matrix.\n    pointwise_regularizer: Regularizer function applied to the pointwise kernel weights matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n    depthwise_constraint: Constraint function applied to the depthwise kernel weights matrix.\n    pointwise_constraint: Constraint function applied to the pointwise kernel weights matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\nMethods:\n    call: This method defines the computation performed at every call. It applies the necessary transformations to the input data, including padding, depthwise convolution, pointwise convolution, and bias addition (if enabled). The output is then optionally passed through an activation function before being returned.\n\"\"\"\n", "\"\"\"\nClass SeparableConv2D\n\nThe SeparableConv2D class represents a 2D separable convolution layer, which is a depthwise convolution followed by a pointwise convolution. This layer is used for efficient model building when the input tensor has many channels.\n\nAttributes:\n    filters: Integer, the dimensionality of the output space (i.e., the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\n    padding: String, one of 'valid' or 'same'.\n    data_format: String, one of 'channels_last' (default) or 'channels_first'.\n    dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution.\n    depth_multiplier: The number by which to multiply the depth of the input before applying depthwise convolution.\n    activation: String, activation function to use. Default is 'linear' (no activation).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: String, Initializer for the depthwise kernel weights matrix. Default is 'glorot_uniform'.\n    pointwise_initializer: String, Initializer for the pointwise kernel weights matrix. Default is 'glorot_uniform'.\n    bias_initializer: String, Initializer for the bias vector. Default is 'zeros'.\n    depthwise_regularizer: String, Regularizer function applied to the depthwise kernel weights matrix. Default is None.\n    pointwise_regularizer: String, Regularizer function applied to the pointwise kernel weights matrix. Default is None.\n    bias_regularizer: String, Regularizer function applied to the bias vector. Default is None.\n    activity_regularizer: String, Regularizer function applied to the output of the layer (its \"activation\"). Default is None.\n    depthwise_constraint: String, Constraint function applied to the depthwise kernel weights matrix. Default is None.\n    pointwise_constraint: String, Constraint function applied to the pointwise kernel weights matrix. Default is None.\n    bias_constraint: String, Constraint function applied to the bias vector. Default is None.\n\nMethods:\n    call(inputs): This method defines the computation performed at every call. It takes input data, performs the separable convolution, and applies the activation function (if specified).\n\"\"\"\n", "\n\n\n\n\nd.sgree.sr.sr\n\n\n\n\n\n\n\n\n\nsr.sr.styref.0.s\n'r.k.score.score.parse_s\n\nsr.srasc.selector.rascriref.1.sr_score_ref.r.ref.selector.nested_selector.selector_scoreref.co.sr.sraging\n\n\n\n\n\n\n\n\n\n       sr.sr.sr.score_s.\n\n       asc.\n\n\n\n\n\n\n\n       sr.nested_scoring_scoreref.ref.sraging_dribriref.selectorrefend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsr.s.1.clasc.s.score_nested_score\n\n\n\n\n\n\n\n\n\nx.sgri.\n\n       \n\n\n\n\n\n\n\n\n\n\n       \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       \n\n\n\n\n       s.drib\n        self.self.\n\nself.self.sdri.sdatt`\n       sg.pyndrib self\n\n\n\n\n\n\n\nra`\n\n\n\nri.r.self.self.cf.cf.self.sr.sr\n\n\nself\n\n\n\n\n\nself\nselfribsr.sdribself selfself_sparghri,\n       self.sr'sr_self.\n\n\nsr_cribri-self.sr_nested_s_spar_sparrib_selector.sr`gh_nested_s.sr.sr.sribs.srascriatt:\n       \n       sr's,\n        sribri.nested.sr.srri_s.py's.sgri.sgribri.self.sr.sr.sr's.sr.nested`sr.reuler.s.d\n\n\n       r.d\nr.s.sr.1.sg.sg.sg.nested.sg.nested.\n\n\n\nre.s.re.re.s.sr.sg.s.s.s.re.s.re.re.s.s.s_s.s.re.s.s_s.s_s.s.sribs`sg_sribd`s\nd's_sri's_reasc_1xribr'd_sribs_s_scri_sri`s.r's_1d.r.d.d'd's.sg.sg.sg_scri.sgribod_1stribc.cribd.d.d.d.v.d.sr.sg_c.s.s.drib_patcher_1.sr.sg.sg.sc.sr.sr.co.self.sgog,\ncl.s.coresh.\n        s.sg. self.self.reuler.s.re.sg.s.re.re.re.re.c.c.re.sg.re.re.re.s.self.sg.re.re.self.sg.s.sg_re.sr.s.s.s.self.s.sr's.s.s.sr's.p.s.sr.sr.sr.self.sr.s.d.\n\n.s.s.s.s.re.sg.\n        self.\n       \n       re.d.r.sc.sc.sr.sg.sg_score_reuler_sc.\n       self.self,\n       \n       self.\n       self.self.\n       \n       re're's.ref.s.s.sg.c.reuler.sg.s.s.sg.s.s.re.s.s.re.c.sg.s.c.s.sc.s.sr.s.c.s.s.c.s.s.sg.sg.sr.s.sg.c.p.s.s.s.c.s.sg.s.s.s.sg.re.sg.sg.1.sg.d.sr.sr.s.re.sr.d.sr.s.sg.re.s.sg.s.s.s.sg.s.s.sg.s.s.self.s.s.c.s.sg.sg", "\"\"\"\nStochastic Gradient Descent (SGD) optimizer with support for momentum, learning rate decay, and Nesterov momentum.\n\nThis class implements the SGD optimization algorithm with momentum and learning rate decay. It can be used as a drop-in replacement for the `tf.keras.optimizers.SGD` class.\n\nArgs:\n    learning_rate: A `Tensor`, floating point value, or a schedule that is a `LearningRateSchedule`. The learning rate to use. Defaults to 0.01.\n    momentum: A `Tensor`, floating point value, or a schedule that is a `LearningRateSchedule`. The momentum to use. Can be a constant value or a callable that takes no arguments and returns the actual value to use. The value should be between 0 and 1. If set to 0, the optimizer will behave like standard SGD. Defaults to 0.0.\n    nesterov: Boolean. Whether to apply Nesterov momentum. Defaults to False.\n    name: String. The name of the optimizer.\n\nRaises:\n    ValueError: If the momentum value is not between 0 and 1.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD: Boolean. Whether this optimizer has the ability to aggregate gradients.\n\"\"\"\n", "\"\"\"\nSoftmax Regression Class\n\nImplements a Softmax Regression model for multi-class classification problems. It is a generalization of logistic regression and is used to predict the probability of each class label for input data points.\n\nMethods:\n- __init__: Initializes the Softmax Regression model with specified learning rate, epochs, L2 regularization, minibatches, number of classes, random seed, and print progress.\n- _net_input: Computes the net input for a given input data.\n- _softmax_activation: Applies the softmax function to the net input to get the class probabilities.\n- _cross_entropy: Computes the cross-entropy loss for the given class probabilities and target values.\n- _cost: Computes the cost function, which is the average of the cross-entropy loss plus the L2 regularization term.\n- _to_classlabels: Converts the class probabilities to class labels.\n- _forward: Performs the forward pass to compute the class probabilities.\n- _backward: Performs the backward pass to compute the gradients of the cost function with respect to the weights and bias.\n- _fit: Trains the model on the given data and target values.\n- predict_proba: Predicts the class probabilities for the given input data.\n- _predict: Predicts the class labels for the given input data.\n\nAttributes:\n- eta: Learning rate (default 0.01).\n- epochs: Number of training epochs (default 50).\n- l2: Regularization parameter (default 0.0).\n- minibatches: Number of minibatches for stochastic gradient descent (default 1).\n- n_classes: Number of classes (automatically determined from target values if not provided).\n- random_seed: Random seed for reproducibility (default None).\n- print_progress: Flag to print the progress of the training (default 0).\n- _is_fitted: Flag indicating whether the model has been fitted (default False).\n\nThis class is a combination of base models for iterative learning, classification, and multi-class classification, providing a comprehensive framework for training and predicting with Softmax Regression.\n\"\"\"\n", "```python\nclass TargetEncoder(OneToOneFeatureMixin, _BaseEncoder):\n    \"\"\"\n    TargetEncoder is a feature encoding technique used to replace categorical\n    variables with the mean of the target variable for each category. It is\n    particularly useful when dealing with high cardinality categorical variables\n    to reduce the dimensionality and improve the performance of machine learning\n    algorithms.\n\n    The TargetEncoder class allows for automatic determination of the target\n    type ('auto', 'continuous', 'binary', or 'multiclass') and smoothing parameter\n    ('auto' or a specific value). It also supports cross-validation for robust\n    encoding and the option to shuffle the data for better results.\n\n    Parameters:\n    - categories (str or list): Determines the categories for each feature. If\n      set to 'auto', categories are determined by a heuristic. If a list is\n      provided, it should contain the categories for each feature.\n    - target_type (str): Specifies the type of the target variable. If set to\n      'auto', the type is inferred from the input data. It can be 'continuous'\n      for regression tasks, 'binary' for binary classification, or 'multiclass'\n      for multiclass classification.\n    - smooth (str or int): Smoothing parameter to avoid division by zero. If set\n      to 'auto', an optimal value is determined. If an integer is provided, it\n      is used as the smoothing parameter.\n    - cv (int): Determines the number of folds for cross-validation. It is used\n      to create different folds for robust encoding.\n    - shuffle (bool): Whether to shuffle the data before splitting for cross-\n      validation.\n    - random_state (int): Seed for the random number generator for shuffling\n      and splitting the data.\n\n    Methods:\n    - fit(X, y): Fit the model to the data and learn the encoding for each feature.\n    - fit_transform(X, y): Fit the model to the data and transform it using the learned encoding.\n    - transform(X): Transform the data using the learned encoding.\n    - get_feature_names_out(input_features): Get output feature names for transformation.\n    - _more_tags(): Additional tags for the estimator.\n\n    Notes:\n    - The TargetEncoder is a stateful transformer, meaning it has learned parameters.\n    - The `fit` method must be called before `transform` or `fit_transform`.\n    - The `transform` method should only be used on data that was fitted on.\n    \"\"\"\n```\n", "\"\"\"\nThe `TransactionEncoder` class is a custom transformer designed to convert transaction data into a binary format suitable for machine learning algorithms. It inherits from `BaseEstimator`, signifying it's a scikit-learn-compatible estimator, and `TransformerMixin`, indicating it provides a `transform` method.\n\nThe class has the following methods:\n\n1. `__init__`: Initializes the `TransactionEncoder` with no parameters.\n\n2. `fit(X)`: This method fits the transformer to the input data `X`, identifying unique items within the transactions and establishing a mapping from items to column indices.\n\n3. `transform(X, sparse=False)`: Transforms the input data `X` into a binary format. If `sparse=True`, it returns a sparse matrix; otherwise, it returns a dense numpy array. Each row represents a transaction, and each column represents an item. A cell is set to `True` if the corresponding item is present in the transaction, `False` otherwise.\n\n4. `inverse_transform(array)`: Performs the inverse transformation of `transform`. It takes a binary array and returns a list of lists, where each sublist contains the items present in the corresponding transaction.\n\n5. `fit_transform(X, sparse=False)`: Combines `fit` and `transform` into a single method. It fits the transformer to the input data `X` and then transforms it.\n\n6. `get_feature_names_out()`: Returns the names of the features. It checks if the transformer has been fitted and returns the names of the items.\n\nThe `TransactionEncoder` class is particularly useful for preprocessing transaction data, converting it into a format that can be used in algorithms that require a binary matrix representation, such as frequent itemset mining.\n\"\"\"\n", "\"\"\"\nUpSampling1D class for 1D upsampling in Keras.\n\nThis class is a custom layer for 1D upsampling in Keras, which is used to increase the temporal resolution of the input data.\nThe upsampling operation is performed by repeating the data points along the time dimension (axis=1).\n\nAttributes:\n    size (int): The upsampling factor. This value determines how much the input data will be upsampled along the temporal dimension. Default is 2.\n    input_spec (InputSpec): The specification of the input data. This layer expects 3D tensor input with shape (batch_size, temporal_dimensions, features).\n\nMethods:\n    __init__(self, size=2, **kwargs):\n        Initializes the UpSampling1D layer with the specified upsampling factor and other keyword arguments.\n\n    compute_output_shape(self, input_shape):\n        Computes the output shape of the layer given the input shape. The output shape is the same as the input shape,\n        but with the second dimension (time) multiplied by the upsampling factor.\n\n    call(self, inputs):\n        Performs the upsampling operation on the input data. It repeats the input data points along the time dimension.\n\n    get_config(self):\n        Returns the configuration dictionary for this layer. It includes the upsampling factor.\n\"\"\"\n", "\"\"\"\nUpSampling2D Layer\n\nThe `UpSampling2D` class is a layer that increases the spatial dimensions of the input tensor.\nIt is commonly used in convolutional neural networks (CNNs) to upsample the feature maps.\n\nAttributes:\n    - `size`: A tuple of two integers, specifying the upsampling factors for the height and width dimensions respectively.\n    - `data_format`: A string, either `'channels_last'` (default) or `'channels_first'`. This parameter specifies the ordering of the dimensions in the inputs.\n    - `interpolation`: A string, either `'nearest'` or `'bilinear'`. This parameter defines the interpolation method used for upsampling.\n\nMethods:\n    - `compute_output_shape(input_shape)`: Computes the output shape of the layer given the input shape.\n    - `call(inputs)`: Applies the upsampling operation to the inputs.\n    - `get_config()`: Returns the configuration dictionary for this layer.\n\"\"\"\n", "\"\"\"\nUpSampling3D Layer\n\nThe `UpSampling3D` class is a Keras layer that performs a 3D upsampling operation on 5D input tensors.\nIt is used to increase the spatial dimensions (depth, height, width) of the input volume.\n\nAttributes:\n    size (tuple): A tuple of three integers representing the upsampling factors along the depth, height, and width dimensions respectively. Default is (2, 2, 2).\n    data_format (str): A string, one of `channels_last` (default) or `channels_first`. It specifies the ordering of the dimensions in the input tensor.\n\nMethods:\n    __init__(size=(2, 2, 2), data_format=None, **kwargs):\n        Initializes the `UpSampling3D` layer with the specified upsampling factors and data format.\n\n    compute_output_shape(input_shape):\n        Computes the output shape of the layer for the given input shape.\n\n    call(inputs):\n        Applies the upsampling operation to the input tensor.\n\n    get_config():\n        Returns the configuration of this layer as a dictionary.\n\"\"\"\n", "\"\"\"\nZeroPadding1D class for 1D zero padding in Keras.\n\nThis class is a custom layer for Keras that adds padding of zeros around the temporal dimension of the input tensor. \nThe padding is applied symmetrically before and after the input tensor along the temporal axis.\n\nAttributes:\n    padding (int or tuple of int): The amount of padding to be added. \n        If an integer is provided, the same padding is applied to both sides. \n        If a tuple of two integers is provided, the first integer represents the padding added before the input, \n        and the second integer represents the padding added after the input.\n\nMethods:\n    compute_output_shape(input_shape): Computes the shape of the output tensor based on the input shape and the applied padding.\n    call(inputs): Applies the zero padding to the input tensor.\n    get_config(): Returns the configuration of the layer including the padding.\n\nThis layer is particularly useful in scenarios where the input data needs to be extended or contracted along the temporal dimension, such as in sequence modeling tasks in deep learning.\n\"\"\"\n", "\"\"\"\nZeroPadding2D class for adding padding to 2D input data in a convolutional neural network.\n\nThe `ZeroPadding2D` class is a layer that can be included in a Keras-based deep learning model to add padding to 2D input data. This is useful for controlling the size of the output feature maps and maintaining the spatial dimensions of the input data. The padding can be specified in various ways:\n\n- As an integer `n`, which will pad the input with `n` pixels on all sides.\n- As a tuple `(h, w)`, which will pad the input with `h` pixels along the height and `w` pixels along the width.\n- As a tuple `((top_pad, bottom_pad), (left_pad, right_pad))`, which allows for asymmetric padding along the height and width dimensions.\n\nThe `data_format` parameter can be set to either 'channels_first' (shape: `(batch, channels, rows, cols)`) or 'channels_last' (shape: `(batch, rows, cols, channels)`).\n\nMethods:\n- `__init__(self, padding, data_format=None, **kwargs)`: Initializes the `ZeroPadding2D` layer with the specified padding and data format.\n- `compute_output_shape(self, input_shape)`: Computes the output shape of the layer given the input shape.\n- `call(self, inputs)`: Applies the padding to the input data.\n- `get_config(self)`: Returns the configuration of the layer.\n\nAttributes:\n- `padding`: The padding to be applied. It can be an integer, a tuple of two integers, or a tuple of two tuples of two integers.\n- `data_format`: The ordering of the dimensions in the inputs. Can be 'channels_first' or 'channels_last'.\n\"\"\"\n", "\"\"\"\nClass for zero padding in 3D data within the context of a neural network.\n\nThis class extends the base Layer class from Keras and is designed to handle 3D data padding. The padding can be specified in various ways, including symmetric padding for each dimension or asymmetric padding defined as tuples of (left, right) for each dimension.\n\nAttributes:\n    padding: A tuple of 3 tuples, where each tuple contains two integers representing the symmetric padding to be added to each spatial dimension of the input data. If an integer is provided, it is treated as symmetric padding for all dimensions.\n    data_format: A string, either 'channels_last' (default) or 'channels_first'. This specifies the ordering of dimensions in the input tensor. 'channels_last' corresponds to inputs with the shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels), while 'channels_first' corresponds to inputs with the shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the value specified in the 'image_data_format' configuration in your Keras setup.\n\nMethods:\n    compute_output_shape: Computes the shape of the output tensor based on the input tensor shape and the applied padding.\n    call: Applies the padding to the input tensor.\n    get_config: Returns the configuration of the layer as a dictionary.\n\nThis layer is particularly useful for maintaining the spatial dimensions of the input data during convolution operations or other transformations that require fixed-size inputs.\n\"\"\"\n", "\"\"\"\nThe `_BaseEncoder` class is a core component of the scikit-learn library, designed to handle categorical data and convert it into a format suitable for machine learning algorithms. This class inherits from `TransformerMixin` and `BaseEstimator`, ensuring it adheres to the scikit-learn estimator API.\n\nThe class is responsible for encoding categorical features into integers. It provides methods for checking and preparing the input data (`_check_X`), fitting the encoder to the data (`_fit`), transforming the data into encoded format (`_transform`), and handling infrequent categories.\n\nKey methods include:\n- `_check_X`: Validates the input data format and prepares it for encoding.\n- `_fit`: Learns the categories and counts of each feature in the data.\n- `_transform`: Encodes the input data using the learned categories.\n- `_more_tags`: Provides metadata about the input data requirements.\n\nThe class has attributes that control the encoding process, such as `categories` for specifying categories manually or setting them to 'auto' to determine automatically, `handle_unknown` for handling unknown categories, and `return_counts` to return category counts.\n\nAdditionally, the class supports infrequent category handling with attributes like `max_categories` and `min_frequency`, and methods like `_identify_infrequent` and `_fit_infrequent_category_mapping` to manage infrequent categories.\n\nThe `infrequent_categories_` attribute provides information about infrequent categories in each feature.\n\nThe class is designed to be flexible and extensible, allowing for customization through the use of subclassing and overriding methods. It is an integral part of the scikit-learn ecosystem, ensuring that categorical data is appropriately managed during the machine learning pipeline.\n\"\"\"\n"]