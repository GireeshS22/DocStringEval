["```python\n\"\"\"\nAdamax optimizer for TensorFlow.\n\nAdamax is a variant of the Adam algorithm that uses the infinity norm to update the learning rate.\n\nArgs:\n    learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.001.\n    beta_1 (float, optional): The exponential decay rate for the first moment estimates. Defaults to 0.9.\n    beta_2 (float, optional): The exponential decay rate for the second moment estimates. Defaults to 0.999.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-07.\n    name (str, optional): The name of the optimizer. Defaults to 'Adamax'.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports gradient aggregation.\n\nMethods:\n    _create_slots(var_list): Creates slots for the optimizer.\n    _prepare_local(var_device, var_dtype, apply_state): Prepares local variables for the optimizer.\n    _resource_apply_dense(grad, var, apply_state=None): Applies the optimizer to a dense variable.\n    _resource_apply_sparse(grad, var, indices, apply_state=None): Applies the optimizer to a sparse variable.\n    get_config(): Returns a dictionary containing the configuration of the optimizer.\n\"\"\"\n```", "```python\nclass AgglomerationTransform(TransformerMixin):\n    \"\"\"\n    AgglomerationTransform is a transformer that aggregates data points based on labels using a specified pooling function.\n\n    Parameters\n    ----------\n    pooling_func : callable, default=np.mean\n        The function to use for pooling the data points. Default is the mean.\n\n    Attributes\n    ----------\n    labels_ : array-like of shape (n_samples,)\n        The labels assigned to each sample.\n\n    Methods\n    -------\n    transform(X)\n        Aggregates the data points based on the labels using the specified pooling function.\n\n    inverse_transform(X=None, *, Xt=None)\n        Reverses the transformation by mapping the aggregated data back to the original data points.\n    \"\"\"\n```", "```python\nclass AveragePooling1D(Pooling1D):\n    \"\"\"\n    Applies a 1D average pooling operation to the input.\n\n    Args:\n        pool_size (int, tuple of int): The size of the pooling window. If a tuple is provided, it should be of length 1.\n        strides (int, tuple of int, optional): The strides of the pooling operation. If a tuple is provided, it should be of length 1. Defaults to None.\n        padding (str, optional): The padding method to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format of the input. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Returns:\n        A 1D tensor after applying the average pooling operation.\n    \"\"\"\n```", "```python\nclass AveragePooling2D(Pooling2D):\n    \"\"\"\n    Applies average pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window. Default is (2, 2).\n        strides (tuple of int, optional): The strides of the pooling operation. If None, the strides are set to the pool_size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_last' or 'channels_first'. Default is None.\n        **kwargs: Additional keyword arguments passed to the parent class.\n\n    Returns:\n        Tensor: The output tensor after applying average pooling.\n    \"\"\"\n```", "```python\nclass AveragePooling3D(Pooling3D):\n    \"\"\"\n    Applies a 3D average pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window in each dimension (depth, height, width). Default is (2, 2, 2).\n        strides (tuple of int, optional): The strides of the pooling window in each dimension. If None, the strides are set to the pool_size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_first' or 'channels_last'. Default is None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The output tensor after applying the 3D average pooling operation.\n    \"\"\"\n```", "```python\nclass BayesianGaussianMixture(BaseMixture):\n    \"\"\"\n    Bayesian Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    n_components : int, default=1\n        The number of mixture components.\n\n    covariance_type : {'spherical', 'tied', 'diag', 'full'}, default='full'\n        Type of covariance parameters to use. Must be one of:\n\n        - 'spherical' : each component has its own diagonal covariance matrix with all values equal.\n        - 'tied' : all components share the same diagonal covariance matrix.\n        - 'diag' : each component has its own diagonal covariance matrix.\n        - 'full' : each component has its own full covariance matrix.\n\n    tol : float, default=1e-3\n        The convergence threshold. EM iterations will stop when the lower bound\n        increase is less than this threshold.\n\n    reg_covar : float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance matrices.\n\n    max_iter : int, default=100\n        The number of EM iterations to perform.\n\n    n_init : int, default=1\n        The number of times the EM algorithm will be run with different\n        initializations. The best result will be kept.\n\n    init_params : {'kmeans', 'random'}, default='kmeans'\n        The method used to initialize the weights, the means and the\n        precisions.\n\n        - 'kmeans' : responsibilities are initialized using kmeans.\n        - 'random' : responsibilities are initialized randomly.\n\n    weight_concentration_prior_type : {'dirichlet_process', 'dirichlet_distribution'}, default='dirichlet_process'\n        The type of prior to use for the weight concentration parameters.\n\n        - 'dirichlet_process' : the weight concentration parameters are drawn from a Dirichlet process.\n        - 'dirichlet_distribution' : the weight concentration parameters are drawn from a Dirichlet distribution.\n\n    weight_concentration_prior : float or None, default=None\n        The prior on the weight concentration parameters. If None, it is set to 1.0 / n_components.\n\n    mean_precision_prior : float or None, default=None\n        The prior on the mean precision parameters. If None, it is set to 1.0.\n\n    mean_prior : array-like or None, default=None\n        The prior on the means. If None, it is set to the mean of the data.\n\n    degrees_of_freedom_prior : float or None, default=None\n        The prior on the degrees of freedom parameters. If None, it is set to the number of features.\n\n    covariance_prior : array-like or None, default=None\n        The prior on the covariance parameters. If None, it is set to the covariance of the data.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to the method for reproducibility.\n\n    warm_start : bool, default=False\n        If 'warm_start' is True, the solution of the previous call to fit is used as\n        initialization.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints the current iteration number\n        and the lower bound for each iteration.\n\n    verbose_interval : int, default=10\n        Number of iterations between each verbose output.\n\n    Attributes\n    ----------\n    weight_concentration_ : array-like of shape (n_components,)\n        The weight concentration parameters for each component.\n\n    means_ : array-like of shape (n_components, n_features)\n        The mean parameters for each component.\n\n    precisions_cholesky_ : array-like of shape (n_components, n_features, n_features)\n        The Cholesky decomposition of the precision matrices for each component.\n\n    covariance_type : str\n        The type of covariance parameters used.\n\n    weight_concentration_prior_type : str\n        The type of prior used for the weight concentration parameters.\n\n    weight_concentration_prior : float or None\n        The prior on the weight concentration parameters.\n\n    mean_precision_prior : float or None\n        The prior on the mean precision parameters.\n\n    mean_prior : array-like or None\n        The prior on the means.\n\n    degrees_of_freedom_prior : float or None\n        The prior on the degrees of freedom parameters.\n\n    covariance_prior : array-like or None\n        The prior on the covariance parameters.\n\n    n_components : int\n        The number of mixture components.\n\n    tol : float\n        The convergence threshold.\n\n    reg_covar : float\n        Non-negative regularization added to the diagonal of covariance matrices.\n\n    max_iter : int\n        The number of EM iterations to perform.\n\n    n_init : int\n        The number of times the EM algorithm will be run with different\n        initializations. The best result will be kept.\n\n    init_params : str\n        The", "```python\nclass Conv(Layer):\n    \"\"\"\n    A class representing a convolutional layer.\n\n    Args:\n        rank (int): The rank of the convolution, e.g., 1 for 1D, 2 for 2D, etc.\n        filters (int): The number of output filters in the convolution.\n        kernel_size (int or tuple of ints): The size of the convolution kernel.\n        strides (int or tuple of ints, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_first' or 'channels_last'. Defaults to None.\n        dilation_rate (int or tuple of ints, optional): The dilation rate of the convolution. Defaults to 1.\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the output activity. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias vector. Defaults to None.\n        trainable (bool, optional): Whether the layer is trainable. Defaults to True.\n        name (str, optional): The name of the layer. Defaults to None.\n        conv_op (callable, optional): The convolution operation to use. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Raises:\n        ValueError: If the number of filters is negative.\n        ValueError: If the number of filters is not evenly divisible by the number of groups.\n        ValueError: If the kernel size contains 0(s).\n        ValueError: If the strides contain 0(s).\n        ValueError: If causal padding is used with a layer other than Conv1D or SeparableConv1D.\n\n    Attributes:\n        rank (int): The rank of the convolution.\n        filters (int): The number of output filters in the convolution.\n        groups (int): The number of groups for grouped convolution.\n        kernel_size (tuple of ints): The size of the convolution kernel.\n        strides (tuple of ints): The strides of the convolution.\n        padding (str): The type of padding to use.\n        data_format (str): The data format to use.\n        dilation_rate (tuple of ints): The dilation rate of the convolution.\n        activation (callable): The activation function to use.\n        use_bias (bool): Whether to use a bias vector.\n        kernel_initializer (callable): The initializer for the kernel weights.\n        bias_initializer (callable): The initializer for the bias vector.\n        kernel_regularizer (callable): The regularizer for the kernel weights.\n        bias_regularizer (callable): The regularizer for the bias vector.\n        activity_regularizer (callable): The regularizer for the output activity.\n        kernel_constraint (callable): The constraint for the kernel weights.\n        bias_constraint (callable): The constraint for the bias vector.\n        input_spec (InputSpec): The input specification for the layer.\n        _is_causal (bool): Whether the layer uses causal padding.\n        _channels_first (bool): Whether the data format is channels first.\n        _tf_data_format (str): The TensorFlow data format.\n        _convolution_op (callable): The convolution operation to use.\n        built (bool): Whether the layer has been built.\n    \"\"\"\n```", "```python\nclass Conv1D(Conv):\n    \"\"\"\n    1D convolution layer.\n\n    Args:\n        filters (int): The dimensionality of the output space (i.e., the number of output filters in the convolution).\n        kernel_size (int or tuple): An integer or tuple of length 1, specifying the length of the 1D convolution window.\n        strides (int or tuple, optional): An integer or tuple of length 1, specifying the stride of the convolution. Defaults to 1.\n        padding (str, optional): One of 'valid' or 'same' (case-insensitive). 'valid' means no padding is added, and 'same' means padding is added so that the output has the same length as the input. Defaults to 'valid'.\n        data_format (str, optional): A string, one of 'channels_last' (default) or 'channels_first'. The ordering of the dimensions in the inputs. 'channels_last' corresponds to inputs with shape (batch, length, channels) while 'channels_first' corresponds to inputs with shape (batch, channels, length). Defaults to 'channels_last'.\n        dilation_rate (int or tuple, optional): An integer or tuple of length 1, specifying the dilation rate of the convolution. Defaults to 1.\n        groups (int, optional): A positive integer specifying the number of groups in the convolution. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. If None, no activation is applied. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the output (activation). Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias vector. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the superclass constructor.\n    \"\"\"\n```", "```python\nclass Conv1DTranspose(Conv1D):\n    \"\"\"\n    Transposed 1D convolution layer.\n\n    Args:\n        filters (int): The dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (int or tuple): The size of the convolution window.\n        strides (int or tuple, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        output_padding (int or tuple, optional): Additional output dimensions. Can only be specified if the input is 3D. Defaults to None.\n        data_format (str, optional): The data format to use. Can be 'channels_first' or 'channels_last'. Defaults to None.\n        dilation_rate (int or tuple, optional): The dilation rate to use. Defaults to 1.\n        activation (str or callable, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer to use for the kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer to use for the bias. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer to use for the kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer to use for the bias. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer to use for the activity. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint to use for the kernel. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint to use for the bias. Defaults to None.\n\n    Returns:\n        A 3D tensor representing the output of the transposed convolution.\n    \"\"\"\n```", "```python\nclass Conv2D(Conv):\n    \"\"\"\n    A 2D convolutional layer.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (tuple): The size of the convolution kernel.\n        strides (tuple, optional): The strides of the convolution. Defaults to (1, 1).\n        padding (str, optional): The type of padding to use. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Defaults to None.\n        dilation_rate (tuple, optional): The dilation rate of the convolution. Defaults to (1, 1).\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias term. Defaults to True.\n        kernel_initializer (str, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias term. Defaults to 'zeros'.\n        kernel_regularizer (str, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias term. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the layer activity. Defaults to None.\n        kernel_constraint (str, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias term. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        None\n    \"\"\"\n```", "```python\nclass Conv2DTranspose(Conv2D):\n    \"\"\"\n    Transposed 2D convolution layer (also known as deconvolution).\n\n    Args:\n        filters (int): The dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (tuple of int): A tuple of integers specifying the height and width of the 2D convolution window.\n        strides (tuple of int, optional): A tuple of integers specifying the strides of the convolution along the height and width. Defaults to (1, 1).\n        padding (str, optional): One of 'valid' or 'same'. 'valid' means no padding is added, and 'same' means padding is added so that the output has the same spatial dimensions as the input. Defaults to 'valid'.\n        output_padding (tuple of int, optional): A tuple of integers specifying the additional padding added to one side of the output. Defaults to None.\n        data_format (str, optional): One of 'channels_first' or 'channels_last'. 'channels_first' means the input and output tensors have shape (batch_size, channels, height, width), and 'channels_last' means they have shape (batch_size, height, width, channels). Defaults to None.\n        dilation_rate (tuple of int, optional): A tuple of integers specifying the dilation rate of the convolution along the height and width. Defaults to (1, 1).\n        activation (str or callable, optional): The activation function to use. If None, no activation is applied. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer to use for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer to use for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer to use for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer to use for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer to use for the output of the layer. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint to use for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint to use for the bias vector. Defaults to None.\n\n    Returns:\n        A 4D tensor of shape (batch_size, height, width, filters) if data_format is 'channels_last', or (batch_size, channels, height, width) if data_format is 'channels_first'.\n    \"\"\"\n```", "```python\nclass Conv3D(Conv):\n    \"\"\"\n    A 3D convolutional layer.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (tuple of int): The size of the convolution kernel.\n        strides (tuple of int, optional): The strides of the convolution. Defaults to (1, 1, 1).\n        padding (str, optional): The type of padding to use. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Defaults to None.\n        dilation_rate (tuple of int, optional): The dilation rate of the convolution. Defaults to (1, 1, 1).\n        groups (int, optional): The number of groups for grouped convolution. Defaults to 1.\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the output activity. Defaults to None.\n        kernel_constraint (str, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias vector. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the base class.\n    \"\"\"\n```", "```python\nclass Conv3DTranspose(Conv3D):\n    \"\"\"\n    A 3D transposed convolution layer, also known as a deconvolution layer.\n\n    Args:\n        filters (int): The number of filters in the convolution.\n        kernel_size (tuple of int): The size of the convolution kernel.\n        strides (tuple of int, optional): The strides of the convolution. Defaults to (1, 1, 1).\n        padding (str, optional): The type of padding to use. Defaults to 'valid'.\n        output_padding (tuple of int, optional): Additional padding to add to the output. Defaults to None.\n        data_format (str, optional): The data format to use. Defaults to None.\n        dilation_rate (tuple of int, optional): The dilation rate of the convolution. Defaults to (1, 1, 1).\n        activation (str or callable, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        kernel_initializer (str or callable, optional): The initializer for the kernel weights. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        kernel_regularizer (str or callable, optional): The regularizer for the kernel weights. Defaults to None.\n        bias_regularizer (str or callable, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): The regularizer for the output activity. Defaults to None.\n        kernel_constraint (str or callable, optional): The constraint for the kernel weights. Defaults to None.\n        bias_constraint (str or callable, optional): The constraint for the bias vector. Defaults to None.\n\n    Returns:\n        A 3D tensor representing the output of the convolution.\n    \"\"\"\n```", "```python\nclass Cropping1D(Layer):\n    \"\"\"\n    Crops the 1D input tensor along the spatial dimension.\n\n    Args:\n        cropping (tuple of int): A tuple of two integers specifying the number of elements to crop from the start and end of the input tensor along the spatial dimension. Defaults to (1, 1).\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Returns:\n        Tensor: The cropped 1D input tensor.\n\n    Notes:\n        - The input tensor is expected to have a shape of (batch_size, length, channels).\n        - The cropping is applied to the length dimension.\n        - If the cropping value is 0 for the end dimension, no elements are cropped from the end.\n        - The output tensor will have a shape of (batch_size, length - cropping[0] - cropping[1], channels).\n    \"\"\"\n```", "```python\nclass Cropping2D(Layer):\n    \"\"\"\n    Crops the input tensor along the spatial dimensions (height and width).\n\n    Args:\n        cropping (int, tuple of 2 ints, or tuple of 2 tuples of 2 ints): \n            - If an int, crops symmetrically on both sides of the input tensor.\n            - If a tuple of 2 ints, crops symmetrically on both sides of the height and width dimensions.\n            - If a tuple of 2 tuples of 2 ints, crops asymmetrically on the height and width dimensions.\n        data_format (str, optional): \n            - Specifies the data format of the input tensor. \n            - Can be 'channels_first' or 'channels_last'. \n            - Defaults to the value specified in the backend configuration.\n\n    Returns:\n        Tensor: Cropped tensor with the specified dimensions.\n\n    Raises:\n        ValueError: If the `cropping` argument is not in the expected format.\n    \"\"\"\n```", "```python\nclass Cropping3D(Layer):\n    \"\"\"\n    Crops 3D input tensors.\n\n    Args:\n        cropping (int, tuple of 3 ints, or tuple of 3 tuples of 2 ints): Amount of cropping at each dimension.\n            If an integer, the same amount of cropping is applied to all dimensions.\n            If a tuple of 3 ints, it specifies the cropping for each dimension (dim1, dim2, dim3).\n            If a tuple of 3 tuples of 2 ints, it specifies the cropping for each dimension as (left, right).\n        data_format (str, optional): The data format of the input tensor ('channels_first' or 'channels_last'). Defaults to None.\n\n    Returns:\n        Tensor: Cropped 3D tensor.\n\n    Raises:\n        ValueError: If `cropping` is not an int, a tuple of 3 ints, or a tuple of 3 tuples of 2 ints.\n    \"\"\"\n```", "```python\nclass DBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"\n    DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that can find\n    clusters of different shapes and sizes. It groups together points that are packed closely together (points with many nearby\n    neighbors), marking as outliers points that lie alone in low-density regions.\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for them to be considered as in the same neighborhood.\n\n    min_samples : int, default=5\n        The number of samples in a neighborhood for a point to be considered as a core point.\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in X. If metric is a string, it must be one of the\n        options provided by `sklearn.metrics.pairwise.pairwise_distances`. If metric is callable, it should take two arrays as\n        input and return a distance matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to use to compute the nearest neighbors. If 'auto', the algorithm will be chosen based on the input data.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree. This can affect the speed of the algorithm, especially in high dimensions.\n\n    p : float, default=None\n        The power parameter for the Minkowski metric. If None, then the Euclidean metric is used.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. If None, then the number of jobs is set to the number of CPU\n        cores.\n\n    Attributes\n    ----------\n    core_sample_indices_ : array-like of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels_ : array-like of shape (n_samples,)\n        Cluster labels for each point. -1 indicates noise.\n\n    components_ : array-like of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    Methods\n    -------\n    fit(X, y=None, sample_weight=None)\n        Compute DBSCAN clustering.\n\n    fit_predict(X, y=None, sample_weight=None)\n        Compute DBSCAN clustering and predict cluster labels for the samples.\n\n    _more_tags()\n        Additional tags for estimator.\n    \"\"\"\n```", "```python\nclass DepthwiseConv2D(Conv2D):\n    \"\"\"\n    Depthwise separable convolution layer.\n\n    Args:\n        kernel_size (tuple): A tuple of integers specifying the height and width of the 2D convolution window.\n        strides (tuple, optional): A tuple of integers specifying the strides of the convolution. Defaults to (1, 1).\n        padding (str, optional): One of 'valid' or 'same'. 'valid' means no padding. 'same' means padding is added so that the output has the same spatial dimensions as the input. Defaults to 'valid'.\n        depth_multiplier (int, optional): The number of depthwise convolution output channels for each input channel. The total number of output channels will be `filters * depth_multiplier`. Defaults to 1.\n        data_format (str, optional): One of 'channels_first' or 'channels_last'. 'channels_first' means the data format is (batch, channels, height, width). 'channels_last' means the data format is (batch, height, width, channels). Defaults to None.\n        dilation_rate (tuple, optional): A tuple of integers specifying the dilation rate of the convolution. Defaults to (1, 1).\n        activation (str or callable, optional): Activation function to use. If None, no activation is applied. Defaults to None.\n        use_bias (bool, optional): Whether to add a bias vector to the output. Defaults to True.\n        depthwise_initializer (str or callable, optional): Initializer for the depthwise convolution kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): Initializer for the bias vector. Defaults to 'zeros'.\n        depthwise_regularizer (str or callable, optional): Regularizer for the depthwise convolution kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): Regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): Regularizer for the output. Defaults to None.\n        depthwise_constraint (str or callable, optional): Constraint for the depthwise convolution kernel. Defaults to None.\n        bias_constraint (str or callable, optional): Constraint for the bias vector. Defaults to None.\n\n    Returns:\n        A tensor representing the output of the depthwise convolution layer.\n    \"\"\"\n```", "```python\nclass Embedding(Layer):\n    \"\"\"\n    Turns positive integers (indexes) into dense vectors of fixed size.\n\n    Args:\n        input_dim (int): Size of the vocabulary, i.e. maximum integer index + 1.\n        output_dim (int): Dimension of the dense embedding.\n        embeddings_initializer (str or Initializer): Initializer for the embeddings matrix.\n        embeddings_regularizer (str or Regularizer): Regularizer function applied to the embeddings matrix.\n        activity_regularizer (str or Regularizer): Regularizer function applied to the output of the layer.\n        embeddings_constraint (str or Constraint): Constraint function applied to the embeddings matrix.\n        mask_zero (bool): If this is set to True, then all input indexes greater than or equal to `input_dim` will be zero vectors.\n        input_length (int or list or tuple): Length of input sequences, can be an integer or a list/tuple of integers.\n\n    Raises:\n        ValueError: If `input_dim` or `output_dim` is not positive.\n\n    Returns:\n        A 2D tensor of shape `(batch_size, input_length, output_dim)` if `input_length` is not None, otherwise `(batch_size, input_length, output_dim)`.\n    \"\"\"\n```", "```python\nclass Flask(App):\n    \"\"\"\n    A WSGI web application class.\n\n    This class provides a framework for building web applications using the Flask framework. It includes methods for handling requests, rendering templates, and managing sessions.\n\n    Parameters:\n    - import_name (str): The name of the application or the module where the application is defined.\n    - static_url_path (str | None): The URL path for serving static files. Defaults to None.\n    - static_folder (str | os.PathLike[str] | None): The folder containing static files. Defaults to 'static'.\n    - static_host (str | None): The host name to serve static files from. Defaults to None.\n    - host_matching (bool): Whether to match the host name in the URL. Defaults to False.\n    - subdomain_matching (bool): Whether to match the subdomain in the URL. Defaults to False.\n    - template_folder (str | os.PathLike[str] | None): The folder containing template files. Defaults to 'templates'.\n    - instance_path (str | None): The path to the instance folder. Defaults to None.\n    - instance_relative_config (bool): Whether the configuration is relative to the instance folder. Defaults to False.\n    - root_path (str | None): The root path of the application. Defaults to None.\n\n    Attributes:\n    - default_config (ImmutableDict): The default configuration for the application.\n    - request_class (type[Request]): The class used for handling requests.\n    - response_class (type[Response]): The class used for handling responses.\n    - session_interface (SessionInterface): The interface used for managing sessions.\n\n    Methods:\n    - __init__: Initializes the Flask application.\n    - get_send_file_max_age: Returns the maximum age for sending files.\n    - send_static_file: Sends a static file.\n    - open_resource: Opens a resource file.\n    - open_instance_resource: Opens an instance resource file.\n    - create_jinja_environment: Creates a Jinja2 environment.\n    - create_url_adapter: Creates a URL adapter.\n    - raise_routing_exception: Raises a routing exception.\n    - update_template_context: Updates the template context.\n    - make_shell_context: Creates a shell context.\n    - run: Runs the Flask application.\n    - test_client: Creates a test client.\n    - test_cli_runner: Creates a test CLI runner.\n    - handle_http_exception: Handles an HTTP exception.\n    - handle_user_exception: Handles a user exception.\n    - handle_exception: Handles an exception.\n    - log_exception: Logs an exception.\n    - dispatch_request: Dispatches a request.\n    - full_dispatch_request: Full dispatches a request.\n    - finalize_request: Finalizes a request.\n    - make_default_options_response: Makes a default options response.\n    - ensure_sync: Ensures a function is synchronous.\n    - async_to_sync: Converts an asynchronous function to synchronous.\n    - url_for: Generates a URL for a given endpoint.\n    - make_response: Creates a response object.\n    - preprocess_request: Preprocesses a request.\n    - process_response: Processes a response.\n    - do_teardown_request: Teardowns a request.\n    - do_teardown_appcontext: Teardowns an application context.\n    - app_context: Creates an application context.\n    - request_context: Creates a request context.\n    - test_request_context: Creates a test request context.\n    - wsgi_app: The WSGI application callable.\n    - __call__: The WSGI application callable.\n    \"\"\"\n```", "```python\nclass FunctionTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    A transformer that applies a custom function to the input data and optionally its inverse.\n\n    Parameters\n    ----------\n    func : callable, optional\n        The function to apply to the input data. If None, the identity function is used.\n    inverse_func : callable, optional\n        The inverse function to apply to the transformed data. If None, the inverse of `func` is not checked.\n    validate : bool, default=False\n        Whether to validate the input data.\n    accept_sparse : bool, default=False\n        Whether to accept sparse input data.\n    check_inverse : bool, default=True\n        Whether to check if the provided functions are strictly inverse of each other.\n    feature_names_out : callable, str, or None, default=None\n        The function to generate output feature names. If 'one-to-one', the output feature names are the same as the input feature names.\n    kw_args : dict, optional\n        Additional keyword arguments to pass to the `func`.\n    inv_kw_args : dict, optional\n        Additional keyword arguments to pass to the `inverse_func`.\n\n    Attributes\n    ----------\n    func : callable\n        The function to apply to the input data.\n    inverse_func : callable\n        The inverse function to apply to the transformed data.\n    validate : bool\n        Whether to validate the input data.\n    accept_sparse : bool\n        Whether to accept sparse input data.\n    check_inverse : bool\n        Whether to check if the provided functions are strictly inverse of each other.\n    feature_names_out : callable, str, or None\n        The function to generate output feature names.\n    kw_args : dict\n        Additional keyword arguments to pass to the `func`.\n    inv_kw_args : dict\n        Additional keyword arguments to pass to the `inverse_func`.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit the transformer to the input data.\n    transform(X)\n        Apply the transformer to the input data.\n    inverse_transform(X)\n        Apply the inverse transformer to the input data.\n    get_feature_names_out(input_features=None)\n        Get the output feature names.\n    _transform(X, func=None, kw_args=None)\n        Apply the provided function to the input data.\n    __sklearn_is_fitted__()\n        Check if the transformer is fitted.\n    _more_tags()\n        Get additional tags for the transformer.\n    set_output(transform=None)\n        Set the output configuration for the transformer.\n    \"\"\"\n```", "```python\nclass GaussianMixture(BaseMixture):\n    \"\"\"\n    Gaussian Mixture Model (GMM) with various covariance types.\n\n    Parameters\n    ----------\n    n_components : int, default=1\n        The number of mixture components.\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'\n        Type of covariance parameters to use. Possible values are:\n        - 'full': each component has its own general covariance matrix.\n        - 'tied': all components share the same general covariance matrix.\n        - 'diag': each component has its own diagonal covariance matrix.\n        - 'spherical': each component has its own single variance.\n\n    tol : float, default=1e-3\n        The convergence threshold. EM iterations will stop when the\n        lower bound average gain is below this threshold.\n\n    reg_covar : float, default=1e-6\n        Non-negative regularization added to the diagonal of covariance\n        matrices to ensure stability of the Cholesky decomposition.\n\n    max_iter : int, default=100\n        The number of EM iterations to perform.\n\n    n_init : int, default=1\n        The number of times the EM algorithm will be run with different\n        initializations. The best result will be kept.\n\n    init_params : {'kmeans', 'random'}, default='kmeans'\n        The method used to initialize the weights, the means and the\n        precisions.\n        - 'kmeans': responsibilities are initialized using kmeans.\n        - 'random': responsibilities are initialized randomly.\n\n    weights_init : array-like of shape (n_components,), default=None\n        The user-provided initial weights, defaults to None which\n        assigns equal weight to each component.\n\n    means_init : array-like of shape (n_components, n_features), default=None\n        The user-provided initial means, defaults to None which\n        assigns random means from the data.\n\n    precisions_init : array-like, default=None\n        The user-provided initial precisions, defaults to None which\n        assigns random precisions from the data.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to the method for\n        reproducibility.\n\n    warm_start : bool, default=False\n        If 'warm_start' is True, the solution of the previous call to fit is\n        used as initialization.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints the current\n        initialization and each iteration step. If greater than 1 then\n        it prints also the log probability and the time needed\n        for each step.\n\n    verbose_interval : int, default=10\n        Number of iteration done before the next print.\n\n    Attributes\n    ----------\n    weights_ : array-like of shape (n_components,)\n        The weights of each mixture component.\n\n    means_ : array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component. The shape depends on\n        `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    precisions_ : array-like\n        The precision matrices for each mixture component. The shape depends on\n        `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    precisions_cholesky_ : array-like\n        The Cholesky decomposition of the precision matrices of each\n        mixture component. The shape depends on `covariance_type`:\n        - 'full': (n_components, n_features, n_features)\n        - 'tied': (n_features, n_features)\n        - 'diag': (n_components, n_features)\n        - 'spherical': (n_components,)\n\n    n_features_in_ : int\n        Number of features seen during fit.\n\n    feature_names_in_ : ndarray of shape (n_features,)\n        Names of features seen during fit. Defined only if `X` has feature\n        names that are all strings.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Estimate model parameters with the EM algorithm.\n\n    fit_predict(X, y=None)\n        Estimate model parameters using X and predict the labels for X.\n\n    predict(X)\n        Predict the labels for the data samples in X using the trained model.\n\n    predict_proba(X)\n        Evaluate the components' density for each sample.\n\n   ", "```python\nclass GlobalAveragePooling1D(GlobalPooling1D):\n    \"\"\"\n    Global average pooling operation for 1D data.\n\n    This layer computes the average value of each feature map across the spatial dimensions.\n\n    Args:\n        data_format (str, optional): The data format of the input. Can be either 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Attributes:\n        supports_masking (bool): Indicates whether the layer supports masking.\n\n    Methods:\n        call(inputs, mask=None): Applies the global average pooling operation to the input tensor.\n        compute_mask(inputs, mask=None): Computes the output mask for the layer.\n    \"\"\"\n```", "```python\nclass GlobalAveragePooling2D(GlobalPooling2D):\n    \"\"\"\n    Global average pooling layer for 2D inputs.\n\n    This layer computes the average value of each feature map in the input tensor,\n    resulting in a single output value per feature map.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either\n            'channels_last' (default) or 'channels_first'. Determines the axis along which\n            the pooling is applied.\n        keepdims (bool, optional): If True, the spatial dimensions of the output will be\n            the same as the input. If False, the spatial dimensions will be reduced to 1.\n            Default is False.\n\n    Returns:\n        Tensor: A tensor with the same number of channels as the input, but with reduced\n            spatial dimensions (height and width) to 1 or kept the same if `keepdims` is True.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n        else:\n            return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n```", "```python\nclass GlobalAveragePooling3D(GlobalPooling3D):\n    \"\"\"\n    Applies global average pooling operation for 3D data.\n\n    This layer computes the average value of each feature map across the spatial dimensions (height, width, depth).\n\n    Args:\n        data_format (str, optional): Specifies the data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. If 'channels_last', the input shape is expected to be (batch_size, height, width, depth, channels). If 'channels_first', the input shape is expected to be (batch_size, channels, height, width, depth).\n        keepdims (bool, optional): If True, retains the reduced dimensions with length 1. Default is False.\n\n    Returns:\n        Tensor: A 4D tensor with the same batch size and number of channels as the input, but with reduced spatial dimensions (height, width, depth) to 1.\n\n    Example:\n        >>> inputs = tf.random.normal((32, 16, 16, 16, 3))\n        >>> pool = GlobalAveragePooling3D(data_format='channels_last')\n        >>> outputs = pool(inputs)\n        >>> outputs.shape\n        (32, 1, 1, 1, 3)\n    \"\"\"\n```", "```python\nclass GlobalMaxPooling1D(GlobalPooling1D):\n    \"\"\"\n    Global max pooling operation for 1D data.\n\n    This layer applies global max pooling operation on the input data along the specified axis.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, retains the reduced dimensions with length 1. Defaults to False.\n\n    Returns:\n        Tensor: The result of the global max pooling operation.\n    \"\"\"\n    def call(self, inputs):\n        steps_axis = 1 if self.data_format == 'channels_last' else 2\n        return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n```", "```python\nclass GlobalMaxPooling2D(GlobalPooling2D):\n    \"\"\"\n    Applies global max pooling operation for 2D inputs.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, retains the reduced dimensions with length 1. Defaults to False.\n\n    Returns:\n        Tensor: The result of applying global max pooling to the input tensor.\n    \"\"\"\n    def call(self, inputs):\n        if self.data_format == 'channels_last':\n            return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n        else:\n            return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n```", "```python\nclass GlobalMaxPooling3D(GlobalPooling3D):\n    \"\"\"\n    Applies global maximum pooling operation for 3D data.\n\n    This layer reduces the spatial dimensions of the input tensor to a single point by computing the maximum value along the spatial dimensions.\n\n    Args:\n        data_format (str, optional): Specifies the data format of the input tensor. Can be either 'channels_last' (default) or 'channels_first'. The 'channels_last' format means that the channel dimension is the last dimension of the input tensor, while the 'channels_first' format means that the channel dimension is the first dimension.\n        keepdims (bool, optional): If True, the spatial dimensions are retained with length 1. If False, the spatial dimensions are removed. Default is False.\n\n    Returns:\n        Tensor: A tensor with the same number of channels as the input tensor, but with reduced spatial dimensions.\n\n    Example:\n        >>> inputs = tf.constant([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]])\n        >>> global_max_pooling = GlobalMaxPooling3D(data_format='channels_last')\n        >>> output = global_max_pooling(inputs)\n        >>> print(output)\n        tf.Tensor([[[[7]]]], shape=(1, 1, 1, 1), dtype=int32)\n    \"\"\"\n```", "```python\nclass GlobalPooling1D(Layer):\n    \"\"\"\n    Global 1D pooling layer for 3D input tensors.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        keepdims (bool, optional): If True, the spatial dimensions of the output will be kept as 1. Defaults to False.\n\n    Attributes:\n        input_spec (InputSpec): Specification of the input shape.\n        data_format (str): The normalized data format.\n        keepdims (bool): Whether to keep the spatial dimensions.\n\n    Methods:\n        compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n        call(inputs): Defines the computation of the layer for a given input.\n        get_config(): Returns a dictionary containing the configuration of the layer.\n    \"\"\"\n```", "```python\nclass GlobalPooling2D(Layer):\n    \"\"\"\n    Global 2D pooling layer for spatial dimensions.\n\n    This layer performs global pooling on the spatial dimensions of the input tensor,\n    reducing the spatial dimensions to a single point while preserving the number of channels.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either\n            'channels_last' (default) or 'channels_first'. If 'channels_last', the input\n            tensor is expected to have the shape (batch_size, height, width, channels).\n            If 'channels_first', the input tensor is expected to have the shape\n            (batch_size, channels, height, width).\n        keepdims (bool, optional): If True, the spatial dimensions are kept as size 1\n            in the output tensor. If False, the spatial dimensions are removed from the\n            output tensor. Default is False.\n\n    Returns:\n        Tensor: The output tensor after global pooling.\n\n    Raises:\n        NotImplementedError: The `call` method is not implemented and should be overridden\n            by subclasses.\n    \"\"\"\n```", "```python\nclass GlobalPooling3D(Layer):\n    \"\"\"\n    Global 3D pooling layer for 5D input tensors.\n\n    Args:\n        data_format (str, optional): The data format of the input tensor. Can be either 'channels_last' or 'channels_first'. If not specified, the default data format is used.\n        keepdims (bool, optional): If True, the spatial dimensions are kept with size 1. If False, the spatial dimensions are removed. Default is False.\n\n    Attributes:\n        data_format (str): The normalized data format of the input tensor.\n        input_spec (InputSpec): Specifies the expected input shape of the layer.\n        keepdims (bool): Whether to keep the spatial dimensions with size 1.\n\n    Methods:\n        compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n        call(inputs): Defines the computation performed by the layer on the input tensor. Raises NotImplementedError.\n        get_config(): Returns a dictionary containing the configuration of the layer.\n    \"\"\"\n```", "```python\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    A class for splitting time series data into training and testing sets based on groups.\n\n    Parameters\n    ----------\n    test_size : int\n        The size of the test set.\n    train_size : int, optional\n        The size of the training set. If not specified, it will be calculated based on the other parameters.\n    n_splits : int, optional\n        The number of splits to generate. If not specified, it will be calculated based on the other parameters.\n    gap_size : int, default=0\n        The number of samples to leave between the end of the training set and the start of the test set.\n    shift_size : int, default=1\n        The number of samples to shift the training and test sets by for each split.\n    window_type : str, default='rolling'\n        The type of window to use for splitting. Can be either 'rolling' or 'expanding'.\n\n    Raises\n    ------\n    ValueError\n        If both train_size and n_splits are not specified.\n        If window_type is not 'rolling' or 'expanding'.\n        If train_size is specified with an expanding window.\n        If the groups are not consecutive.\n\n    Attributes\n    ----------\n    test_size : int\n        The size of the test set.\n    train_size : int\n        The size of the training set.\n    n_splits : int\n        The number of splits to generate.\n    gap_size : int\n        The number of samples to leave between the end of the training set and the start of the test set.\n    shift_size : int\n        The number of samples to shift the training and test sets by for each split.\n    window_type : str\n        The type of window to use for splitting.\n    _n_groups : int\n        The number of groups.\n    _train_start_idx : int\n        The starting index of the training set.\n\n    Methods\n    -------\n    split(X, y=None, groups=None)\n        Generate indices to split data into training and test sets.\n    get_n_splits(X=None, y=None, groups=None)\n        Return the number of splits.\n    _calculate_split_params()\n        Calculate the parameters for splitting the data.\n    \"\"\"\n```", "```python\nclass Kmeans(_BaseModel, _Cluster, _IterativeModel):\n    \"\"\"\n    K-means clustering algorithm implementation.\n\n    Parameters:\n    k (int): The number of clusters to form as well as the number of centroids to generate.\n    max_iter (int, optional): Maximum number of iterations of the k-means algorithm for a single run. Default is 10.\n    convergence_tolerance (float, optional): Tolerance for stopping criteria. Default is 1e-05.\n    random_seed (int, optional): Seed for the random number generator. Default is None.\n    print_progress (int, optional): If greater than 0, print progress every `print_progress` iterations. Default is 0.\n\n    Attributes:\n    k (int): The number of clusters.\n    max_iter (int): Maximum number of iterations.\n    convergence_tolerance (float): Tolerance for stopping criteria.\n    random_seed (int): Seed for the random number generator.\n    print_progress (int): Progress printing frequency.\n    _is_fitted (bool): Whether the model has been fitted.\n    centroids_ (numpy.ndarray): Array of cluster centroids.\n    clusters_ (dict): Dictionary of cluster indices.\n    iterations_ (int): Number of iterations performed.\n\n    Methods:\n    _fit(X, init_params=True): Fit the K-means model to the data.\n    _get_cluster_idx(X, centroids): Get the cluster index for each sample.\n    _predict(X): Predict the cluster labels for the data.\n    \"\"\"\n```", "```python\nclass LabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Binarize labels in a multi-class problem.\n\n    Parameters\n    ----------\n    neg_label : int, default=0\n        Value with which negative labels will be encoded.\n    pos_label : int, default=1\n        Value with which positive labels will be encoded.\n    sparse_output : bool, default=False\n        If True, the output will be a sparse matrix. Only supported with non-zero pos_label and zero neg_label.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Array of class labels.\n    y_type_ : str\n        Type of the target data.\n    sparse_input_ : bool\n        Whether the input data is sparse.\n\n    Methods\n    -------\n    fit(y)\n        Fit the label binarizer to the data.\n    fit_transform(y)\n        Fit the label binarizer and transform the data.\n    transform(y)\n        Transform the data.\n    inverse_transform(Y, threshold=None)\n        Inverse transform the data.\n\n    Raises\n    ------\n    ValueError\n        If neg_label is not strictly less than pos_label.\n        If sparse_output is True and pos_label is not zero or neg_label is not zero.\n        If the object was not fitted with multilabel input.\n        If y has 0 samples.\n        If Multioutput target data is not supported with label binarization.\n    \"\"\"\n```", "```python\nclass LabelEncoder(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Encodes target labels with value between 0 and n_classes-1.\n\n    Parameters\n    ----------\n    None\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        A copy of the unique values provided in fit.\n\n    Methods\n    -------\n    fit(y)\n        Fit label encoder.\n\n    fit_transform(y)\n        Fit label encoder and return encoded labels.\n\n    transform(y)\n        Transform labels to normalized encoding.\n\n    inverse_transform(y)\n        Transform labels back to original encoding.\n\n    _more_tags()\n        Additional tags for estimator.\n    \"\"\"\n```", "```python\nclass LinearRegression(_BaseModel, _IterativeModel, _Regressor):\n    \"\"\"\n    A class for implementing linear regression models.\n\n    Parameters:\n    method (str): The method to use for fitting the model. Supported methods are 'sgd', 'direct', 'svd', and 'qr'. Default is 'direct'.\n    eta (float): The learning rate for stochastic gradient descent. Default is 0.01.\n    epochs (int): The number of epochs to run for stochastic gradient descent. Default is 50.\n    minibatches (int): The number of minibatches to use for stochastic gradient descent. Default is None.\n    random_seed (int): The random seed for reproducibility. Default is None.\n    print_progress (int): The frequency of printing progress. Default is 0.\n\n    Attributes:\n    eta (float): The learning rate for stochastic gradient descent.\n    epochs (int): The number of epochs to run for stochastic gradient descent.\n    minibatches (int): The number of minibatches to use for stochastic gradient descent.\n    random_seed (int): The random seed for reproducibility.\n    print_progress (int): The frequency of printing progress.\n    _is_fitted (bool): A flag indicating whether the model has been fitted.\n    method (str): The method used for fitting the model.\n    b_ (numpy.ndarray): The bias term of the model.\n    w_ (numpy.ndarray): The weight vector of the model.\n    cost_ (list): A list of the cost values at each epoch during fitting.\n\n    Methods:\n    __init__(self, method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0):\n        Initializes the LinearRegression model with the given parameters.\n\n    _fit(self, X, y, init_params=True):\n        Fits the model to the given data using the specified method.\n\n    _normal_equation(self, X, y):\n        Fits the model using the normal equation method.\n\n    _net_input(self, X):\n        Computes the net input for the given input data.\n\n    _predict(self, X):\n        Predicts the target values for the given input data.\n\n    _sum_squared_error_cost(self, y, y_val):\n        Computes the sum of squared error cost for the given predictions.\n    \"\"\"\n```", "```python\nclass LogisticRegression(_BaseModel, _IterativeModel, _Classifier):\n    \"\"\"\n    Logistic Regression classifier.\n\n    Parameters\n    ----------\n    eta : float, optional (default=0.01)\n        Learning rate for gradient descent.\n    epochs : int, optional (default=50)\n        Number of iterations over the training dataset.\n    l2_lambda : float, optional (default=0.0)\n        Regularization parameter for L2 regularization.\n    minibatches : int, optional (default=1)\n        Number of minibatches for stochastic gradient descent.\n    random_seed : int, optional (default=None)\n        Seed for random number generator for reproducibility.\n    print_progress : int, optional (default=0)\n        Frequency of progress printing during training.\n\n    Attributes\n    ----------\n    w_ : array-like, shape (n_features,)\n        Weights after fitting.\n    b_ : array-like, shape (1,)\n        Bias after fitting.\n    cost_ : list\n        List of costs computed during training.\n    init_time_ : float\n        Time taken to initialize the model.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the model to the training data.\n    predict(X)\n        Predict class labels for samples in X.\n    predict_proba(X)\n        Predict class probabilities for samples in X.\n    \"\"\"\n```", "```python\n\"\"\"\nA base class for defining custom loss functions in Keras.\n\nThis class provides a framework for creating and using custom loss functions in Keras models. It includes methods for initializing the loss function, computing the loss, and retrieving configuration details.\n\nAttributes:\n    name (str): The name of the loss function.\n    reduction (str): The reduction method to apply to the loss. Default is 'sum_over_batch_size'.\n    dtype (dtype): The data type of the loss function.\n\nMethods:\n    __init__(self, name=None, reduction='sum_over_batch_size', dtype=None):\n        Initializes the loss function with the given name, reduction method, and data type.\n\n    __call__(self, y_true, y_pred, sample_weight=None):\n        Computes the loss for the given true and predicted values, optionally applying sample weights.\n\n    call(self, y_true, y_pred):\n        Computes the loss for the given true and predicted values. This method must be implemented by subclasses.\n\n    get_config(self):\n        Returns a dictionary containing the configuration of the loss function.\n\n    from_config(cls, config):\n        Creates a new instance of the loss function from the given configuration.\n\n    _type(self):\n        Returns the type of the loss function.\n\"\"\"\n```", "```python\nclass MaxPooling1D(Pooling1D):\n    \"\"\"\n    Applies a 1D max pooling operation to the input.\n\n    Args:\n        pool_size (int, tuple of int): The size of the pooling window. If a tuple is provided, it should be of length 1.\n        strides (int, tuple of int, optional): The strides of the pooling window. If a tuple is provided, it should be of length 1. Defaults to None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The format of the input data. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        A 1D tensor after applying the max pooling operation.\n    \"\"\"\n```", "```python\nclass MaxPooling2D(Pooling2D):\n    \"\"\"\n    Applies a 2D max pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window (height, width). Defaults to (2, 2).\n        strides (tuple of int, optional): The strides of the pooling window (height, width). If None, the strides are set to the pool_size. Defaults to None.\n        padding (str, optional): The type of padding to use ('valid' or 'same'). Defaults to 'valid'.\n        data_format (str, optional): The format of the input tensor ('channels_first' or 'channels_last'). If None, the default format is used. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The output tensor after applying the max pooling operation.\n    \"\"\"\n```", "```python\nclass MaxPooling3D(Pooling3D):\n    \"\"\"\n    Applies a 3D max pooling operation to the input tensor.\n\n    Args:\n        pool_size (tuple of int): The size of the pooling window in each dimension (depth, height, width). Default is (2, 2, 2).\n        strides (tuple of int, optional): The strides of the pooling window in each dimension. If None, the strides are set to the pool_size. Default is None.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Default is 'valid'.\n        data_format (str, optional): The format of the input tensor. Can be 'channels_last' or 'channels_first'. Default is None.\n        **kwargs: Additional keyword arguments to pass to the base class.\n\n    Returns:\n        Tensor: The result of the max pooling operation.\n    \"\"\"\n```", "```python\n\"\"\"\nA base class for all metrics in Keras.\n\nThis class provides a framework for implementing custom metrics. It includes methods for resetting the state, updating the state with new data, and computing the result of the metric. The class also supports stateless operations, which can be useful for distributed training.\n\nAttributes:\n    name (str): The name of the metric.\n    dtype (dtype): The data type of the metric.\n    variables (list): A list of variables used by the metric.\n    _metrics (list): A list of nested metrics.\n    _variables (list): A list of variables tracked by the metric.\n    _tracker (Tracker): A tracker object used to manage the metric's variables and nested metrics.\n\nMethods:\n    __init__(self, dtype=None, name=None): Initializes the metric with the given data type and name.\n    reset_state(self): Resets the state of the metric.\n    update_state(self, *args, **kwargs): Updates the state of the metric with new data.\n    stateless_update_state(self, metric_variables, *args, **kwargs): Updates the state of the metric in a stateless manner.\n    result(self): Computes the result of the metric.\n    stateless_result(self, metric_variables): Computes the result of the metric in a stateless manner.\n    stateless_reset_state(self): Resets the state of the metric in a stateless manner.\n    dtype: Returns the data type of the metric.\n    _obj_type(self): Returns the type of the object.\n    add_variable(self, shape, initializer, dtype=None, aggregation='sum', name=None): Adds a variable to the metric.\n    add_weight(self, shape=(), initializer=None, dtype=None, name=None): Adds a weight to the metric.\n    variables: Returns a list of variables used by the metric.\n    __call__(self, *args, **kwargs): Updates the state of the metric with new data and computes the result.\n    get_config(self): Returns a dictionary containing the configuration of the metric.\n    from_config(cls, config): Creates a new instance of the metric from a configuration dictionary.\n    __setattr__(self, name, value): Sets an attribute on the metric, tracking it if necessary.\n    _check_super_called(self): Checks if the superclass's __init__ method has been called.\n    __repr__(self): Returns a string representation of the metric.\n    __str__(self): Returns a string representation of the metric.\n\"\"\"\n```", "```python\nclass MultiLabelBinarizer(TransformerMixin, BaseEstimator, auto_wrap_output_keys=None):\n    \"\"\"\n    Transforms a collection of label sets into a binary matrix.\n\n    Parameters\n    ----------\n    classes : array-like, optional\n        A list of unique class labels. If None, the classes are inferred from the data.\n    sparse_output : bool, default=False\n        If True, the output will be a sparse matrix. If False, the output will be a dense matrix.\n\n    Attributes\n    ----------\n    classes_ : array\n        A list of unique class labels.\n    _cached_dict : dict, optional\n        A cache for class to index mapping.\n\n    Methods\n    -------\n    fit(y)\n        Fit the MultiLabelBinarizer to the data.\n    fit_transform(y)\n        Fit the MultiLabelBinarizer to the data and transform it.\n    transform(y)\n        Transform the data.\n    inverse_transform(yt)\n        Transform the binary matrix back to the original label sets.\n    _build_cache()\n        Build a cache for class to index mapping.\n    _transform(y, class_mapping)\n        Transform the data using the given class mapping.\n    _more_tags()\n        Return additional tags for the estimator.\n    \"\"\"\n```", "```python\nclass OneHotEncoder(_BaseEncoder):\n    \"\"\"\n    Encodes categorical features as a one-hot numeric array.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list of arrays, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the i-th\n          feature. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n\n    drop : {'first', 'if_binary'} or array-like of shape (n_features,), default=None\n        Specifies a methodology to use to drop one of the categories per feature:\n\n        - 'first' : First category is dropped.\n        - 'if_binary' : If the feature contains two categories, one is dropped.\n        - array : Array of shape (n_features,) specifying which categories to drop.\n\n    sparse_output : bool, default=True\n        Whether the output should be in sparse format.\n\n    dtype : dtype, default=np.float64\n        Desired dtype of output.\n\n    handle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, default='error'\n        Specifies how to handle unknown categories:\n\n        - 'error' : Raise an error when unknown categories are present.\n        - 'ignore' : Ignore unknown categories.\n        - 'infrequent_if_exist' : Treat unknown categories as infrequent.\n\n    min_frequency : int or float, default=None\n        Minimum frequency of categories to be included in the encoding.\n\n    max_categories : int, default=None\n        Maximum number of categories to be included in the encoding.\n\n    feature_name_combiner : {'concat'} or callable, default='concat'\n        Method to combine feature names with category names.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature.\n\n    drop_idx_ : array of shape (n_features,)\n        Indices of categories to drop.\n\n    n_features_out_ : int\n        Number of features in the transformed output.\n\n    feature_names_out_ : ndarray of shape (n_features_out,)\n        Feature names in the transformed output.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit OneHotEncoder to X.\n\n    transform(X)\n        Transform X using one-hot encoding.\n\n    inverse_transform(X)\n        Transform X back to the original space.\n\n    get_feature_names_out(input_features=None)\n        Get output feature names for transformation.\n\n    _map_drop_idx_to_infrequent(feature_idx, drop_idx)\n        Map drop index to infrequent category.\n\n    _set_drop_idx()\n        Set drop indices for each feature.\n\n    _compute_transformed_categories(i, remove_dropped=True)\n        Compute transformed categories for feature i.\n\n    _remove_dropped_categories(categories, i)\n        Remove dropped categories from feature i.\n\n    _compute_n_features_outs()\n        Compute number of features in the transformed output.\n\n    _fit(X, handle_unknown, force_all_finite)\n        Fit OneHotEncoder to X.\n\n    _transform(X, handle_unknown, force_all_finite, warn_on_unknown)\n        Transform X using one-hot encoding.\n\n    _check_get_feature_name_combiner()\n        Check and return the feature name combiner.\n    \"\"\"\n```", "```python\nclass OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"\n    Perform OPTICS (Ordering Points To Identify the Clustering Structure) clustering.\n\n    Parameters\n    ----------\n    min_samples : int or float, default=5\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        If float, it represents a fraction of the total number of samples.\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n\n    metric : str or callable, default='minkowski'\n        The distance metric to use. If a string, it must be one of the options in _VALID_METRICS.\n        If callable, it must be a function that takes two arrays as input and returns a distance value.\n\n    p : float, default=2\n        The power parameter for the Minkowski metric.\n\n    metric_params : dict or None, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The method to use for clustering. Can be 'dbscan' or 'xi'.\n\n    eps : float or None, default=None\n        The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n        If None, it is set to max_eps.\n\n    xi : float, default=0.05\n        The xi parameter for the xi method.\n\n    predecessor_correction : bool, default=True\n        Whether to use predecessor correction.\n\n    min_cluster_size : int, float, or None, default=None\n        The minimum number of samples in a cluster. If float, it represents a fraction of the total number of samples.\n\n    algorithm : str, default='auto'\n        The algorithm to use for computing the OPTICS graph. Can be 'auto', 'brute', 'ball_tree', or 'kd_tree'.\n\n    leaf_size : int, default=30\n        The leaf size for the ball_tree or kd_tree algorithm.\n\n    memory : str or HasMethods('cache'), default=None\n        Used to cache the computation of the OPTICS graph.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n\n    Attributes\n    ----------\n    labels_ : array of shape (n_samples,)\n        The cluster labels for each sample.\n\n    ordering_ : array of shape (n_samples,)\n        The ordering of the samples.\n\n    core_distances_ : array of shape (n_samples,)\n        The core distances of the samples.\n\n    reachability_ : array of shape (n_samples,)\n        The reachability distances of the samples.\n\n    predecessor_ : array of shape (n_samples,)\n        The predecessor of each sample in the ordering.\n\n    cluster_hierarchy_ : array of shape (n_samples,)\n        The cluster hierarchy for the xi method.\n    \"\"\"\n```", "```python\nclass OrdinalEncoder(OneToOneFeatureMixin, _BaseEncoder):\n    \"\"\"\n    Encodes categorical features as ordinal integers.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list of arrays, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the i-th\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n\n    dtype : type, default=np.float64\n        Desired dtype of output.\n\n    handle_unknown : {'error', 'use_encoded_value'}, default='error'\n        Strategy to handle unknown categories.\n\n        - 'error' : Raise an error when unknown categories are present during\n          transform.\n        - 'use_encoded_value' : Assign a value for unknown categories.\n\n    unknown_value : int or np.nan, default=None\n        Value to use for unknown categories when ``handle_unknown='use_encoded_value'``.\n\n    encoded_missing_value : int or np.nan, default=np.nan\n        Value to use to encode missing values.\n\n    min_frequency : int or float, default=None\n        Minimum frequency of categories to be included. Categories with a frequency\n        lower than this will be considered as 'infrequent' and encoded as 'infrequent_sklearn'.\n\n    max_categories : int, default=None\n        Maximum number of categories to be included. If the number of unique categories\n        exceeds this value, the least frequent categories will be considered as 'infrequent'.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        Categories of each feature.\n\n    feature_names_in_ : ndarray of shape (n_features,)\n        Names of features seen during fit.\n\n    n_features_in_ : int\n        Number of features seen during fit.\n\n    See Also\n    --------\n    OneHotEncoder : Encode categorical features as a one-hot numeric array.\n    \"\"\"\n```", "```python\n\"\"\"\nA 1D pooling layer for 1D inputs.\n\nArgs:\n    pool_function: A callable that defines the pooling operation.\n    pool_size: An integer or tuple of a single integer, specifying the size of the pooling window.\n    strides: An integer or tuple of a single integer, specifying the strides of the pooling operation.\n    padding: A string, either 'valid' or 'same', specifying the padding mode.\n    data_format: A string, either 'channels_last' or 'channels_first', specifying the data format.\n    name: A string, the name of the layer.\n    **kwargs: Additional keyword arguments passed to the base Layer class.\n\nAttributes:\n    pool_function: The callable that defines the pooling operation.\n    pool_size: The size of the pooling window.\n    strides: The strides of the pooling operation.\n    padding: The padding mode.\n    data_format: The data format.\n    input_spec: An InputSpec object specifying the expected input shape.\n\nMethods:\n    call(inputs): Applies the pooling operation to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer.\n    get_config(): Returns a dictionary containing the configuration of the layer.\n\"\"\"\n```", "```python\n\"\"\"\nA 2D pooling layer for spatial data.\n\nArgs:\n    pool_function (callable): The pooling function to use, such as `tf.nn.max_pool` or `tf.nn.avg_pool`.\n    pool_size (tuple of int): The size of the pooling window (height, width).\n    strides (tuple of int): The strides of the pooling window (height, width).\n    padding (str, optional): The padding method, either 'valid' or 'same'. Defaults to 'valid'.\n    data_format (str, optional): The data format, either 'channels_last' or 'channels_first'. Defaults to the backend's default format.\n    name (str, optional): The name of the layer. Defaults to None.\n\nAttributes:\n    pool_function (callable): The pooling function used by the layer.\n    pool_size (tuple of int): The size of the pooling window.\n    strides (tuple of int): The strides of the pooling window.\n    padding (str): The padding method used by the layer.\n    data_format (str): The data format used by the layer.\n    input_spec (InputSpec): Specification of the input shape.\n\nMethods:\n    call(inputs): Applies the pooling operation to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n    get_config(): Returns a dictionary containing the configuration of the layer.\n\"\"\"\n```", "```python\n\"\"\"\nA 3D pooling layer that applies a pooling function to the input tensor.\n\nArgs:\n    pool_function (callable): The pooling function to apply, such as `tf.nn.max_pool3d`.\n    pool_size (tuple of int): The size of the pooling window in each dimension.\n    strides (tuple of int): The strides of the pooling window in each dimension.\n    padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n    data_format (str, optional): The data format of the input tensor. Can be 'channels_last' or 'channels_first'. Defaults to 'channels_last'.\n    name (str, optional): The name of the layer. Defaults to None.\n\nAttributes:\n    pool_function (callable): The pooling function to apply.\n    pool_size (tuple of int): The size of the pooling window in each dimension.\n    strides (tuple of int): The strides of the pooling window in each dimension.\n    padding (str): The type of padding used.\n    data_format (str): The data format of the input tensor.\n    input_spec (InputSpec): Specification of the input shape.\n\nMethods:\n    call(inputs): Applies the pooling operation to the input tensor.\n    compute_output_shape(input_shape): Computes the output shape of the layer.\n    get_config(): Returns a dictionary containing the configuration of the layer.\n\"\"\"\n```", "```python\nclass PrincipalComponentAnalysis(_BaseModel):\n    \"\"\"\n    Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the data into a new coordinate system such that the first coordinate (or principal component) has the largest possible variance, and each subsequent coordinate is orthogonal to the previous ones and has the next largest variance.\n\n    Parameters\n    ----------\n    n_components : int or None, optional (default=None)\n        Number of components to keep. If None, all components are kept.\n\n    solver : str, optional (default='svd')\n        Solver to use for computing the principal components. Options are 'eigen' and 'svd'.\n\n    whitening : bool, optional (default=False)\n        If True, the components are scaled to unit variance.\n\n    Attributes\n    ----------\n    n_components_ : int\n        The estimated number of components.\n\n    explained_variance_ : array-like, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n    explained_variance_ratio_ : array-like, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    components_ : array-like, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of maximum variance in the data.\n\n    Notes\n    -----\n    This implementation uses Singular Value Decomposition (SVD) by default, which is more numerically stable than the eigenvalue decomposition method.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import PCA\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.components_)\n    [[-0.70710678 -0.70710678]\n     [ 0.70710678 -0.70710678]]\n    \"\"\"\n```", "```python\n\"\"\"\nRMSprop optimizer for TensorFlow.\n\nRMSprop is an optimizer that uses the root mean square (RMS) of recent gradients to normalize the gradients. This can help to mitigate the problem of exploding gradients and can lead to faster convergence.\n\nArgs:\n    learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.001.\n    rho (float, optional): The decay rate for the moving average of squared gradients. Defaults to 0.9.\n    momentum (float, optional): The momentum for the optimizer. Defaults to 0.0.\n    epsilon (float, optional): A small constant for numerical stability. Defaults to 1e-07.\n    centered (bool, optional): If True, use the centered version of RMSprop. Defaults to False.\n    name (str, optional): The name of the optimizer. Defaults to 'RMSprop'.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports gradient aggregation.\n\nMethods:\n    __init__(self, learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name='RMSprop', **kwargs): Initializes the RMSprop optimizer.\n    _create_slots(self, var_list): Creates slots for the optimizer.\n    _prepare_local(self, var_device, var_dtype, apply_state): Prepares local variables for the optimizer.\n    _resource_apply_dense(self, grad, var, apply_state=None): Applies the optimizer to a dense variable.\n    _resource_apply_sparse(self, grad, var, indices, apply_state=None): Applies the optimizer to a sparse variable.\n    set_weights(self, weights): Sets the weights of the optimizer.\n    get_config(self): Returns the configuration of the optimizer.\n\"\"\"\n```", "```python\nclass SelfTrainingClassifier(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"\n    A self-training classifier that iteratively trains on labeled and predicted data.\n\n    Parameters\n    ----------\n    estimator : estimator object, optional (default=None)\n        An estimator object implementing 'fit' method. If None, the base_estimator will be used.\n    base_estimator : estimator object, optional (default='deprecated')\n        An estimator object implementing 'fit' method. Deprecated in version 1.6 and will be removed in version 1.8. Use 'estimator' instead.\n    threshold : float, optional (default=0.75)\n        The threshold for selecting samples based on their predicted probability.\n    criterion : str, optional (default='threshold')\n        The criterion for selecting samples. Can be 'threshold' or 'k_best'.\n    k_best : int, optional (default=10)\n        The number of samples to select based on the highest predicted probability when criterion is 'k_best'.\n    max_iter : int or None, optional (default=10)\n        The maximum number of iterations. If None, there is no limit.\n    verbose : bool, optional (default=False)\n        Whether to print verbose output during training.\n\n    Attributes\n    ----------\n    estimator_ : estimator object\n        The fitted estimator.\n    transduction_ : array-like of shape (n_samples,)\n        The transduction of the labels.\n    labeled_iter_ : array-like of shape (n_samples,)\n        The iteration at which each sample was labeled.\n    n_iter_ : int\n        The number of iterations performed.\n    termination_condition_ : str\n        The reason for termination ('no_change', 'max_iter', 'all_labeled').\n    classes_ : array-like of shape (n_classes,)\n        The classes of the target.\n\n    Methods\n    -------\n    fit(X, y, **params)\n        Fit the model using self-training.\n    predict(X, **params)\n        Predict the target values for the provided data.\n    predict_proba(X, **params)\n        Predict the class probabilities for the provided data.\n    decision_function(X, **params)\n        Compute the decision function of the provided data.\n    predict_log_proba(X, **params)\n        Predict the class log-probabilities for the provided data.\n    score(X, y, **params)\n        Return the mean accuracy on the given test data and labels.\n    get_metadata_routing()\n        Get the metadata routing configuration for this estimator.\n    \"\"\"\n```", "```python\nclass SeparableConv(Conv):\n    \"\"\"\n    A separable convolution layer that performs a depthwise convolution followed by a pointwise convolution.\n\n    Args:\n        rank (int): An integer, the rank of the convolution, e.g., 1 for 1D, 2 for 2D, 3 for 3D.\n        filters (int): An integer, the dimensionality of the output space (i.e., the number of filters in the convolution).\n        kernel_size (tuple or list): A tuple or list of integers, specifying the size of the convolution window.\n        strides (tuple or list, optional): A tuple or list of integers, specifying the strides of the convolution. Defaults to 1.\n        padding (str, optional): A string, one of 'valid' or 'same'. The type of padding algorithm to use. Defaults to 'valid'.\n        data_format (str, optional): A string, one of 'channels_last' or 'channels_first'. The data format of the input and output tensors. Defaults to None.\n        dilation_rate (tuple or list, optional): A tuple or list of integers, specifying the dilation rate of the convolution. Defaults to 1.\n        depth_multiplier (int, optional): An integer, the number of depthwise convolution outputs per input filter. Defaults to 1.\n        activation (str or callable, optional): A string or callable, the activation function to use. Defaults to None.\n        use_bias (bool, optional): A boolean, whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str or callable, optional): A string or callable, the initializer for the depthwise kernel. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str or callable, optional): A string or callable, the initializer for the pointwise kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str or callable, optional): A string or callable, the initializer for the bias vector. Defaults to 'zeros'.\n        depthwise_regularizer (str or callable, optional): A string or callable, the regularizer for the depthwise kernel. Defaults to None.\n        pointwise_regularizer (str or callable, optional): A string or callable, the regularizer for the pointwise kernel. Defaults to None.\n        bias_regularizer (str or callable, optional): A string or callable, the regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str or callable, optional): A string or callable, the regularizer for the output activity. Defaults to None.\n        depthwise_constraint (str or callable, optional): A string or callable, the constraint for the depthwise kernel. Defaults to None.\n        pointwise_constraint (str or callable, optional): A string or callable, the constraint for the pointwise kernel. Defaults to None.\n        bias_constraint (str or callable, optional): A string or callable, the constraint for the bias vector. Defaults to None.\n        trainable (bool, optional): A boolean, whether the layer's variables are trainable. Defaults to True.\n        name (str, optional): A string, the name of the layer. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the base class.\n\n    Returns:\n        A tensor representing the output of the separable convolution layer.\n    \"\"\"\n```", "```python\nclass SeparableConv1D(SeparableConv):\n    \"\"\"\n    A 1D separable convolution layer.\n\n    Args:\n        filters (int): The number of output filters in the convolution.\n        kernel_size (int): The size of the convolution kernel.\n        strides (int, optional): The strides of the convolution. Defaults to 1.\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (int, optional): The dilation rate of the convolution. Defaults to 1.\n        depth_multiplier (int, optional): The number of depthwise convolution output channels for each input channel. Defaults to 1.\n        activation (str, optional): The activation function to use. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str, optional): The initializer for the depthwise convolution kernel. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str, optional): The initializer for the pointwise convolution kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer for the bias vector. Defaults to 'zeros'.\n        depthwise_regularizer (str, optional): The regularizer for the depthwise convolution kernel. Defaults to None.\n        pointwise_regularizer (str, optional): The regularizer for the pointwise convolution kernel. Defaults to None.\n        bias_regularizer (str, optional): The regularizer for the bias vector. Defaults to None.\n        activity_regularizer (str, optional): The regularizer for the output activity. Defaults to None.\n        depthwise_constraint (str, optional): The constraint for the depthwise convolution kernel. Defaults to None.\n        pointwise_constraint (str, optional): The constraint for the pointwise convolution kernel. Defaults to None.\n        bias_constraint (str, optional): The constraint for the bias vector. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: The output tensor after applying the separable convolution.\n    \"\"\"\n```", "```python\nclass SeparableConv2D(SeparableConv):\n    \"\"\"\n    A 2D separable convolution layer.\n\n    Args:\n        filters (int): The number of output filters in the convolution.\n        kernel_size (tuple): A tuple of integers specifying the height and width of the 2D convolution window.\n        strides (tuple, optional): A tuple of integers specifying the strides of the convolution along the height and width. Defaults to (1, 1).\n        padding (str, optional): The type of padding to use. Can be 'valid' or 'same'. Defaults to 'valid'.\n        data_format (str, optional): The data format to use. Can be 'channels_last' or 'channels_first'. Defaults to None.\n        dilation_rate (tuple, optional): A tuple of integers specifying the dilation rate of the convolution along the height and width. Defaults to (1, 1).\n        depth_multiplier (int, optional): The number of depthwise convolution outputs per input filter. Defaults to 1.\n        activation (str, optional): The activation function to use. Can be 'relu', 'sigmoid', etc. Defaults to None.\n        use_bias (bool, optional): Whether to use a bias vector. Defaults to True.\n        depthwise_initializer (str, optional): The initializer to use for the depthwise kernel. Defaults to 'glorot_uniform'.\n        pointwise_initializer (str, optional): The initializer to use for the pointwise kernel. Defaults to 'glorot_uniform'.\n        bias_initializer (str, optional): The initializer to use for the bias vector. Defaults to 'zeros'.\n        depthwise_regularizer (str, optional): The regularizer to use for the depthwise kernel. Defaults to None.\n        pointwise_regularizer (str, optional): The regularizer to use for the pointwise kernel. Defaults to None.\n        bias_regularizer (str, optional): The regularizer to use for the bias vector. Defaults to None.\n        activity_regularizer (str, optional): The regularizer to use for the output of the layer. Defaults to None.\n        depthwise_constraint (str, optional): The constraint to use for the depthwise kernel. Defaults to None.\n        pointwise_constraint (str, optional): The constraint to use for the pointwise kernel. Defaults to None.\n        bias_constraint (str, optional): The constraint to use for the bias vector. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor after applying the separable convolution.\n    \"\"\"\n```", "```python\nclass SequentialFeatureSelector(_BaseXComposition, MetaEstimatorMixin):\n    \"\"\"\n    Sequential Feature Selector for feature selection using a greedy approach.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A supervised learning estimator with a `fit` method that provides information about feature importance either through a `coef_` attribute or through a `feature_importances_` attribute.\n\n    k_features : int, tuple, or str, default=1\n        The number of features to select. If an integer, it specifies the exact number of features to select. If a tuple, it specifies a range (min, max) of features to select. If a string, it can be 'best' to select the best features or 'parsimonious' to select the most parsimonious features.\n\n    forward : bool, default=True\n        Whether to use a forward selection approach (starting with no features and adding features one by one) or a backward elimination approach (starting with all features and removing features one by one).\n\n    floating : bool, default=False\n        Whether to use a floating forward selection or elimination approach, which allows features to be added or removed after each step.\n\n    verbose : int, default=0\n        The verbosity level of the output. If 0, no output is generated. If 1, a progress bar is displayed. If greater than 1, detailed output is displayed.\n\n    scoring : str or callable, default=None\n        The scoring function to use for model evaluation. If None, the scoring function is inferred from the estimator type (accuracy for classifiers, R^2 for regressors).\n\n    cv : int, cross-validation generator, or iterable, default=5\n        Determines the cross-validation splitting strategy. If None, no cross-validation is performed.\n\n    n_jobs : int, default=1\n        The number of jobs to run in parallel.\n\n    pre_dispatch : str or int, default='2*n_jobs'\n        Controls the number of jobs that are dispatched in parallel.\n\n    clone_estimator : bool, default=True\n        Whether to clone the estimator before fitting.\n\n    fixed_features : tuple or None, default=None\n        A tuple of fixed features that should not be removed during feature selection.\n\n    feature_groups : list of lists, default=None\n        A list of lists, where each sublist contains indices of features that should be treated as a group.\n\n    Attributes\n    ----------\n    k_feature_idx_ : tuple\n        The indices of the selected features.\n\n    k_feature_names_ : tuple\n        The names of the selected features.\n\n    k_score_ : float\n        The score of the selected features.\n\n    subsets_ : dict\n        A dictionary containing the subsets of features and their corresponding scores.\n\n    Methods\n    -------\n    fit(X, y, groups=None, **fit_params)\n        Fit the SequentialFeatureSelector to the data.\n\n    transform(X)\n        Transform the data by selecting the selected features.\n\n    fit_transform(X, y, groups=None, **fit_params)\n        Fit the SequentialFeatureSelector to the data and transform the data.\n\n    get_metric_dict(confidence_interval=0.95)\n        Get a dictionary containing the metrics for each subset of features.\n\n    _calc_confidence(ary, confidence=0.95)\n        Calculate the confidence interval for the scores.\n\n    _check_fitted()\n        Check if the SequentialFeatureSelector has been fitted.\n    \"\"\"\n```", "```python\n\"\"\"\nStochastic Gradient Descent (SGD) optimizer.\n\nThis class implements the SGD algorithm, which is a simple yet effective optimization algorithm for training neural networks. It updates the model parameters in the direction of the negative gradient of the loss function with respect to the parameters.\n\nArgs:\n    learning_rate (float, optional): The learning rate for the optimizer. Defaults to 0.01.\n    momentum (float, optional): The momentum factor. Defaults to 0.0.\n    nesterov (bool, optional): Whether to use Nesterov momentum. Defaults to False.\n    name (str, optional): The name of the optimizer. Defaults to 'SGD'.\n    **kwargs: Additional keyword arguments.\n\nAttributes:\n    _HAS_AGGREGATE_GRAD (bool): Indicates whether the optimizer supports gradient aggregation.\n    _momentum (bool): Whether momentum is enabled.\n    nesterov (bool): Whether to use Nesterov momentum.\n\nMethods:\n    __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs): Initializes the SGD optimizer.\n    _create_slots(self, var_list): Creates slots for momentum variables.\n    _prepare_local(self, var_device, var_dtype, apply_state): Prepares local variables for the optimizer.\n    _resource_apply_dense(self, grad, var, apply_state=None): Applies the SGD update to a dense variable.\n    _resource_apply_sparse_duplicate_indices(self, grad, var, indices, **kwargs): Applies the SGD update to a sparse variable with duplicate indices.\n    _resource_apply_sparse(self, grad, var, indices, apply_state=None): Applies the SGD update to a sparse variable.\n    get_config(self): Returns the configuration of the optimizer.\n\"\"\"\n```", "```python\nclass SoftmaxRegression(_BaseModel, _IterativeModel, _Classifier, _MultiClass):\n    \"\"\"\n    A class implementing the Softmax Regression algorithm for multi-class classification.\n\n    Parameters:\n    eta (float): Learning rate (default is 0.01).\n    epochs (int): Number of iterations over the training dataset (default is 50).\n    l2 (float): L2 regularization strength (default is 0.0).\n    minibatches (int): Number of minibatches for stochastic gradient descent (default is 1).\n    n_classes (int, optional): Number of classes. If None, it will be inferred from the data (default is None).\n    random_seed (int, optional): Seed for random number generation (default is None).\n    print_progress (int): Interval for printing progress (default is 0, which means no progress is printed).\n\n    Attributes:\n    eta (float): Learning rate.\n    epochs (int): Number of iterations over the training dataset.\n    l2 (float): L2 regularization strength.\n    minibatches (int): Number of minibatches for stochastic gradient descent.\n    n_classes (int): Number of classes.\n    random_seed (int): Seed for random number generation.\n    print_progress (int): Interval for printing progress.\n    _is_fitted (bool): Flag indicating whether the model has been fitted.\n    _n_features (int): Number of features in the training data.\n    b_ (np.ndarray): Bias term.\n    w_ (np.ndarray): Weight matrix.\n    cost_ (list): List to store the cost at each epoch.\n    init_time_ (float): Time when the model was initialized.\n\n    Methods:\n    _net_input(X): Computes the net input for the given input X.\n    _softmax_activation(z): Applies the softmax activation function to the net input z.\n    _cross_entropy(output, y_target): Computes the cross-entropy loss.\n    _cost(cross_entropy): Computes the total cost including L2 regularization.\n    _to_classlabels(z): Converts the output probabilities to class labels.\n    _forward(X): Performs the forward pass through the network.\n    _backward(X, y_true, y_probas): Computes the gradients for the weights and bias.\n    _fit(X, y, init_params=True): Fits the model to the training data.\n    predict_proba(X): Predicts the class probabilities for the given input X.\n    _predict(X): Predicts the class labels for the given input X.\n    \"\"\"\n```", "```python\nclass TargetEncoder(OneToOneFeatureMixin, _BaseEncoder):\n    \"\"\"\n    Encodes categorical features by replacing them with target-encoded values.\n\n    Parameters\n    ----------\n    categories : {'auto'} or list of lists, default='auto'\n        Categories (unique values) to encode. 'auto' means that the encoder will determine the categories\n        automatically from the data.\n\n    target_type : {'auto', 'continuous', 'binary', 'multiclass'}, default='auto'\n        Type of target variable. 'auto' means that the type will be inferred from the data.\n\n    smooth : {'auto'} or float, default='auto'\n        Smoothing parameter to prevent overfitting. 'auto' means that the smoothing will be determined\n        automatically from the data.\n\n    cv : int, default=5\n        Number of folds for cross-validation.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting into folds.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness of the cross-validation process.\n\n    Attributes\n    ----------\n    encodings_ : array-like\n        Encodings for each feature.\n\n    target_mean_ : array-like\n        Mean of the target variable for each class.\n\n    classes_ : array-like\n        Classes for multiclass target.\n\n    n_features_in_ : int\n        Number of input features.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the encoder to the data.\n\n    fit_transform(X, y)\n        Fit the encoder to the data and transform it.\n\n    transform(X)\n        Transform the data using the fitted encoder.\n\n    get_feature_names_out(input_features=None)\n        Get the feature names out.\n\n    _more_tags()\n        Return additional tags for the estimator.\n    \"\"\"\n```", "```python\nclass TransactionEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encodes a dataset of transactions into a binary matrix where each row represents a transaction and each column represents a unique item.\n\n    Parameters\n    ----------\n    None\n\n    Attributes\n    ----------\n    columns_ : list\n        A list of unique items sorted alphabetically.\n    columns_mapping_ : dict\n        A dictionary mapping each unique item to its corresponding column index.\n\n    Methods\n    -------\n    fit(X)\n        Fits the encoder to the dataset of transactions.\n    transform(X, sparse=False)\n        Transforms the dataset of transactions into a binary matrix.\n    inverse_transform(array)\n        Inversely transforms the binary matrix back into a dataset of transactions.\n    fit_transform(X, sparse=False)\n        Fits the encoder to the dataset and then transforms it.\n    get_feature_names_out()\n        Returns the feature names of the encoded matrix.\n    \"\"\"\n```", "```python\nclass UpSampling1D(Layer):\n    \"\"\"\n    Upsamples a 1D input sequence by repeating each element along the specified axis.\n\n    Args:\n        size (int, optional): The upsampling factor. Each element in the input sequence will be repeated `size` times along the specified axis. Defaults to 2.\n\n    Returns:\n        Tensor: The upsampled 1D sequence.\n\n    Example:\n        >>> upsample = UpSampling1D(size=3)\n        >>> input_seq = tf.constant([[1, 2, 3], [4, 5, 6]])\n        >>> output_seq = upsample(input_seq)\n        >>> print(output_seq)\n        tf.Tensor(\n            [[1 1 1 2 2 2 3 3 3]\n             [4 4 4 5 5 5 6 6 6]], shape=(2, 9), dtype=int32)\n    \"\"\"\n```", "```python\n\"\"\"\nA layer that upsamples a 2D input.\n\nThis layer uses the specified interpolation method to increase the spatial dimensions of the input tensor.\n\nArgs:\n    size (tuple of int): The upsampling factors for the height and width. Defaults to (2, 2).\n    data_format (str, optional): The data format of the input tensor. Can be either 'channels_first' or 'channels_last'. If not specified, the default data format is used.\n    interpolation (str): The interpolation method to use. Can be either 'nearest' or 'bilinear'. Defaults to 'nearest'.\n\nRaises:\n    ValueError: If the `interpolation` argument is not one of 'nearest' or 'bilinear'.\n\nInput shape:\n    4D tensor with shape `(batch_size, channels, height, width)` if `data_format` is 'channels_first', or `(batch_size, height, width, channels)` if `data_format` is 'channels_last'.\n\nOutput shape:\n    4D tensor with shape `(batch_size, channels, height * size[0], width * size[1])` if `data_format` is 'channels_first', or `(batch_size, height * size[0], width * size[1], channels)` if `data_format` is 'channels_last'.\n\nReturns:\n    A 4D tensor with the spatial dimensions increased by the specified factors.\n\"\"\"\n```", "```python\nclass UpSampling3D(Layer):\n    \"\"\"\n    Upsamples the 3D input tensor.\n\n    Args:\n        size (tuple of int): The upsampling factor for each spatial dimension. Default is (2, 2, 2).\n        data_format (str, optional): The data format of the input tensor. Can be 'channels_first' or 'channels_last'. Default is None.\n        **kwargs: Additional keyword arguments passed to the base Layer class.\n\n    Returns:\n        Tensor: The upsampled 3D tensor.\n\n    Methods:\n        compute_output_shape(input_shape): Computes the output shape of the layer given the input shape.\n        call(inputs): Applies the upsampling operation to the input tensor.\n        get_config(): Returns a dictionary containing the configuration of the layer.\n    \"\"\"\n```", "```python\nclass ZeroPadding1D(Layer):\n    \"\"\"\n    Zero-padding layer for 1D input (e.g. temporal sequences).\n\n    Args:\n        padding (int or tuple of int): The amount of padding to add on both sides of the input. If a tuple is provided, it should be of length 2, where the first element is the padding to add before the input and the second element is the padding to add after the input. Defaults to 1.\n        **kwargs: Additional keyword arguments passed to the base Layer class.\n\n    Returns:\n        A 3D tensor with the same number of channels as the input, but with the specified padding added to the length dimension.\n\n    Notes:\n        - The input shape is expected to be (batch_size, length, channels).\n        - The output shape will be (batch_size, length + padding[0] + padding[1], channels).\n    \"\"\"\n```", "```python\nclass ZeroPadding2D(Layer):\n    \"\"\"\n    Adds zero padding to the input tensor.\n\n    Args:\n        padding (int, tuple of 2 ints, or tuple of 2 tuples of 2 ints): \n            - If an int, the same symmetric padding is applied to all sides.\n            - If a tuple of 2 ints, interpreted as (symmetric_height_pad, symmetric_width_pad).\n            - If a tuple of 2 tuples of 2 ints, interpreted as ((top_pad, bottom_pad), (left_pad, right_pad)).\n        data_format (str, optional): \n            - 'channels_first' or 'channels_last'. Defaults to the default image data format configured in Keras.\n\n    Returns:\n        Tensor: The padded tensor.\n\n    Raises:\n        ValueError: If `padding` is not an int, a tuple of 2 ints, or a tuple of 2 tuples of 2 ints.\n    \"\"\"\n```", "```python\nclass ZeroPadding3D(Layer):\n    \"\"\"\n    Zero-padding layer for 3D input (e.g. spatial or spatio-temporal).\n\n    Args:\n        padding (int, tuple of 3 ints, or tuple of 3 tuples of 2 ints): \n            - If an integer, the same symmetric padding is applied to all three spatial dimensions.\n            - If a tuple of 3 ints, interpreted as (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad).\n            - If a tuple of 3 tuples of 2 ints, interpreted as ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)).\n        data_format (str, optional): \n            - The data format to use ('channels_first' or 'channels_last'). If not specified, the default is 'channels_last'.\n        **kwargs: \n            - Additional keyword arguments passed to the base Layer class.\n\n    Returns:\n        Tensor: \n            - The padded tensor with the same number of dimensions as the input tensor.\n\n    Raises:\n        ValueError: \n            - If the `padding` argument is not an int, a tuple of 3 ints, or a tuple of 3 tuples of 2 ints.\n            - If the `padding` tuple does not have exactly 3 elements.\n    \"\"\"\n```", "```python\nclass _BaseEncoder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Base class for encoding categorical features.\n\n    This class provides methods for checking input data, fitting the encoder to the data,\n    transforming the data, and handling infrequent categories.\n\n    Parameters\n    ----------\n    categories : 'auto' or list of arrays, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list of arrays : List of arrays with categories for each feature.\n\n    handle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, default='error'\n        Strategy to handle unknown categories during transform:\n\n        - 'error' : Raise an error when unknown categories are found.\n        - 'ignore' : Ignore unknown categories and treat them as missing.\n        - 'infrequent_if_exist' : Treat unknown categories as infrequent if they exist in the training data.\n\n    max_categories : int, default=None\n        Maximum number of categories for each feature. If not None, infrequent categories will be mapped to a single category.\n\n    min_frequency : int or float, default=None\n        Minimum frequency of categories. If not None, infrequent categories will be mapped to a single category.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        Categories (unique values) per feature.\n\n    categories_counts_ : list of arrays\n        Counts of each category per feature.\n\n    infrequent_indices_ : list of arrays\n        Indices of infrequent categories per feature.\n\n    default_to_infrequent_mappings_ : list of arrays\n        Mappings for defaulting to infrequent categories.\n\n    n_features_in_ : int\n        Number of features in the input data.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit the encoder to the data.\n\n    transform(X)\n        Transform the data using the fitted encoder.\n\n    fit_transform(X, y=None)\n        Fit the encoder to the data and transform the data.\n\n    infrequent_categories_\n        Get the infrequent categories for each feature.\n\n    _check_X(X, force_all_finite=True)\n        Check the input data and return it in a suitable format.\n\n    _fit(X, handle_unknown='error', force_all_finite=True, return_counts=False, return_and_ignore_missing_for_infrequent=False)\n        Fit the encoder to the data.\n\n    _transform(X, handle_unknown='error', force_all_finite=True, warn_on_unknown=False, ignore_category_indices=None)\n        Transform the data using the fitted encoder.\n\n    _check_infrequent_enabled()\n        Check if infrequent category handling is enabled.\n\n    _identify_infrequent(category_count, n_samples, col_idx)\n        Identify infrequent categories based on frequency.\n\n    _fit_infrequent_category_mapping(n_samples, category_counts, missing_indices)\n        Fit the mapping for infrequent categories.\n\n    _map_infrequent_categories(X_int, X_mask, ignore_category_indices)\n        Map infrequent categories in the transformed data.\n\n    _more_tags()\n        Return additional tags for the estimator.\n    \"\"\"\n```"]